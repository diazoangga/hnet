{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdde6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c49793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import os.path as ops\n",
    "from os.path import exists\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tensorflow.python.framework.ops import convert_to_tensor\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bab9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'D:\\\\Project\\\\00_CommonTools\\\\data'\n",
    "#dataset_path ='D:\\\\Project\\\\LaneNet_TF\\\\dataset'\n",
    "train_path = 'D:\\\\Project\\\\LaneNet_TF\\\\dataset\\\\train.txt'\n",
    "val_path = 'D:\\\\Project\\\\LaneNet_TF\\\\dataset\\\\val.txt'\n",
    "test_path = 'D:\\\\Project\\\\LaneNet_TF\\\\dataset\\\\test.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676372e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test():\n",
    "    file = 'D:\\\\Project\\\\20_Carla0.9.5\\\\PythonAPI\\\\examples\\\\_out\\\\detect'\n",
    "    img_path = []\n",
    "    for idx, img in enumerate(glob.glob('{:s}/*.png'.format(file))):\n",
    "        img_path.append(img)\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = read_test()\n",
    "print(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dbafbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SRGBResize(im, size, filter):\n",
    "    # Convert to numpy array of float\n",
    "    arr = np.array(im, dtype=np.float32) / 255.0\n",
    "    # Convert sRGB -> linear\n",
    "    arr = np.where(arr <= 0.04045, arr/12.92, ((arr+0.055)/1.055)**2.4)\n",
    "    # Resize using PIL\n",
    "    arrOut = np.zeros((size[1], size[0], arr.shape[2]))\n",
    "    for i in range(arr.shape[2]):\n",
    "        chan = Image.fromarray(arr[:,:,i])\n",
    "        chan = chan.resize(size, filter)\n",
    "        arrOut[:,:,i] = np.array(chan).clip(0.0, 1.0)\n",
    "    # Convert linear -> sRGB\n",
    "    arrOut = np.where(arrOut <= 0.0031308, 12.92*arrOut, 1.055*arrOut**(1.0/2.4) - 0.055)\n",
    "    # Convert to 8-bit\n",
    "    arrOut = np.uint8(np.rint(arrOut * 255.0))\n",
    "    # Convert back to PIL\n",
    "    return Image.fromarray(arrOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5310c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRResize(im, size, filter):\n",
    "    # Convert to numpy array of float\n",
    "    arr = np.array(im, dtype=np.float32) / 255.0\n",
    "    # Convert sRGB -> linear\n",
    "    arr = np.where(arr <= 0.04045, arr/12.92, ((arr+0.055)/1.055)**2.4)\n",
    "    # Resize using PIL\n",
    "    arrOut = np.zeros((size[1], size[0]))\n",
    "    chan = Image.fromarray(arr[:,:,i])\n",
    "    chan = chan.resize(size, filter)\n",
    "    arrOut[:,:] = np.array(chan).clip(0.0, 1.0)\n",
    "    # Convert linear -> sRGB\n",
    "    arrOut = np.where(arrOut <= 0.0031308, 12.92*arrOut, 1.055*arrOut**(1.0/2.4) - 0.055)\n",
    "    # Convert to 8-bit\n",
    "    arrOut = np.uint8(np.rint(arrOut * 255.0))\n",
    "    # Convert back to PIL\n",
    "    return Image.fromarray(arrOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b591579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(root, flag):\n",
    "    img_path = []\n",
    "    bin_path = []\n",
    "    inst_path = []\n",
    "\n",
    "    train_file = ops.join(root, 'train.txt')\n",
    "    val_file = ops.join(root, 'val.txt')\n",
    "    test_file = ops.join(root, 'test.txt')\n",
    "\n",
    "    if flag == 'train':\n",
    "        assert exists(train_file)\n",
    "        file_open = train_file\n",
    "    elif flag == 'valid':\n",
    "        assert exists(val_file)\n",
    "        file_open = val_file\n",
    "    else:\n",
    "        assert exists(test_file)\n",
    "        file_open = test_file\n",
    "\n",
    "    df = pd.read_csv(file_open, header=None, delim_whitespace=True, names=['img', 'bin', 'inst'])\n",
    "    #print(df)\n",
    "    img_path = df['img'].values\n",
    "    bin_path = df['bin'].values\n",
    "    inst_path = df['inst'].values\n",
    "\n",
    "    #print(img_path)\n",
    "    return img_path, bin_path, inst_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1219c52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54935\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "img_path, bin_path, inst_path = read_txt(dataset_path, 'train')\n",
    "random.shuffle(img_path)\n",
    "random.shuffle(bin_path)\n",
    "random.shuffle(inst_path)\n",
    "print(len(bin_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56750c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, bin_path, inst_path = img_path[0:6000], bin_path[0:6000], inst_path[0:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ds = tf.zeros([1,256, 512,3], tf.float32)\n",
    "for i, image_name in enumerate(img_path):\n",
    "    image = cv2.imread(image_name)\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize((512,256))\n",
    "    image = img_to_array(image)\n",
    "    image = tf.math.divide(image, tf.constant(255, tf.uint8))\n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    image = tf.expand_dims(image,0)\n",
    "    image_ds = tf.concat([image_ds, image],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b896800",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ds = []\n",
    "for i, image_name in enumerate(img_path):\n",
    "    image = cv2.imread(image_name)\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize((512,256))\n",
    "    image_ds.append(np.array(image, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(image_ds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199eeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.where(tf.math.is_nan(image_ds), 0, 1)\n",
    "np.unique(np.array(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ds = tf.zeros([1,256, 512], tf.uint8)\n",
    "for i, image_name in enumerate(bin_path):\n",
    "    image = cv2.imread(image_name, 0)\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize((512,256))\n",
    "    mask = tf.where(tf.math.equal(image, 255), 1, 0)\n",
    "    mask = tf.cast(mask, tf.uint8)\n",
    "    mask = tf.expand_dims(mask,0)\n",
    "    mask_ds = tf.concat([mask_ds, mask], 0)\n",
    "mask_ds = tf.expand_dims(mask_ds,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1f92d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n",
      "[  0 255]\n"
     ]
    }
   ],
   "source": [
    "mask_ds = []\n",
    "for i, image_name in enumerate(bin_path):\n",
    "    image = cv2.imread(image_name, 0)\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize((512,256))\n",
    "    label_binary = np.zeros([256, 512], dtype=np.uint8)\n",
    "    mask = np.where(np.array(image)[:,:] != [0])\n",
    "    print(np.unique(np.array(image)))\n",
    "    label_binary[mask] = 1\n",
    "    mask_ds.append(np.array(label_binary, dtype=np.uint8))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4838cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(mask_ds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e2897",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_ds = tf.zeros([1,256, 512,1], tf.float32)\n",
    "for i, image_name in enumerate(inst_path):\n",
    "    image = cv2.imread(image_name, 0)\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize((512,256))\n",
    "    image = img_to_array(image)\n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    image = tf.expand_dims(image,0)\n",
    "    inst_ds = tf.concat([inst_ds, image],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccae33b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[ 0 70]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[ 0 70]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[ 0 20 70]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[ 0 70]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[ 0 20 70]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  20  70 120 170]\n",
      "[  0  20  70 120]\n",
      "[  0  70 120]\n"
     ]
    }
   ],
   "source": [
    "inst_ds = []\n",
    "for i, image_name in enumerate(inst_path):\n",
    "    image = cv2.imread(image_name, 0)\n",
    "    ex = image\n",
    "    ex = Image.fromarray(ex)\n",
    "    ex = ex.resize((512,256))\n",
    "    print(np.unique(np.array(ex)))\n",
    "    #np.unique(np.array(ex))\n",
    "    ##label_inst = np.empty([256,512,1])\n",
    "    #label_1 = np.zeros([256, 512], dtype=np.uint8)\n",
    "    #label_2 = np.zeros([256, 512], dtype=np.uint8)\n",
    "    #mask_1 = np.where(np.array(ex)[:,:] == [1])\n",
    "    #mask_2 = np.where(np.array(ex)[:,:] == [2])\n",
    "    #print(mask_1)\n",
    "    #label_1[mask_1] = 1\n",
    "    #label_2[mask_2] = 1\n",
    "    #label_1 = np.expand_dims(label_1,axis=2)\n",
    "    #label_2 = np.expand_dims(label_2,axis=2)\n",
    "    #label_inst = np.concatenate((label_1, label_2), axis=-1)\n",
    "    inst_ds.append(np.array(ex,dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af94fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(inst_ds).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e7cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = inst_ds[0]\n",
    "ex = Image.fromarray(ex)\n",
    "ex = ex.resize((512,256))\n",
    "np.unique(np.array(ex))\n",
    "#label_inst = np.empty([256,512,1])\n",
    "label_1 = np.zeros([256, 512], dtype=np.uint8)\n",
    "label_2 = np.zeros([256, 512], dtype=np.uint8)\n",
    "mask_1 = np.where(np.array(ex)[:,:] == [1])\n",
    "mask_2 = np.where(np.array(ex)[:,:] == [2])\n",
    "print(mask_1)\n",
    "label_1[mask_1] = 1\n",
    "label_2[mask_2] = 1\n",
    "label_1 = np.expand_dims(label_1,axis=-1)\n",
    "label_2 = np.expand_dims(label_2,axis=-1)\n",
    "label_inst = np.concatenate((label_1, label_2), axis=-1)\n",
    "print(label_inst.shape)\n",
    "plt.imshow(label_inst[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f056108",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(inst_ds[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f9b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "n, h, w = np.array(inst_ds).shape\n",
    "train_masks_reshaped = np.array(inst_ds).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcbbd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_masks_reshaped_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb3675fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_ds = np.expand_dims(normalize(np.array(image_ds), axis=1),-1)\n",
    "image_ds = np.array(image_ds)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbe04ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ds = np.expand_dims((np.array(mask_ds)),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac92af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_ds = np.expand_dims(np.array(inst_ds), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mask_ds.shape)\n",
    "print(inst_ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(inst_ds)\n",
    "#plt.imshow(inst_ds[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9aa944",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask_ds[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a8adea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.model_selection import iterative_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f8872c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(40)\n",
    "X_train, X_test, bin_train, bin_test, inst_train, inst_test = train_test_split(image_ds, mask_ds, inst_ds, test_size = 0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf3abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, bin_train, bin_test = train_test_split(image_ds, mask_ds, test_size = 0.10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71f9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2be2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_cat = to_categorical(inst_train, num_classes=3)\n",
    "inst_train_cat = y_train_cat.reshape((inst_train.shape[0], inst_train.shape[1], inst_train.shape[2], 3), dtype=np.float32)\n",
    "\n",
    "y_test_cat = to_categorical(inst_test, num_classes=3)\n",
    "inst_test_cat = y_test_cat.reshape((inst_test.shape[0], inst_test.shape[1], inst_test.shape[2], 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([0.,1.,2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7def9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique([0.,1.,2.]),\n",
    "                                                 train_masks_reshaped_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27501779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0528af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check, view few mages\n",
    "import random\n",
    "import numpy as np\n",
    "image_number = random.randint(0, len(X_train))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(131)\n",
    "plt.imshow(np.reshape(X_train[image_number], (256, 512,3)))\n",
    "plt.subplot(132)\n",
    "plt.imshow(np.reshape(bin_train[image_number], (256, 512)), cmap='gray')\n",
    "plt.subplot(133)\n",
    "plt.imshow(np.reshape(inst_train[image_number], (256, 512)), cmap='gray')\n",
    "plt.show()\n",
    "print(np.unique(bin_train[image_number]))\n",
    "print(np.unique(inst_train[image_number]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5cd9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_loss(instance_label, net_out):\n",
    "    \n",
    "    k_instance = 1.0\n",
    "    k_dist = 1.0\n",
    "\n",
    "    \n",
    "    inst_seg = net_out\n",
    "    #instance_label = tf.squeeze(instance_label, [0])\n",
    "    # print('net_inst shape: {}'.format(inst_seg.shape))\n",
    "    # print('instance label shape: {}'.format(instance_label.shape))\n",
    "    _, var_loss, dist_loss, reg_loss = discriminative_loss(inst_seg, instance_label, 4, (256, 512))\n",
    "\n",
    "    \n",
    "    var_loss = var_loss * k_instance\n",
    "    dist_loss = dist_loss * k_dist\n",
    "    inst_loss = var_loss + dist_loss\n",
    "\n",
    "    # print(inst_loss.dtype)\n",
    "\n",
    "    return inst_loss\n",
    "\n",
    "def binary_loss(binary_label, net_out):\n",
    "    #binary_label = tf.squeeze(binary_label, [0])\n",
    "    # print('binary label shape: {}'.format(binary_label.shape))\n",
    "    k_binary = 10.0\n",
    "    binary_seg = net_out\n",
    "    # print('net_bin label shape: {}'.format(binary_seg.shape))\n",
    "    loss_fn = BinaryCrossentropy(from_logits=True)\n",
    "    #print(loss_fn)\n",
    "    #print(binary_label)\n",
    "    bin_loss = loss_fn(binary_seg, binary_label)\n",
    "    bin_loss = bin_loss * k_binary\n",
    "    bin_loss = tf.cast(bin_loss, dtype=tf.float32)\n",
    "    # print(bin_loss.dtype)\n",
    "    \n",
    "    return bin_loss\n",
    "\n",
    "def total_loss(net_out, binary_label, instance_label):\n",
    "    bin_loss = binary_loss(binary_label, net_out)\n",
    "    inst_loss = instance_loss(instance_label, net_out)\n",
    "    return bin_loss + inst_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def discriminative_loss_single(prediction, correct_label, feature_dim, label_shape, \n",
    "                            delta_v=0.5, delta_d=3, param_var=1.0, param_dist=1.0, param_reg=0.0):\n",
    "    \n",
    "    ''' Discriminative loss for a single prediction/label pair.\n",
    "    :param prediction: inference of network\n",
    "    :param correct_label: instance label\n",
    "    :feature_dim: feature dimension of prediction\n",
    "    :param label_shape: shape of label\n",
    "    :param delta_v: cutoff variance distance\n",
    "    :param delta_d: curoff cluster distance\n",
    "    :param param_var: weight for intra cluster variance\n",
    "    :param param_dist: weight for inter cluster distances\n",
    "    :param param_reg: weight regularization\n",
    "    '''\n",
    "\n",
    "    ### Reshape so pixels are aligned along a vector\n",
    "    # print(correct_label.shape)\n",
    "    correct_label = tf.reshape(correct_label, [label_shape[1]*label_shape[0]])\n",
    "    # print(correct_label.shape)\n",
    "    # print(prediction.shape)\n",
    "    reshaped_pred = tf.reshape(prediction, [label_shape[1]*label_shape[0], feature_dim])\n",
    "\n",
    "    correct_label = tf.cast(correct_label, dtype=tf.float32)\n",
    "    reshaped_pred = tf.cast(reshaped_pred, dtype=tf.float32)\n",
    "    ### Count instances\n",
    "    unique_labels, unique_id, counts = tf.unique_with_counts(correct_label)\n",
    "    counts = tf.cast(counts, tf.float32)\n",
    "    num_instances = tf.size(unique_labels)\n",
    "    # print('num instances: {}'.format(unique_id))\n",
    "\n",
    "    segmented_sum = tf.math.unsorted_segment_sum(reshaped_pred, unique_id, num_instances)\n",
    "    segmented_sum = tf.cast(segmented_sum, dtype=tf.float32)\n",
    "    mu = tf.math.divide(segmented_sum, tf.reshape(counts, (-1, 1)))\n",
    "    mu_expand = tf.gather(mu, unique_id)\n",
    "\n",
    "    ### Calculate l_var\n",
    "    distance = tf.subtract(mu_expand, reshaped_pred)\n",
    "    distance = tf.norm(distance, axis=1)\n",
    "    distance = tf.subtract(distance, delta_v)\n",
    "    distance = tf.clip_by_value(distance, 0., distance)\n",
    "    distance = tf.square(distance)\n",
    "\n",
    "    l_var = tf.math.unsorted_segment_sum(distance, unique_id, num_instances)\n",
    "    l_var = tf.math.divide(l_var, counts)\n",
    "    l_var = tf.reduce_sum(l_var)\n",
    "    l_var = tf.math.divide(l_var, tf.cast(num_instances, tf.float32))\n",
    "    \n",
    "    ### Calculate l_dist\n",
    "    \n",
    "    # Get distance for each pair of clusters like this:\n",
    "    #   mu_1 - mu_1\n",
    "    #   mu_2 - mu_1\n",
    "    #   mu_3 - mu_1\n",
    "    #   mu_1 - mu_2\n",
    "    #   mu_2 - mu_2\n",
    "    #   mu_3 - mu_2\n",
    "    #   mu_1 - mu_3\n",
    "    #   mu_2 - mu_3\n",
    "    #   mu_3 - mu_3\n",
    "\n",
    "    mu_interleaved_rep = tf.tile(mu, [num_instances, 1])\n",
    "    mu_band_rep = tf.tile(mu, [1, num_instances])\n",
    "    mu_band_rep = tf.reshape(mu_band_rep, (num_instances*num_instances, feature_dim))\n",
    "\n",
    "    mu_diff = tf.subtract(mu_band_rep, mu_interleaved_rep)\n",
    "    \n",
    "    # Filter out zeros from same cluster subtraction\n",
    "    intermediate_tensor = tf.reduce_sum(tf.abs(mu_diff),axis=1)\n",
    "    zero_vector = tf.zeros(1, dtype=tf.float32)\n",
    "    bool_mask = tf.not_equal(intermediate_tensor, zero_vector)\n",
    "    mu_diff_bool = tf.boolean_mask(mu_diff, bool_mask)\n",
    "\n",
    "    mu_norm = tf.norm(mu_diff_bool, axis=1)\n",
    "    mu_norm = tf.subtract(2.*delta_d, mu_norm)\n",
    "    mu_norm = tf.clip_by_value(mu_norm, 0., mu_norm)\n",
    "    mu_norm = tf.square(mu_norm)\n",
    "\n",
    "    l_dist = tf.reduce_mean(mu_norm)\n",
    "\n",
    "    ### Calculate l_reg\n",
    "    l_reg = tf.reduce_mean(tf.norm(mu, axis=1))\n",
    "\n",
    "    param_scale = 1.\n",
    "    l_var = param_var * l_var\n",
    "    l_dist = param_dist * l_dist\n",
    "    l_reg = param_reg * l_reg\n",
    "\n",
    "    loss = param_scale*(l_var + l_dist + l_reg)\n",
    "    \n",
    "    return loss, l_var, l_dist, l_reg\n",
    "\n",
    "def discriminative_loss(prediction, correct_label, feature_dim, image_shape, \n",
    "                delta_v=0.5, delta_d=1.5, param_var=1.5, param_dist=1.0, param_reg=1.0):\n",
    "    ''' Iterate over a batch of prediction/label and cumulate loss\n",
    "    :return: discriminative loss and its three components\n",
    "    '''\n",
    "    def cond(label, batch, out_loss, out_var, out_dist, out_reg, i):\n",
    "        return tf.math.less(i, tf.shape(batch)[0])\n",
    "\n",
    "    def body(label, batch, out_loss, out_var, out_dist, out_reg, i):\n",
    "        disc_loss, l_var, l_dist, l_reg = discriminative_loss_single(prediction[i], correct_label[i], feature_dim, image_shape, \n",
    "                        delta_v, delta_d, param_var, param_dist, param_reg)\n",
    "\n",
    "        out_loss = out_loss.write(i, disc_loss)\n",
    "        out_var = out_var.write(i, l_var)\n",
    "        out_dist = out_dist.write(i, l_dist)\n",
    "        out_reg = out_reg.write(i, l_reg)\n",
    "\n",
    "        return label, batch, out_loss, out_var, out_dist, out_reg, i + 1\n",
    "\n",
    "    # TensorArray is a data structure that support dynamic writing\n",
    "    output_ta_loss = tf.TensorArray(dtype=tf.float32,\n",
    "                   size=0,\n",
    "                   dynamic_size=True)\n",
    "    output_ta_var = tf.TensorArray(dtype=tf.float32,\n",
    "                   size=0,\n",
    "                   dynamic_size=True)\n",
    "    output_ta_dist = tf.TensorArray(dtype=tf.float32,\n",
    "                   size=0,\n",
    "                   dynamic_size=True)\n",
    "    output_ta_reg = tf.TensorArray(dtype=tf.float32,\n",
    "                   size=0,\n",
    "                   dynamic_size=True)\n",
    "    # print(tf.shape(prediction))\n",
    "    # print(tf.shape(correct_label))\n",
    "    _, _, out_loss_op, out_var_op, out_dist_op, out_reg_op, _  = tf.while_loop(cond, body, [correct_label, \n",
    "                                                        prediction, \n",
    "                                                        output_ta_loss, \n",
    "                                                        output_ta_var, \n",
    "                                                        output_ta_dist, \n",
    "                                                        output_ta_reg, \n",
    "                                                        0])\n",
    "    out_loss_op = out_loss_op.stack()\n",
    "    out_var_op = out_var_op.stack()\n",
    "    out_dist_op = out_dist_op.stack()\n",
    "    out_reg_op = out_reg_op.stack()\n",
    "    \n",
    "    disc_loss = tf.reduce_mean(out_loss_op)\n",
    "    l_var = tf.reduce_mean(out_var_op)\n",
    "    l_dist = tf.reduce_mean(out_dist_op)\n",
    "    l_reg = tf.reduce_mean(out_reg_op)\n",
    "\n",
    "    return disc_loss, l_var, l_dist, l_reg\n",
    "\n",
    "# pred = tf.ones((1, 720, 1280, 4), tf.float32)\n",
    "\n",
    "# pred = tf.constant(([[[0,1,1,0],[0,0,0,0],[0,0,0,0]], [[0,0,0,0],[0,0,0,0],[0,2,2,2]]]), tf.float32)\n",
    "# pred = tf.expand_dims(pred, axis=0)\n",
    "# img = cv2.imread('D:\\\\Project\\\\IrohXu_LaneDetection\\\\data\\\\training_data_example\\\\gt_image_instance\\\\0000.png')\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# label = tf.convert_to_tensor(img, tf.float32)\n",
    "\n",
    "# label = tf.constant([[0,1,1,0],[0,0,0,0],[0,2,2,2]], tf.float32)\n",
    "# label = tf.expand_dims(label, axis=0)\n",
    "\n",
    "# disc_loss, l_var, l_dist, l_reg = compute_loss(pred, label, label)\n",
    "# # print('disc_loss: {}'.format(disc_loss))\n",
    "# # print('l_var: {}'.format(l_var))\n",
    "# # print('l_dist: {}'.format(l_dist))\n",
    "# # print('l_reg: {}'.format(l_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0521908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_loss(correct_label, prediction):\n",
    "    k_instance = 0.3;\n",
    "    k_dist = 1.0;\n",
    "    \n",
    "    _, var_loss, dist_loss, reg_loss = discriminative_loss(prediction, correct_label, 4, (256,512))\n",
    "    \n",
    "    inst_loss = var_loss*k_instance + dist_loss*k_dist\n",
    "    \n",
    "    return inst_loss\n",
    "\n",
    "def discriminative_loss_single(\n",
    "        prediction,\n",
    "        correct_label,\n",
    "        feature_dim,\n",
    "        label_shape,\n",
    "        delta_v,\n",
    "        delta_d,\n",
    "        param_var,\n",
    "        param_dist,\n",
    "        param_reg):\n",
    "    \"\"\"\n",
    "    discriminative loss\n",
    "    :param prediction: inference of network\n",
    "    :param correct_label: instance label\n",
    "    :param feature_dim: feature dimension of prediction\n",
    "    :param label_shape: shape of label\n",
    "    :param delta_v: cut off variance distance\n",
    "    :param delta_d: cut off cluster distance\n",
    "    :param param_var: weight for intra cluster variance\n",
    "    :param param_dist: weight for inter cluster distances\n",
    "    :param param_reg: weight regularization\n",
    "    \"\"\"\n",
    "    correct_label = tf.reshape(\n",
    "        correct_label, [label_shape[1] * label_shape[0]]\n",
    "    )\n",
    "    reshaped_pred = tf.reshape(\n",
    "        prediction, [label_shape[1] * label_shape[0], feature_dim]\n",
    "    )\n",
    "\n",
    "    # calculate instance nums\n",
    "    unique_labels, unique_id, counts = tf.unique_with_counts(correct_label)\n",
    "    counts = tf.cast(counts, tf.float32)\n",
    "    num_instances = tf.size(unique_labels)\n",
    "\n",
    "    # calculate instance pixel embedding mean vec\n",
    "    segmented_sum = tf.math.unsorted_segment_sum(\n",
    "        reshaped_pred, unique_id, num_instances)\n",
    "    mu = tf.math.divide(segmented_sum, tf.reshape(counts, (-1, 1)))\n",
    "    mu_expand = tf.gather(mu, unique_id)\n",
    "\n",
    "    distance = tf.norm(tf.subtract(mu_expand, reshaped_pred), axis=1, ord=1)\n",
    "    distance = tf.subtract(distance, delta_v)\n",
    "    distance = tf.clip_by_value(distance, 0., distance)\n",
    "    distance = tf.square(distance)\n",
    "\n",
    "    l_var = tf.math.unsorted_segment_sum(distance, unique_id, num_instances)\n",
    "    l_var = tf.math.divide(l_var, counts)\n",
    "    l_var = tf.reduce_sum(l_var)\n",
    "    l_var = tf.math.divide(l_var, tf.cast(num_instances, tf.float32))\n",
    "\n",
    "    mu_interleaved_rep = tf.tile(mu, [num_instances, 1])\n",
    "    mu_band_rep = tf.tile(mu, [1, num_instances])\n",
    "    mu_band_rep = tf.reshape(\n",
    "        mu_band_rep,\n",
    "        (num_instances *\n",
    "         num_instances,\n",
    "         feature_dim))\n",
    "\n",
    "    mu_diff = tf.subtract(mu_band_rep, mu_interleaved_rep)\n",
    "\n",
    "    intermediate_tensor = tf.reduce_sum(tf.abs(mu_diff), axis=1)\n",
    "    zero_vector = tf.zeros(1, dtype=tf.float32)\n",
    "    bool_mask = tf.not_equal(intermediate_tensor, zero_vector)\n",
    "    mu_diff_bool = tf.boolean_mask(mu_diff, bool_mask)\n",
    "\n",
    "    mu_norm = tf.norm(mu_diff_bool, axis=1, ord=1)\n",
    "    mu_norm = tf.subtract(2. * delta_d, mu_norm)\n",
    "    mu_norm = tf.clip_by_value(mu_norm, 0., mu_norm)\n",
    "    mu_norm = tf.square(mu_norm)\n",
    "\n",
    "    l_dist = tf.math.reduce_mean(mu_norm)\n",
    "\n",
    "    l_reg = tf.math.reduce_mean(tf.norm(mu, axis=1, ord=1))\n",
    "\n",
    "    param_scale = 1.\n",
    "    l_var = param_var * l_var\n",
    "    l_dist = param_dist * l_dist\n",
    "    l_reg = param_reg * l_reg\n",
    "\n",
    "    loss = param_scale * (l_var + l_dist + l_reg)\n",
    "\n",
    "    return loss, l_var, l_dist, l_reg\n",
    "\n",
    "\n",
    "def discriminative_loss(prediction, correct_label, feature_dim, image_shape,\n",
    "                        delta_v=0.5, delta_d=1.5, param_var=1.0, param_dist=1.0, param_reg=0.001):\n",
    "    \"\"\"\n",
    "    :return: discriminative loss and its three components\n",
    "    \"\"\"\n",
    "\n",
    "    def cond(label, batch, out_loss, out_var, out_dist, out_reg, i):\n",
    "        return tf.less(i, tf.shape(batch)[0])\n",
    "\n",
    "    def body(label, batch, out_loss, out_var, out_dist, out_reg, i):\n",
    "        disc_loss, l_var, l_dist, l_reg = discriminative_loss_single(\n",
    "            prediction[i], correct_label[i], feature_dim, image_shape, delta_v, delta_d, param_var, param_dist, param_reg)\n",
    "\n",
    "        out_loss = out_loss.write(i, disc_loss)\n",
    "        out_var = out_var.write(i, l_var)\n",
    "        out_dist = out_dist.write(i, l_dist)\n",
    "        out_reg = out_reg.write(i, l_reg)\n",
    "\n",
    "        return label, batch, out_loss, out_var, out_dist, out_reg, i + 1\n",
    "\n",
    "    # TensorArray is a data structure that support dynamic writing\n",
    "    output_ta_loss = tf.TensorArray(\n",
    "        dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    output_ta_var = tf.TensorArray(\n",
    "        dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    output_ta_dist = tf.TensorArray(\n",
    "        dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    output_ta_reg = tf.TensorArray(\n",
    "        dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "    _, _, out_loss_op, out_var_op, out_dist_op, out_reg_op, _ = tf.while_loop(\n",
    "        cond, body, [\n",
    "            correct_label, prediction, output_ta_loss, output_ta_var, output_ta_dist, output_ta_reg, 0])\n",
    "    out_loss_op = out_loss_op.stack()\n",
    "    out_var_op = out_var_op.stack()\n",
    "    out_dist_op = out_dist_op.stack()\n",
    "    out_reg_op = out_reg_op.stack()\n",
    "\n",
    "    disc_loss = tf.math.reduce_mean(out_loss_op)\n",
    "    l_var = tf.math.reduce_mean(out_var_op)\n",
    "    l_dist = tf.math.reduce_mean(out_dist_op)\n",
    "    l_reg = tf.math.reduce_mean(out_reg_op)\n",
    "\n",
    "    return disc_loss, l_var, l_dist, l_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88604cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Rescaling, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, ReLU, Lambda\n",
    "\n",
    "################################################################\n",
    "def simple_unet_model(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS):\n",
    "#Build the model\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    #s = Lambda(lambda x: x / 255.0)(inputs)   #No need for this if we normalize our inputs beforehand\n",
    "    s = inputs\n",
    "\n",
    "    #Contraction path\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "    c1 = Dropout(0.1)(c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.1)(c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "     \n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "     \n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "     \n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    \n",
    "    #Expansive path bin\n",
    "    u6_bin = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6_bin = concatenate([u6_bin, c4])\n",
    "    c6_bin = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6_bin)\n",
    "    c6_bin = Dropout(0.2)(c6_bin)\n",
    "    c6_bin = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6_bin)\n",
    "     \n",
    "    u7_bin = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6_bin)\n",
    "    u7_bin = concatenate([u7_bin, c3])\n",
    "    c7_bin = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7_bin)\n",
    "    c7_bin = Dropout(0.2)(c7_bin)\n",
    "    c7_bin = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7_bin)\n",
    "     \n",
    "    u8_bin = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7_bin)\n",
    "    u8_bin = concatenate([u8_bin, c2])\n",
    "    c8_bin = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8_bin)\n",
    "    c8_bin = Dropout(0.1)(c8_bin)\n",
    "    c8_bin = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8_bin)\n",
    "     \n",
    "    u9_bin = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8_bin)\n",
    "    u9_bin = concatenate([u9_bin, c1], axis=3)\n",
    "    c9_bin = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9_bin)\n",
    "    c9_bin = Dropout(0.1)(c9_bin)\n",
    "    c9_bin = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9_bin)\n",
    "     \n",
    "    bin_seg = Conv2D(1, (1, 1), activation='sigmoid', name='bin_seg')(c9_bin)\n",
    "    \n",
    "    #Expansive path inst\n",
    "    u6_inst = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6_inst = concatenate([u6_inst, c4])\n",
    "    c6_inst = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6_inst)\n",
    "    c6_inst = Dropout(0.2)(c6_inst)\n",
    "    c6_inst = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6_inst)\n",
    "     \n",
    "    u7_inst = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6_inst)\n",
    "    u7_inst = concatenate([u7_inst, c3])\n",
    "    c7_inst = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7_inst)\n",
    "    c7_inst = Dropout(0.2)(c7_inst)\n",
    "    c7_inst = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7_inst)\n",
    "     \n",
    "    u8_inst = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7_inst)\n",
    "    u8_inst = concatenate([u8_inst, c2])\n",
    "    c8_inst = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8_inst)\n",
    "    c8_inst = Dropout(0.1)(c8_inst)\n",
    "    c8_inst = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8_inst)\n",
    "     \n",
    "    u9_inst = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8_inst)\n",
    "    u9_inst = concatenate([u9_inst, c1], axis=3)\n",
    "    c9_inst = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9_inst)\n",
    "    c9_inst = Dropout(0.1)(c9_inst)\n",
    "    c9_inst = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9_inst)\n",
    "    \n",
    "    c9_inst = BatchNormalization()(c9_inst)\n",
    "    c9_inst = ReLU()(c9_inst)\n",
    "    inst_seg = Conv2D(4, (1, 1), activation='sigmoid', name='inst_seg')(c9_inst)\n",
    "     \n",
    "    model = Model(inputs=[inputs], outputs=[bin_seg, inst_seg])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08da086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = image_ds.shape[1]\n",
    "IMG_WIDTH = image_ds.shape[2]\n",
    "IMG_CHANNELS = image_ds.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff2cd316",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 512\n",
    "IMG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0be95b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 256, 512, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4213a225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 256, 512, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9339c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 512 3\n"
     ]
    }
   ],
   "source": [
    "print(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eebc15bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 512, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 256, 512, 16  448         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256, 512, 16  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 256, 512, 16  2320        ['dropout[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 128, 256, 16  0           ['conv2d_1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 128, 256, 32  4640        ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128, 256, 32  0           ['conv2d_2[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 128, 256, 32  9248        ['dropout_1[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 64, 128, 32)  0          ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 64, 128, 64)  18496       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64, 128, 64)  0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 64, 128, 64)  36928       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 32, 64, 64)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 64, 128)  73856       ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 32, 64, 128)  0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 64, 128)  147584      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 16, 32, 128)  0          ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 32, 256)  295168      ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 16, 32, 256)  0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 32, 256)  590080      ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 32, 64, 128)  131200     ['conv2d_9[0][0]']               \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 32, 64, 256)  0           ['conv2d_transpose_4[0][0]',     \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 32, 64, 128)  131200     ['conv2d_9[0][0]']               \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 32, 64, 128)  295040      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 64, 256)  0           ['conv2d_transpose[0][0]',       \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 32, 64, 128)  0           ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 64, 128)  295040      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 32, 64, 128)  147584      ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 32, 64, 128)  0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 64, 128, 64)  32832      ['conv2d_19[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 64, 128)  147584      ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 64, 128, 128  0           ['conv2d_transpose_5[0][0]',     \n",
      "                                )                                 'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 64, 128, 64)  32832      ['conv2d_11[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 64, 128, 64)  73792       ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 128, 128  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                )                                 'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 64, 128, 64)  0           ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 64, 128, 64)  73792       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 64, 128, 64)  36928       ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 64, 128, 64)  0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 128, 256, 32  8224       ['conv2d_21[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 64, 128, 64)  36928       ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 128, 256, 64  0           ['conv2d_transpose_6[0][0]',     \n",
      "                                )                                 'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 128, 256, 32  8224       ['conv2d_13[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 128, 256, 32  18464       ['concatenate_6[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 256, 64  0           ['conv2d_transpose_2[0][0]',     \n",
      "                                )                                 'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 128, 256, 32  0           ['conv2d_22[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 128, 256, 32  18464       ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 128, 256, 32  9248        ['dropout_11[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 128, 256, 32  0           ['conv2d_14[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 256, 512, 16  2064       ['conv2d_23[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 128, 256, 32  9248        ['dropout_7[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 256, 512, 32  0           ['conv2d_transpose_7[0][0]',     \n",
      "                                )                                 'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 256, 512, 16  2064       ['conv2d_15[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 256, 512, 16  4624        ['concatenate_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 256, 512, 32  0           ['conv2d_transpose_3[0][0]',     \n",
      "                                )                                 'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 256, 512, 16  0           ['conv2d_24[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 256, 512, 16  4624        ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 256, 512, 16  2320        ['dropout_12[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 256, 512, 16  0           ['conv2d_16[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 256, 512, 16  64         ['conv2d_25[0][0]']              \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 256, 512, 16  2320        ['dropout_8[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 256, 512, 16  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bin_seg (Conv2D)               (None, 256, 512, 1)  17          ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " inst_seg (Conv2D)              (None, 256, 512, 4)  68          ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,703,557\n",
      "Trainable params: 2,703,525\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.random.set_seed(40)\n",
    "tf.autograph.set_verbosity(10)\n",
    "model = simple_unet_model(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ce63091",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./output/Model_VariousWeather/lanenet_4lane2_5_model2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "119c96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_loss import BinaryFocalLoss\n",
    "from tensorflow.keras.callbacks import TerminateOnNaN\n",
    "import datetime\n",
    "LR = 1e-4\n",
    "EPOCHS = 300\n",
    "BS = 16\n",
    "LossWeights = [1,1]\n",
    "term = TerminateOnNaN()\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "log_dir = \"./logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "tf.random.set_seed(40)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR, decay=LR/EPOCHS)\n",
    "model.compile(optimizer=optimizer,\n",
    "                  loss=[BinaryFocalLoss(gamma=2), instance_loss],\n",
    "                  loss_weights=LossWeights,\n",
    "                  metrics={'bin_seg': tf.keras.metrics.MeanIoU(num_classes=2), 'inst_seg': \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89380372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_loss import BinaryFocalLoss\n",
    "LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BS = 4\n",
    "LossWeights = [1,1]\n",
    "\n",
    "with tf.device('device:GPU:0'):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LR, decay=LR/EPOCHS)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=BinaryFocalLoss(gamma=2),\n",
    "                  metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da3ce046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x0000026E81D04708>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x0000026E81D04708>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x0000026E81D04948>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(5100,) dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x0000026E81D04948>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x0000026E81D04798>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(5100, 256, 512, 3) dtype=float32>, (<tf.Tensor 'args_2:0' shape=(5100, 256, 512, 1) dtype=uint8>, <tf.Tensor 'args_3:0' shape=(5100, 256, 512, 1) dtype=float32>)))\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x0000026E81D04798>: DoNotConvert rule for keras\n",
      "Epoch 1/300\n",
      "INFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.train_function at 0x0000026E81D04E58>\n",
      "    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000026E83A1A748>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:<function Model.make_train_function.<locals>.train_function at 0x0000026E81D04E58> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function Model.make_train_function.<locals>.train_function at 0x0000026E81D04E58>:\n",
      "\n",
      "def train_function(iterator):\n",
      "  \"\"\"Runs a training execution with one step.\"\"\"\n",
      "  return step_function(self, iterator)\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <function Model.make_train_function.<locals>.train_function at 0x0000026E81D04E58>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__train_function(iterator):\n",
      "    'Runs a training execution with one step.'\n",
      "    with ag__.FunctionScope('train_function', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__train_function at 0x0000026E8407D558> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__train_function at 0x0000026E8407D558> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__train_function at 0x0000026E8407D558> with\n",
      "    iterator: <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000026E83A1A748>\n",
      "\n",
      "INFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.step_function at 0x0000026E81D049D8>\n",
      "    args: (<keras.engine.functional.Functional object at 0x0000026E83961C08>, <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000026E83A1A748>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function Model.make_train_function.<locals>.step_function at 0x0000026E81D049D8>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x0000026E8404DEE8>\n",
      "    args: ((<tf.Tensor 'IteratorGetNext:0' shape=(None, 256, 512, 3) dtype=float32>, (<tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>)),)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x0000026E8404DEE8>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <bound method BinaryFocalLoss.call of <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, <tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x0000026E83E0B048>: default rule\n",
      "INFO:tensorflow:Not allowed: <class 'focal_loss._binary_focal_loss.BinaryFocalLoss'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method BinaryFocalLoss.call of <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>>: default rule\n",
      "INFO:tensorflow:<bound method BinaryFocalLoss.call of <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <bound method BinaryFocalLoss.call of <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>>:\n",
      "\n",
      "def call(self, y_true, y_pred):\n",
      "    \"\"\"Compute the per-example focal loss.\n",
      "\n",
      "        This method simply calls :meth:`~focal_loss.binary_focal_loss` with the\n",
      "        appropriate arguments.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : tensor-like\n",
      "            Binary (0 or 1) class labels.\n",
      "\n",
      "        y_pred : tensor-like\n",
      "            Either probabilities for the positive class or logits for the\n",
      "            positive class, depending on the `from_logits` attribute. The shapes\n",
      "            of `y_true` and `y_pred` should be broadcastable.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`tf.Tensor`\n",
      "            The per-example focal loss. Reduction to a scalar is handled by\n",
      "            this layer's :meth:`~focal_loss.BinaryFocalLoss.__call__` method.\n",
      "        \"\"\"\n",
      "    return binary_focal_loss(y_true=y_true, y_pred=y_pred, gamma=self.gamma,\n",
      "    pos_weight=self.pos_weight,\n",
      "    from_logits=self.from_logits,\n",
      "    label_smoothing=self.label_smoothing)\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <bound method BinaryFocalLoss.call of <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__call(self, y_true, y_pred):\n",
      "    \"Compute the per-example focal loss.\\n\\n        This method simply calls :meth:`~focal_loss.binary_focal_loss` with the\\n        appropriate arguments.\\n\\n        Parameters\\n        ----------\\n        y_true : tensor-like\\n            Binary (0 or 1) class labels.\\n\\n        y_pred : tensor-like\\n            Either probabilities for the positive class or logits for the\\n            positive class, depending on the `from_logits` attribute. The shapes\\n            of `y_true` and `y_pred` should be broadcastable.\\n\\n        Returns\\n        -------\\n        :class:`tf.Tensor`\\n            The per-example focal loss. Reduction to a scalar is handled by\\n            this layer's :meth:`~focal_loss.BinaryFocalLoss.__call__` method.\\n        \"\n",
      "    with ag__.FunctionScope('call', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.converted_call(ag__.ld(binary_focal_loss), (), dict(y_true=ag__.ld(y_true), y_pred=ag__.ld(y_pred), gamma=ag__.ld(self).gamma, pos_weight=ag__.ld(self).pos_weight, from_logits=ag__.ld(self).from_logits, label_smoothing=ag__.ld(self).label_smoothing), fscope)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x0000026E851A1AF8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x0000026E851A1AF8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x0000026E851A1AF8> with\n",
      "    self: <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>\n",
      "    y_true: Tensor(\"IteratorGetNext:1\", shape=(None, 256, 512, 1), dtype=uint8)\n",
      "    y_pred: Tensor(\"model/bin_seg/Sigmoid:0\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function binary_focal_loss at 0x0000026E8399C5E8>\n",
      "    args: ()\n",
      "    kwargs: {'y_true': <tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, 'y_pred': <tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>, 'gamma': 2.0, 'pos_weight': None, 'from_logits': False, 'label_smoothing': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399C5E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function binary_focal_loss at 0x0000026E8399C5E8>: default rule\n",
      "INFO:tensorflow:<function binary_focal_loss at 0x0000026E8399C5E8> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function binary_focal_loss at 0x0000026E8399C5E8>:\n",
      "\n",
      "def binary_focal_loss(y_true, y_pred, gamma, *, pos_weight=None,\n",
      "                      from_logits=False, label_smoothing=None):\n",
      "    r\"\"\"Focal loss function for binary classification.\n",
      "\n",
      "    This loss function generalizes binary cross-entropy by introducing a\n",
      "    hyperparameter :math:`\\gamma` (gamma), called the *focusing parameter*,\n",
      "    that allows hard-to-classify examples to be penalized more heavily relative\n",
      "    to easy-to-classify examples.\n",
      "\n",
      "    The focal loss [1]_ is defined as\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        L(y, \\hat{p})\n",
      "        = -\\alpha y \\left(1 - \\hat{p}\\right)^\\gamma \\log(\\hat{p})\n",
      "        - (1 - y) \\hat{p}^\\gamma \\log(1 - \\hat{p})\n",
      "\n",
      "    where\n",
      "\n",
      "    *   :math:`y \\in \\{0, 1\\}` is a binary class label,\n",
      "    *   :math:`\\hat{p} \\in [0, 1]` is an estimate of the probability of the\n",
      "        positive class,\n",
      "    *   :math:`\\gamma` is the *focusing parameter* that specifies how much\n",
      "        higher-confidence correct predictions contribute to the overall loss\n",
      "        (the higher the :math:`\\gamma`, the higher the rate at which\n",
      "        easy-to-classify examples are down-weighted).\n",
      "    *   :math:`\\alpha` is a hyperparameter that governs the trade-off between\n",
      "        precision and recall by weighting errors for the positive class up or\n",
      "        down (:math:`\\alpha=1` is the default, which is the same as no\n",
      "        weighting),\n",
      "\n",
      "    The usual weighted binary cross-entropy loss is recovered by setting\n",
      "    :math:`\\gamma = 0`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : tensor-like\n",
      "        Binary (0 or 1) class labels.\n",
      "\n",
      "    y_pred : tensor-like\n",
      "        Either probabilities for the positive class or logits for the positive\n",
      "        class, depending on the `from_logits` parameter. The shapes of `y_true`\n",
      "        and `y_pred` should be broadcastable.\n",
      "\n",
      "    gamma : float\n",
      "        The focusing parameter :math:`\\gamma`. Higher values of `gamma` make\n",
      "        easy-to-classify examples contribute less to the loss relative to\n",
      "        hard-to-classify examples. Must be non-negative.\n",
      "\n",
      "    pos_weight : float, optional\n",
      "        The coefficient :math:`\\alpha` to use on the positive examples. Must be\n",
      "        non-negative.\n",
      "\n",
      "    from_logits : bool, optional\n",
      "        Whether `y_pred` contains logits or probabilities.\n",
      "\n",
      "    label_smoothing : float, optional\n",
      "        Float in [0, 1]. When 0, no smoothing occurs. When positive, the binary\n",
      "        ground truth labels `y_true` are squeezed toward 0.5, with larger values\n",
      "        of `label_smoothing` leading to label values closer to 0.5.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    :class:`tf.Tensor`\n",
      "        The focal loss for each example (assuming `y_true` and `y_pred` have the\n",
      "        same shapes). In general, the shape of the output is the result of\n",
      "        broadcasting the shapes of `y_true` and `y_pred`.\n",
      "\n",
      "    Warnings\n",
      "    --------\n",
      "    This function does not reduce its output to a scalar, so it cannot be passed\n",
      "    to :meth:`tf.keras.Model.compile` as a `loss` argument. Instead, use the\n",
      "    wrapper class :class:`~focal_loss.BinaryFocalLoss`.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "\n",
      "    This function computes the per-example focal loss between a label and\n",
      "    prediction tensor:\n",
      "\n",
      "    >>> import numpy as np\n",
      "    >>> from focal_loss import binary_focal_loss\n",
      "    >>> loss = binary_focal_loss([0, 1, 1], [0.1, 0.7, 0.9], gamma=2)\n",
      "    >>> np.set_printoptions(precision=3)\n",
      "    >>> print(loss.numpy())\n",
      "    [0.001 0.032 0.001]\n",
      "\n",
      "    Below is a visualization of the focal loss between the positive class and\n",
      "    predicted probabilities between 0 and 1. Note that as :math:`\\gamma`\n",
      "    increases, the losses for predictions closer to 1 get smoothly pushed to 0.\n",
      "\n",
      "    .. plot::\n",
      "        :include-source:\n",
      "        :align: center\n",
      "\n",
      "        import numpy as np\n",
      "        import matplotlib.pyplot as plt\n",
      "\n",
      "        from focal_loss import binary_focal_loss\n",
      "\n",
      "        ps = np.linspace(0, 1, 100)\n",
      "        gammas = (0, 0.5, 1, 2, 5)\n",
      "\n",
      "        plt.figure()\n",
      "        for gamma in gammas:\n",
      "            loss = binary_focal_loss(1, ps, gamma=gamma)\n",
      "            label = rf'$\\gamma$={gamma}'\n",
      "            if gamma == 0:\n",
      "                label += ' (cross-entropy)'\n",
      "            plt.plot(ps, loss, label=label)\n",
      "        plt.legend(loc='best', frameon=True, shadow=True)\n",
      "        plt.xlim(0, 1)\n",
      "        plt.ylim(0, 4)\n",
      "        plt.xlabel(r'Probability of positive class $\\hat{p}$')\n",
      "        plt.ylabel('Loss')\n",
      "        plt.title(r'Plot of focal loss $L(1, \\hat{p})$ for different $\\gamma$',\n",
      "                  fontsize=14)\n",
      "        plt.show()\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    A classifier often estimates the positive class probability :math:`\\hat{p}`\n",
      "    by computing a real-valued *logit* :math:`\\hat{y} \\in \\mathbb{R}` and\n",
      "    applying the *sigmoid function* :math:`\\sigma : \\mathbb{R} \\to (0, 1)`\n",
      "    defined by\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        \\sigma(t) = \\frac{1}{1 + e^{-t}}, \\qquad (t \\in \\mathbb{R}).\n",
      "\n",
      "    That is, :math:`\\hat{p} = \\sigma(\\hat{y})`. In this case, the focal loss\n",
      "    can be written as a function of the logit :math:`\\hat{y}` instead of the\n",
      "    predicted probability :math:`\\hat{p}`:\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        L(y, \\hat{y})\n",
      "        = -\\alpha y \\left(1 - \\sigma(\\hat{y})\\right)^\\gamma\n",
      "        \\log(\\sigma(\\hat{y}))\n",
      "        - (1 - y) \\sigma(\\hat{y})^\\gamma \\log(1 - \\sigma(\\hat{y})).\n",
      "\n",
      "    This is the formula that is computed when specifying `from_logits=True`.\n",
      "    However, this formula is not very numerically stable if implemented\n",
      "    directly; for example, there are multiple log and sigmoid computations\n",
      "    involved. Instead, we use some tricks to rewrite it in the more numerically\n",
      "    stable form\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        L(y, \\hat{y})\n",
      "        = (1 - y) \\hat{p}^\\gamma \\hat{y}\n",
      "        + \\left(\\alpha y \\hat{q}^\\gamma + (1 - y) \\hat{p}^\\gamma\\right)\n",
      "        \\left(\\log(1 + e^{-|\\hat{y}|}) + \\max\\{-\\hat{y}, 0\\}\\right),\n",
      "\n",
      "    where :math:`\\hat{p} = \\sigma(\\hat{y})` and :math:`\\hat{q} = 1 - \\hat{p}`\n",
      "    denote the estimates of the probabilities of the positive and negative\n",
      "    classes, respectively.\n",
      "\n",
      "    Indeed, starting with the observations that\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        \\log(\\sigma(\\hat{y}))\n",
      "        = \\log\\left(\\frac{1}{1 + e^{-\\hat{y}}}\\right)\n",
      "        = -\\log(1 + e^{-\\hat{y}})\n",
      "\n",
      "    and\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        \\log(1 - \\sigma(\\hat{y}))\n",
      "        = \\log\\left(\\frac{e^{-\\hat{y}}}{1 + e^{-\\hat{y}}}\\right)\n",
      "        = -\\hat{y} - \\log(1 + e^{-\\hat{y}}),\n",
      "\n",
      "    we obtain\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        \\begin{aligned}\n",
      "        L(y, \\hat{y})\n",
      "        &= -\\alpha y \\hat{q}^\\gamma \\log(\\sigma(\\hat{y}))\n",
      "        - (1 - y) \\hat{p}^\\gamma \\log(1 - \\sigma(\\hat{y})) \\        &= \\alpha y \\hat{q}^\\gamma \\log(1 + e^{-\\hat{y}})\n",
      "        + (1 - y) \\hat{p}^\\gamma \\left(\\hat{y} + \\log(1 + e^{-\\hat{y}})\\right)\\        &= (1 - y) \\hat{p}^\\gamma \\hat{y}\n",
      "        + \\left(\\alpha y \\hat{q}^\\gamma + (1 - y) \\hat{p}^\\gamma\\right)\n",
      "        \\log(1 + e^{-\\hat{y}}).\n",
      "        \\end{aligned}\n",
      "\n",
      "    Note that if :math:`\\hat{y} < 0`, then the exponential term\n",
      "    :math:`e^{-\\hat{y}}` could become very large. In this case, we can instead\n",
      "    observe that\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        \\begin{align*}\n",
      "        \\log(1 + e^{-\\hat{y}})\n",
      "        &= \\log(1 + e^{-\\hat{y}}) + \\hat{y} - \\hat{y} \\        &= \\log(1 + e^{-\\hat{y}}) + \\log(e^{\\hat{y}}) - \\hat{y} \\        &= \\log(1 + e^{\\hat{y}}) - \\hat{y}.\n",
      "        \\end{align*}\n",
      "\n",
      "    Moreover, the :math:`\\hat{y} < 0` and :math:`\\hat{y} \\geq 0` cases can be\n",
      "    unified by writing\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        \\log(1 + e^{-\\hat{y}})\n",
      "        = \\log(1 + e^{-|\\hat{y}|}) + \\max\\{-\\hat{y}, 0\\}.\n",
      "\n",
      "    Thus, we arrive at the numerically stable formula shown earlier.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] T. Lin, P. Goyal, R. Girshick, K. He and P. Dollr. Focal loss for\n",
      "        dense object detection. IEEE Transactions on Pattern Analysis and\n",
      "        Machine Intelligence, 2018.\n",
      "        (`DOI <https://doi.org/10.1109/TPAMI.2018.2858826>`__)\n",
      "        (`arXiv preprint <https://arxiv.org/abs/1708.02002>`__)\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    :meth:`~focal_loss.BinaryFocalLoss`\n",
      "        A wrapper around this function that makes it a\n",
      "        :class:`tf.keras.losses.Loss`.\n",
      "    \"\"\"\n",
      "    # Validate arguments\n",
      "    gamma = check_float(gamma, name='gamma', minimum=0)\n",
      "    pos_weight = check_float(pos_weight, name='pos_weight', minimum=0,\n",
      "                             allow_none=True)\n",
      "    from_logits = check_bool(from_logits, name='from_logits')\n",
      "    label_smoothing = check_float(label_smoothing, name='label_smoothing',\n",
      "                                  minimum=0, maximum=1, allow_none=True)\n",
      "\n",
      "    # Ensure predictions are a floating point tensor; converting labels to a\n",
      "    # tensor will be done in the helper functions\n",
      "    y_pred = tf.convert_to_tensor(y_pred)\n",
      "    if not y_pred.dtype.is_floating:\n",
      "        y_pred = tf.dtypes.cast(y_pred, dtype=tf.float32)\n",
      "\n",
      "    # Delegate per-example loss computation to helpers depending on whether\n",
      "    # predictions are logits or probabilities\n",
      "    if from_logits:\n",
      "        return _binary_focal_loss_from_logits(labels=y_true, logits=y_pred,\n",
      "                                              gamma=gamma,\n",
      "                                              pos_weight=pos_weight,\n",
      "                                              label_smoothing=label_smoothing)\n",
      "    else:\n",
      "        return _binary_focal_loss_from_probs(labels=y_true, p=y_pred,\n",
      "                                             gamma=gamma, pos_weight=pos_weight,\n",
      "                                             label_smoothing=label_smoothing)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transformed <function binary_focal_loss at 0x0000026E8399C5E8>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__binary_focal_loss(y_true, y_pred, gamma, *, pos_weight=None, from_logits=None, label_smoothing=None):\n",
      "    \"Focal loss function for binary classification.\\n\\n    This loss function generalizes binary cross-entropy by introducing a\\n    hyperparameter :math:`\\\\gamma` (gamma), called the *focusing parameter*,\\n    that allows hard-to-classify examples to be penalized more heavily relative\\n    to easy-to-classify examples.\\n\\n    The focal loss [1]_ is defined as\\n\\n    .. math::\\n\\n        L(y, \\\\hat{p})\\n        = -\\\\alpha y \\\\left(1 - \\\\hat{p}\\\\right)^\\\\gamma \\\\log(\\\\hat{p})\\n        - (1 - y) \\\\hat{p}^\\\\gamma \\\\log(1 - \\\\hat{p})\\n\\n    where\\n\\n    *   :math:`y \\\\in \\\\{0, 1\\\\}` is a binary class label,\\n    *   :math:`\\\\hat{p} \\\\in [0, 1]` is an estimate of the probability of the\\n        positive class,\\n    *   :math:`\\\\gamma` is the *focusing parameter* that specifies how much\\n        higher-confidence correct predictions contribute to the overall loss\\n        (the higher the :math:`\\\\gamma`, the higher the rate at which\\n        easy-to-classify examples are down-weighted).\\n    *   :math:`\\\\alpha` is a hyperparameter that governs the trade-off between\\n        precision and recall by weighting errors for the positive class up or\\n        down (:math:`\\\\alpha=1` is the default, which is the same as no\\n        weighting),\\n\\n    The usual weighted binary cross-entropy loss is recovered by setting\\n    :math:`\\\\gamma = 0`.\\n\\n    Parameters\\n    ----------\\n    y_true : tensor-like\\n        Binary (0 or 1) class labels.\\n\\n    y_pred : tensor-like\\n        Either probabilities for the positive class or logits for the positive\\n        class, depending on the `from_logits` parameter. The shapes of `y_true`\\n        and `y_pred` should be broadcastable.\\n\\n    gamma : float\\n        The focusing parameter :math:`\\\\gamma`. Higher values of `gamma` make\\n        easy-to-classify examples contribute less to the loss relative to\\n        hard-to-classify examples. Must be non-negative.\\n\\n    pos_weight : float, optional\\n        The coefficient :math:`\\\\alpha` to use on the positive examples. Must be\\n        non-negative.\\n\\n    from_logits : bool, optional\\n        Whether `y_pred` contains logits or probabilities.\\n\\n    label_smoothing : float, optional\\n        Float in [0, 1]. When 0, no smoothing occurs. When positive, the binary\\n        ground truth labels `y_true` are squeezed toward 0.5, with larger values\\n        of `label_smoothing` leading to label values closer to 0.5.\\n\\n    Returns\\n    -------\\n    :class:`tf.Tensor`\\n        The focal loss for each example (assuming `y_true` and `y_pred` have the\\n        same shapes). In general, the shape of the output is the result of\\n        broadcasting the shapes of `y_true` and `y_pred`.\\n\\n    Warnings\\n    --------\\n    This function does not reduce its output to a scalar, so it cannot be passed\\n    to :meth:`tf.keras.Model.compile` as a `loss` argument. Instead, use the\\n    wrapper class :class:`~focal_loss.BinaryFocalLoss`.\\n\\n    Examples\\n    --------\\n\\n    This function computes the per-example focal loss between a label and\\n    prediction tensor:\\n\\n    >>> import numpy as np\\n    >>> from focal_loss import binary_focal_loss\\n    >>> loss = binary_focal_loss([0, 1, 1], [0.1, 0.7, 0.9], gamma=2)\\n    >>> np.set_printoptions(precision=3)\\n    >>> print(loss.numpy())\\n    [0.001 0.032 0.001]\\n\\n    Below is a visualization of the focal loss between the positive class and\\n    predicted probabilities between 0 and 1. Note that as :math:`\\\\gamma`\\n    increases, the losses for predictions closer to 1 get smoothly pushed to 0.\\n\\n    .. plot::\\n        :include-source:\\n        :align: center\\n\\n        import numpy as np\\n        import matplotlib.pyplot as plt\\n\\n        from focal_loss import binary_focal_loss\\n\\n        ps = np.linspace(0, 1, 100)\\n        gammas = (0, 0.5, 1, 2, 5)\\n\\n        plt.figure()\\n        for gamma in gammas:\\n            loss = binary_focal_loss(1, ps, gamma=gamma)\\n            label = rf'$\\\\gamma$={gamma}'\\n            if gamma == 0:\\n                label += ' (cross-entropy)'\\n            plt.plot(ps, loss, label=label)\\n        plt.legend(loc='best', frameon=True, shadow=True)\\n        plt.xlim(0, 1)\\n        plt.ylim(0, 4)\\n        plt.xlabel(r'Probability of positive class $\\\\hat{p}$')\\n        plt.ylabel('Loss')\\n        plt.title(r'Plot of focal loss $L(1, \\\\hat{p})$ for different $\\\\gamma$',\\n                  fontsize=14)\\n        plt.show()\\n\\n    Notes\\n    -----\\n    A classifier often estimates the positive class probability :math:`\\\\hat{p}`\\n    by computing a real-valued *logit* :math:`\\\\hat{y} \\\\in \\\\mathbb{R}` and\\n    applying the *sigmoid function* :math:`\\\\sigma : \\\\mathbb{R} \\\\to (0, 1)`\\n    defined by\\n\\n    .. math::\\n\\n        \\\\sigma(t) = \\\\frac{1}{1 + e^{-t}}, \\\\qquad (t \\\\in \\\\mathbb{R}).\\n\\n    That is, :math:`\\\\hat{p} = \\\\sigma(\\\\hat{y})`. In this case, the focal loss\\n    can be written as a function of the logit :math:`\\\\hat{y}` instead of the\\n    predicted probability :math:`\\\\hat{p}`:\\n\\n    .. math::\\n\\n        L(y, \\\\hat{y})\\n        = -\\\\alpha y \\\\left(1 - \\\\sigma(\\\\hat{y})\\\\right)^\\\\gamma\\n        \\\\log(\\\\sigma(\\\\hat{y}))\\n        - (1 - y) \\\\sigma(\\\\hat{y})^\\\\gamma \\\\log(1 - \\\\sigma(\\\\hat{y})).\\n\\n    This is the formula that is computed when specifying `from_logits=True`.\\n    However, this formula is not very numerically stable if implemented\\n    directly; for example, there are multiple log and sigmoid computations\\n    involved. Instead, we use some tricks to rewrite it in the more numerically\\n    stable form\\n\\n    .. math::\\n\\n        L(y, \\\\hat{y})\\n        = (1 - y) \\\\hat{p}^\\\\gamma \\\\hat{y}\\n        + \\\\left(\\\\alpha y \\\\hat{q}^\\\\gamma + (1 - y) \\\\hat{p}^\\\\gamma\\\\right)\\n        \\\\left(\\\\log(1 + e^{-|\\\\hat{y}|}) + \\\\max\\\\{-\\\\hat{y}, 0\\\\}\\\\right),\\n\\n    where :math:`\\\\hat{p} = \\\\sigma(\\\\hat{y})` and :math:`\\\\hat{q} = 1 - \\\\hat{p}`\\n    denote the estimates of the probabilities of the positive and negative\\n    classes, respectively.\\n\\n    Indeed, starting with the observations that\\n\\n    .. math::\\n\\n        \\\\log(\\\\sigma(\\\\hat{y}))\\n        = \\\\log\\\\left(\\\\frac{1}{1 + e^{-\\\\hat{y}}}\\\\right)\\n        = -\\\\log(1 + e^{-\\\\hat{y}})\\n\\n    and\\n\\n    .. math::\\n\\n        \\\\log(1 - \\\\sigma(\\\\hat{y}))\\n        = \\\\log\\\\left(\\\\frac{e^{-\\\\hat{y}}}{1 + e^{-\\\\hat{y}}}\\\\right)\\n        = -\\\\hat{y} - \\\\log(1 + e^{-\\\\hat{y}}),\\n\\n    we obtain\\n\\n    .. math::\\n\\n        \\\\begin{aligned}\\n        L(y, \\\\hat{y})\\n        &= -\\\\alpha y \\\\hat{q}^\\\\gamma \\\\log(\\\\sigma(\\\\hat{y}))\\n        - (1 - y) \\\\hat{p}^\\\\gamma \\\\log(1 - \\\\sigma(\\\\hat{y})) \\\\        &= \\\\alpha y \\\\hat{q}^\\\\gamma \\\\log(1 + e^{-\\\\hat{y}})\\n        + (1 - y) \\\\hat{p}^\\\\gamma \\\\left(\\\\hat{y} + \\\\log(1 + e^{-\\\\hat{y}})\\\\right)\\\\        &= (1 - y) \\\\hat{p}^\\\\gamma \\\\hat{y}\\n        + \\\\left(\\\\alpha y \\\\hat{q}^\\\\gamma + (1 - y) \\\\hat{p}^\\\\gamma\\\\right)\\n        \\\\log(1 + e^{-\\\\hat{y}}).\\n        \\\\end{aligned}\\n\\n    Note that if :math:`\\\\hat{y} < 0`, then the exponential term\\n    :math:`e^{-\\\\hat{y}}` could become very large. In this case, we can instead\\n    observe that\\n\\n    .. math::\\n\\n        \\\\begin{align*}\\n        \\\\log(1 + e^{-\\\\hat{y}})\\n        &= \\\\log(1 + e^{-\\\\hat{y}}) + \\\\hat{y} - \\\\hat{y} \\\\        &= \\\\log(1 + e^{-\\\\hat{y}}) + \\\\log(e^{\\\\hat{y}}) - \\\\hat{y} \\\\        &= \\\\log(1 + e^{\\\\hat{y}}) - \\\\hat{y}.\\n        \\\\end{align*}\\n\\n    Moreover, the :math:`\\\\hat{y} < 0` and :math:`\\\\hat{y} \\\\geq 0` cases can be\\n    unified by writing\\n\\n    .. math::\\n\\n        \\\\log(1 + e^{-\\\\hat{y}})\\n        = \\\\log(1 + e^{-|\\\\hat{y}|}) + \\\\max\\\\{-\\\\hat{y}, 0\\\\}.\\n\\n    Thus, we arrive at the numerically stable formula shown earlier.\\n\\n    References\\n    ----------\\n    .. [1] T. Lin, P. Goyal, R. Girshick, K. He and P. Dollr. Focal loss for\\n        dense object detection. IEEE Transactions on Pattern Analysis and\\n        Machine Intelligence, 2018.\\n        (`DOI <https://doi.org/10.1109/TPAMI.2018.2858826>`__)\\n        (`arXiv preprint <https://arxiv.org/abs/1708.02002>`__)\\n\\n    See Also\\n    --------\\n    :meth:`~focal_loss.BinaryFocalLoss`\\n        A wrapper around this function that makes it a\\n        :class:`tf.keras.losses.Loss`.\\n    \"\n",
      "    with ag__.FunctionScope('binary_focal_loss', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        gamma = ag__.converted_call(ag__.ld(check_float), (ag__.ld(gamma),), dict(name='gamma', minimum=0), fscope)\n",
      "        pos_weight = ag__.converted_call(ag__.ld(check_float), (ag__.ld(pos_weight),), dict(name='pos_weight', minimum=0, allow_none=True), fscope)\n",
      "        from_logits = ag__.converted_call(ag__.ld(check_bool), (ag__.ld(from_logits),), dict(name='from_logits'), fscope)\n",
      "        label_smoothing = ag__.converted_call(ag__.ld(check_float), (ag__.ld(label_smoothing),), dict(name='label_smoothing', minimum=0, maximum=1, allow_none=True), fscope)\n",
      "        y_pred = ag__.converted_call(ag__.ld(tf).convert_to_tensor, (ag__.ld(y_pred),), None, fscope)\n",
      "\n",
      "        def get_state():\n",
      "            return (y_pred,)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal y_pred\n",
      "            (y_pred,) = vars_\n",
      "\n",
      "        def if_body():\n",
      "            nonlocal y_pred\n",
      "            y_pred = ag__.converted_call(ag__.ld(tf).dtypes.cast, (ag__.ld(y_pred),), dict(dtype=ag__.ld(tf).float32), fscope)\n",
      "\n",
      "        def else_body():\n",
      "            nonlocal y_pred\n",
      "            pass\n",
      "        ag__.if_stmt(ag__.not_(ag__.ld(y_pred).dtype.is_floating), if_body, else_body, get_state, set_state, ('y_pred',), 1)\n",
      "\n",
      "        def get_state_1():\n",
      "            return (do_return, retval_)\n",
      "\n",
      "        def set_state_1(vars_):\n",
      "            nonlocal do_return, retval_\n",
      "            (do_return, retval_) = vars_\n",
      "\n",
      "        def if_body_1():\n",
      "            nonlocal do_return, retval_\n",
      "            try:\n",
      "                do_return = True\n",
      "                retval_ = ag__.converted_call(ag__.ld(_binary_focal_loss_from_logits), (), dict(labels=ag__.ld(y_true), logits=ag__.ld(y_pred), gamma=ag__.ld(gamma), pos_weight=ag__.ld(pos_weight), label_smoothing=ag__.ld(label_smoothing)), fscope)\n",
      "            except:\n",
      "                do_return = False\n",
      "                raise\n",
      "\n",
      "        def else_body_1():\n",
      "            nonlocal do_return, retval_\n",
      "            try:\n",
      "                do_return = True\n",
      "                retval_ = ag__.converted_call(ag__.ld(_binary_focal_loss_from_probs), (), dict(labels=ag__.ld(y_true), p=ag__.ld(y_pred), gamma=ag__.ld(gamma), pos_weight=ag__.ld(pos_weight), label_smoothing=ag__.ld(label_smoothing)), fscope)\n",
      "            except:\n",
      "                do_return = False\n",
      "                raise\n",
      "        ag__.if_stmt(ag__.ld(from_logits), if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__binary_focal_loss at 0x0000026E851AFA68> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__binary_focal_loss at 0x0000026E851AFA68> : {'pos_weight': None, 'from_logits': False, 'label_smoothing': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__binary_focal_loss at 0x0000026E851AFA68> with\n",
      "    y_true: Tensor(\"IteratorGetNext:1\", shape=(None, 256, 512, 1), dtype=uint8)\n",
      "    y_pred: Tensor(\"model/bin_seg/Sigmoid:0\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "    gamma: 2.0\n",
      "    pos_weight: None\n",
      "    from_logits: False\n",
      "    label_smoothing: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_float at 0x0000026E839A35E8>\n",
      "    args: (2.0,)\n",
      "    kwargs: {'name': 'gamma', 'minimum': 0}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_float at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:<function check_float at 0x0000026E839A35E8> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function check_float at 0x0000026E839A35E8>:\n",
      "\n",
      "def check_float(obj, *, name=None, positive=False, minimum=None, maximum=None,\n",
      "                allow_none=False, default=None):\n",
      "    \"\"\"Validate float function arguments.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    obj : object\n",
      "        The object to be validated.\n",
      "\n",
      "    name : str, optional\n",
      "        The name of `obj` in the calling function.\n",
      "\n",
      "    positive : bool, optional\n",
      "        Whether `obj` must be a positive float.\n",
      "\n",
      "    minimum : float, optional\n",
      "        The minimum value that `obj` can take (inclusive).\n",
      "\n",
      "    maximum : float, optional\n",
      "        The maximum value that `obj` can take (inclusive).\n",
      "\n",
      "    allow_none : bool, optional\n",
      "        Indicates whether the value None should be allowed.\n",
      "\n",
      "    default : object, optional\n",
      "        The default value to return if `obj` is None and `allow_none` is True.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    float or None\n",
      "        The validated float.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    TypeError\n",
      "        If `obj` is not a float.\n",
      "\n",
      "    ValueError\n",
      "        If any of the optional positivity or minimum and maximum value\n",
      "        constraints are violated.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> check_float(0)\n",
      "    0.0\n",
      "    >>> check_float(1.0, positive=True)\n",
      "    1.0\n",
      "    >>> check_float(1.0 + 1.0j)\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    TypeError: Invalid type. Expected: Real. Actual: complex.\n",
      "    >>> check_float(-1, positive=True)\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: Parameter must be positive.\n",
      "    >>> check_float(1.2, name='a', minimum=10)\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: Parameter 'a' must be at least 10.0.\n",
      "\n",
      "    \"\"\"\n",
      "    return _check_numeric(check_func=check_float, obj=obj, name=name,\n",
      "                          base=numbers.Real, func=float, positive=positive,\n",
      "                          minimum=minimum, maximum=maximum,\n",
      "                          allow_none=allow_none, default=default)\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <function check_float at 0x0000026E839A35E8>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__check_float(obj, *, name=None, positive=None, minimum=None, maximum=None, allow_none=None, default=None):\n",
      "    \"Validate float function arguments.\\n\\n    Parameters\\n    ----------\\n    obj : object\\n        The object to be validated.\\n\\n    name : str, optional\\n        The name of `obj` in the calling function.\\n\\n    positive : bool, optional\\n        Whether `obj` must be a positive float.\\n\\n    minimum : float, optional\\n        The minimum value that `obj` can take (inclusive).\\n\\n    maximum : float, optional\\n        The maximum value that `obj` can take (inclusive).\\n\\n    allow_none : bool, optional\\n        Indicates whether the value None should be allowed.\\n\\n    default : object, optional\\n        The default value to return if `obj` is None and `allow_none` is True.\\n\\n    Returns\\n    -------\\n    float or None\\n        The validated float.\\n\\n    Raises\\n    ------\\n    TypeError\\n        If `obj` is not a float.\\n\\n    ValueError\\n        If any of the optional positivity or minimum and maximum value\\n        constraints are violated.\\n\\n    Examples\\n    --------\\n    >>> check_float(0)\\n    0.0\\n    >>> check_float(1.0, positive=True)\\n    1.0\\n    >>> check_float(1.0 + 1.0j)\\n    Traceback (most recent call last):\\n    ...\\n    TypeError: Invalid type. Expected: Real. Actual: complex.\\n    >>> check_float(-1, positive=True)\\n    Traceback (most recent call last):\\n    ...\\n    ValueError: Parameter must be positive.\\n    >>> check_float(1.2, name='a', minimum=10)\\n    Traceback (most recent call last):\\n    ...\\n    ValueError: Parameter 'a' must be at least 10.0.\\n\\n    \"\n",
      "    with ag__.FunctionScope('check_float', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.converted_call(ag__.ld(_check_numeric), (), dict(check_func=ag__.ld(check_float), obj=ag__.ld(obj), name=ag__.ld(name), base=ag__.ld(numbers).Real, func=ag__.ld(float), positive=ag__.ld(positive), minimum=ag__.ld(minimum), maximum=ag__.ld(maximum), allow_none=ag__.ld(allow_none), default=ag__.ld(default)), fscope)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E851AF4C8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E851AF4C8> : {'name': None, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E851AF4C8> with\n",
      "    name: gamma\n",
      "    minimum: 0\n",
      "    obj: 2.0\n",
      "    positive: False\n",
      "    maximum: None\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function _check_numeric at 0x0000026E839A34C8>\n",
      "    args: ()\n",
      "    kwargs: {'check_func': <function check_float at 0x0000026E839A35E8>, 'obj': 2.0, 'name': 'gamma', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'positive': False, 'minimum': 0, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _check_numeric at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:<function _check_numeric at 0x0000026E839A34C8> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function _check_numeric at 0x0000026E839A34C8>:\n",
      "\n",
      "def _check_numeric(*, check_func, obj, name, base, func, positive, minimum,\n",
      "                   maximum, allow_none, default):\n",
      "    \"\"\"Helper function for check_float and check_int.\"\"\"\n",
      "    obj = check_type(obj, name=name, base=base, func=func,\n",
      "                     allow_none=allow_none, default=default)\n",
      "\n",
      "    if obj is None:\n",
      "        return None\n",
      "\n",
      "    positive = check_bool(positive, name='positive')\n",
      "    if positive and obj <= 0:\n",
      "        if name is None:\n",
      "            message = 'Parameter must be positive.'\n",
      "        else:\n",
      "            message = f'Parameter \\'{name}\\' must be positive.'\n",
      "        raise ValueError(message)\n",
      "\n",
      "    if minimum is not None:\n",
      "        minimum = check_func(minimum, name='minimum')\n",
      "        if obj < minimum:\n",
      "            if name is None:\n",
      "                message = f'Parameter must be at least {minimum}.'\n",
      "            else:\n",
      "                message = f'Parameter \\'{name}\\' must be at least {minimum}.'\n",
      "            raise ValueError(message)\n",
      "\n",
      "    if maximum is not None:\n",
      "        maximum = check_func(maximum, name='minimum')\n",
      "        if obj > maximum:\n",
      "            if name is None:\n",
      "                message = f'Parameter must be at most {maximum}.'\n",
      "            else:\n",
      "                message = f'Parameter \\'{name}\\' must be at most {maximum}.'\n",
      "            raise ValueError(message)\n",
      "\n",
      "    return obj\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <function _check_numeric at 0x0000026E839A34C8>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf___check_numeric(*, check_func, obj, name, base, func, positive, minimum, maximum, allow_none, default):\n",
      "    'Helper function for check_float and check_int.'\n",
      "    with ag__.FunctionScope('_check_numeric', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        obj = ag__.converted_call(ag__.ld(check_type), (ag__.ld(obj),), dict(name=ag__.ld(name), base=ag__.ld(base), func=ag__.ld(func), allow_none=ag__.ld(allow_none), default=ag__.ld(default)), fscope)\n",
      "\n",
      "        def get_state_8():\n",
      "            return (do_return, retval_, maximum, minimum, positive)\n",
      "\n",
      "        def set_state_8(vars_):\n",
      "            nonlocal maximum, minimum, positive, retval_, do_return\n",
      "            (do_return, retval_, maximum, minimum, positive) = vars_\n",
      "\n",
      "        def if_body_8():\n",
      "            nonlocal maximum, minimum, positive, retval_, do_return\n",
      "            try:\n",
      "                do_return = True\n",
      "                retval_ = None\n",
      "            except:\n",
      "                do_return = False\n",
      "                raise\n",
      "\n",
      "        def else_body_8():\n",
      "            nonlocal maximum, minimum, positive, retval_, do_return\n",
      "            positive = ag__.converted_call(ag__.ld(check_bool), (ag__.ld(positive),), dict(name='positive'), fscope)\n",
      "\n",
      "            def get_state_1():\n",
      "                return ()\n",
      "\n",
      "            def set_state_1(block_vars):\n",
      "                pass\n",
      "\n",
      "            def if_body_1():\n",
      "\n",
      "                def get_state():\n",
      "                    return (message,)\n",
      "\n",
      "                def set_state(vars_):\n",
      "                    nonlocal message\n",
      "                    (message,) = vars_\n",
      "\n",
      "                def if_body():\n",
      "                    nonlocal message\n",
      "                    message = 'Parameter must be positive.'\n",
      "\n",
      "                def else_body():\n",
      "                    nonlocal message\n",
      "                    message = f\"Parameter '{ag__.ld(name)}' must be positive.\"\n",
      "                message = ag__.Undefined('message')\n",
      "                ag__.if_stmt((ag__.ld(name) is None), if_body, else_body, get_state, set_state, ('message',), 1)\n",
      "                raise ag__.converted_call(ag__.ld(ValueError), (ag__.ld(message),), None, fscope)\n",
      "\n",
      "            def else_body_1():\n",
      "                pass\n",
      "            message = ag__.Undefined('message')\n",
      "            ag__.if_stmt(ag__.and_((lambda : ag__.ld(positive)), (lambda : (ag__.ld(obj) <= 0))), if_body_1, else_body_1, get_state_1, set_state_1, (), 0)\n",
      "\n",
      "            def get_state_4():\n",
      "                return (minimum,)\n",
      "\n",
      "            def set_state_4(vars_):\n",
      "                nonlocal minimum\n",
      "                (minimum,) = vars_\n",
      "\n",
      "            def if_body_4():\n",
      "                nonlocal minimum\n",
      "                minimum = ag__.converted_call(ag__.ld(check_func), (ag__.ld(minimum),), dict(name='minimum'), fscope)\n",
      "\n",
      "                def get_state_3():\n",
      "                    return ()\n",
      "\n",
      "                def set_state_3(block_vars):\n",
      "                    pass\n",
      "\n",
      "                def if_body_3():\n",
      "\n",
      "                    def get_state_2():\n",
      "                        return (message,)\n",
      "\n",
      "                    def set_state_2(vars_):\n",
      "                        nonlocal message\n",
      "                        (message,) = vars_\n",
      "\n",
      "                    def if_body_2():\n",
      "                        nonlocal message\n",
      "                        message = f'Parameter must be at least {ag__.ld(minimum)}.'\n",
      "\n",
      "                    def else_body_2():\n",
      "                        nonlocal message\n",
      "                        message = f\"Parameter '{ag__.ld(name)}' must be at least {ag__.ld(minimum)}.\"\n",
      "                    message = ag__.Undefined('message')\n",
      "                    ag__.if_stmt((ag__.ld(name) is None), if_body_2, else_body_2, get_state_2, set_state_2, ('message',), 1)\n",
      "                    raise ag__.converted_call(ag__.ld(ValueError), (ag__.ld(message),), None, fscope)\n",
      "\n",
      "                def else_body_3():\n",
      "                    pass\n",
      "                message = ag__.Undefined('message')\n",
      "                ag__.if_stmt((ag__.ld(obj) < ag__.ld(minimum)), if_body_3, else_body_3, get_state_3, set_state_3, (), 0)\n",
      "\n",
      "            def else_body_4():\n",
      "                nonlocal minimum\n",
      "                pass\n",
      "            message = ag__.Undefined('message')\n",
      "            ag__.if_stmt((ag__.ld(minimum) is not None), if_body_4, else_body_4, get_state_4, set_state_4, ('minimum',), 0)\n",
      "\n",
      "            def get_state_7():\n",
      "                return (maximum,)\n",
      "\n",
      "            def set_state_7(vars_):\n",
      "                nonlocal maximum\n",
      "                (maximum,) = vars_\n",
      "\n",
      "            def if_body_7():\n",
      "                nonlocal maximum\n",
      "                maximum = ag__.converted_call(ag__.ld(check_func), (ag__.ld(maximum),), dict(name='minimum'), fscope)\n",
      "\n",
      "                def get_state_6():\n",
      "                    return ()\n",
      "\n",
      "                def set_state_6(block_vars):\n",
      "                    pass\n",
      "\n",
      "                def if_body_6():\n",
      "\n",
      "                    def get_state_5():\n",
      "                        return (message,)\n",
      "\n",
      "                    def set_state_5(vars_):\n",
      "                        nonlocal message\n",
      "                        (message,) = vars_\n",
      "\n",
      "                    def if_body_5():\n",
      "                        nonlocal message\n",
      "                        message = f'Parameter must be at most {ag__.ld(maximum)}.'\n",
      "\n",
      "                    def else_body_5():\n",
      "                        nonlocal message\n",
      "                        message = f\"Parameter '{ag__.ld(name)}' must be at most {ag__.ld(maximum)}.\"\n",
      "                    message = ag__.Undefined('message')\n",
      "                    ag__.if_stmt((ag__.ld(name) is None), if_body_5, else_body_5, get_state_5, set_state_5, ('message',), 1)\n",
      "                    raise ag__.converted_call(ag__.ld(ValueError), (ag__.ld(message),), None, fscope)\n",
      "\n",
      "                def else_body_6():\n",
      "                    pass\n",
      "                message = ag__.Undefined('message')\n",
      "                ag__.if_stmt((ag__.ld(obj) > ag__.ld(maximum)), if_body_6, else_body_6, get_state_6, set_state_6, (), 0)\n",
      "\n",
      "            def else_body_7():\n",
      "                nonlocal maximum\n",
      "                pass\n",
      "            message = ag__.Undefined('message')\n",
      "            ag__.if_stmt((ag__.ld(maximum) is not None), if_body_7, else_body_7, get_state_7, set_state_7, ('maximum',), 0)\n",
      "            try:\n",
      "                do_return = True\n",
      "                retval_ = ag__.ld(obj)\n",
      "            except:\n",
      "                do_return = False\n",
      "                raise\n",
      "        message = ag__.Undefined('message')\n",
      "        ag__.if_stmt((ag__.ld(obj) is None), if_body_8, else_body_8, get_state_8, set_state_8, ('do_return', 'retval_', 'maximum', 'minimum', 'positive'), 2)\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E86666C18> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E86666C18> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E86666C18> with\n",
      "    check_func: <function check_float at 0x0000026E839A35E8>\n",
      "    obj: 2.0\n",
      "    name: gamma\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    positive: False\n",
      "    minimum: 0\n",
      "    maximum: None\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (2.0,)\n",
      "    kwargs: {'name': 'gamma', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:<function check_type at 0x0000026E8399CEE8> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function check_type at 0x0000026E8399CEE8>:\n",
      "\n",
      "def check_type(obj, base, *, name=None, func=None, allow_none=False,\n",
      "               default=None, error_message=None):\n",
      "    \"\"\"Check whether an object is an instance of a base type.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    obj : object\n",
      "        The object to be validated.\n",
      "\n",
      "    name : str\n",
      "        The name of `obj` in the calling function.\n",
      "\n",
      "    base : type or tuple of type\n",
      "        The base type that `obj` should be an instance of.\n",
      "\n",
      "    func: callable, optional\n",
      "        A function to be applied to `obj` if it is of type `base`. If None, no\n",
      "        function will be applied and `obj` will be returned as-is.\n",
      "\n",
      "    allow_none : bool, optional\n",
      "        Indicates whether the value None should be allowed to pass through.\n",
      "\n",
      "    default : object, optional\n",
      "        The default value to return if `obj` is None and `allow_none` is True.\n",
      "        If `default` is not None, it must be of type `base`, and it will have\n",
      "        `func` applied to it if `func` is not None.\n",
      "\n",
      "    error_message : str or None, optional\n",
      "        Custom error message to display if the type is incorrect.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    base type or None\n",
      "        The validated object.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    TypeError\n",
      "        If `obj` is not an instance of `base`.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> check_type(1, int)\n",
      "    1\n",
      "    >>> check_type(1, (int, str))\n",
      "    1\n",
      "    >>> check_type(1, str)\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    TypeError: Invalid type. Expected: str. Actual: int.\n",
      "    >>> check_type(1, (str, bool))\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    TypeError: Invalid type. Expected: (str, bool). Actual: int.\n",
      "    >>> print(check_type(None, str, allow_none=True))\n",
      "    None\n",
      "    >>> check_type(1, str, name='num')\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    TypeError: Invalid type for parameter 'num'. Expected: str. Actual: int.\n",
      "    >>> check_type(1, int, func=str)\n",
      "    '1'\n",
      "    >>> check_type(1, int, func='not callable')\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: Parameter 'func' must be callable or None.\n",
      "    >>> check_type(2.0, str, error_message='Not a string!')\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    TypeError: Not a string!\n",
      "    >>> check_type(None, int, allow_none=True, default=0)\n",
      "    0\n",
      "\n",
      "    \"\"\"\n",
      "    if allow_none and obj is None:\n",
      "        if default is not None:\n",
      "            return check_type(default, base=base, name=name, func=func,\n",
      "                              allow_none=False)\n",
      "        return None\n",
      "\n",
      "    if isinstance(obj, base):\n",
      "        if func is None:\n",
      "            return obj\n",
      "        elif callable(func):\n",
      "            return func(obj)\n",
      "        else:\n",
      "            raise ValueError('Parameter \\'func\\' must be callable or None.')\n",
      "\n",
      "    # Handle wrong type\n",
      "    if isinstance(base, tuple):\n",
      "        expect = '(' + ', '.join(cls.__name__ for cls in base) + ')'\n",
      "    else:\n",
      "        expect = base.__name__\n",
      "    actual = type(obj).__name__\n",
      "    if error_message is None:\n",
      "        error_message = 'Invalid type'\n",
      "        if name is not None:\n",
      "            error_message += f' for parameter \\'{name}\\''\n",
      "        error_message += f'. Expected: {expect}. Actual: {actual}.'\n",
      "    raise TypeError(error_message)\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <function check_type at 0x0000026E8399CEE8>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__check_type(obj, base, *, name=None, func=None, allow_none=None, default=None, error_message=None):\n",
      "    \"Check whether an object is an instance of a base type.\\n\\n    Parameters\\n    ----------\\n    obj : object\\n        The object to be validated.\\n\\n    name : str\\n        The name of `obj` in the calling function.\\n\\n    base : type or tuple of type\\n        The base type that `obj` should be an instance of.\\n\\n    func: callable, optional\\n        A function to be applied to `obj` if it is of type `base`. If None, no\\n        function will be applied and `obj` will be returned as-is.\\n\\n    allow_none : bool, optional\\n        Indicates whether the value None should be allowed to pass through.\\n\\n    default : object, optional\\n        The default value to return if `obj` is None and `allow_none` is True.\\n        If `default` is not None, it must be of type `base`, and it will have\\n        `func` applied to it if `func` is not None.\\n\\n    error_message : str or None, optional\\n        Custom error message to display if the type is incorrect.\\n\\n    Returns\\n    -------\\n    base type or None\\n        The validated object.\\n\\n    Raises\\n    ------\\n    TypeError\\n        If `obj` is not an instance of `base`.\\n\\n    Examples\\n    --------\\n    >>> check_type(1, int)\\n    1\\n    >>> check_type(1, (int, str))\\n    1\\n    >>> check_type(1, str)\\n    Traceback (most recent call last):\\n    ...\\n    TypeError: Invalid type. Expected: str. Actual: int.\\n    >>> check_type(1, (str, bool))\\n    Traceback (most recent call last):\\n    ...\\n    TypeError: Invalid type. Expected: (str, bool). Actual: int.\\n    >>> print(check_type(None, str, allow_none=True))\\n    None\\n    >>> check_type(1, str, name='num')\\n    Traceback (most recent call last):\\n    ...\\n    TypeError: Invalid type for parameter 'num'. Expected: str. Actual: int.\\n    >>> check_type(1, int, func=str)\\n    '1'\\n    >>> check_type(1, int, func='not callable')\\n    Traceback (most recent call last):\\n    ...\\n    ValueError: Parameter 'func' must be callable or None.\\n    >>> check_type(2.0, str, error_message='Not a string!')\\n    Traceback (most recent call last):\\n    ...\\n    TypeError: Not a string!\\n    >>> check_type(None, int, allow_none=True, default=0)\\n    0\\n\\n    \"\n",
      "    with ag__.FunctionScope('check_type', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "\n",
      "        def get_state_8():\n",
      "            return (do_return, retval_, error_message)\n",
      "\n",
      "        def set_state_8(vars_):\n",
      "            nonlocal do_return, error_message, retval_\n",
      "            (do_return, retval_, error_message) = vars_\n",
      "\n",
      "        def if_body_8():\n",
      "            nonlocal do_return, error_message, retval_\n",
      "\n",
      "            def get_state():\n",
      "                return (do_return, retval_)\n",
      "\n",
      "            def set_state(vars_):\n",
      "                nonlocal do_return, retval_\n",
      "                (do_return, retval_) = vars_\n",
      "\n",
      "            def if_body():\n",
      "                nonlocal do_return, retval_\n",
      "                try:\n",
      "                    do_return = True\n",
      "                    retval_ = ag__.converted_call(ag__.ld(check_type), (ag__.ld(default),), dict(base=ag__.ld(base), name=ag__.ld(name), func=ag__.ld(func), allow_none=False), fscope)\n",
      "                except:\n",
      "                    do_return = False\n",
      "                    raise\n",
      "\n",
      "            def else_body():\n",
      "                nonlocal do_return, retval_\n",
      "                try:\n",
      "                    do_return = True\n",
      "                    retval_ = None\n",
      "                except:\n",
      "                    do_return = False\n",
      "                    raise\n",
      "            ag__.if_stmt((ag__.ld(default) is not None), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n",
      "\n",
      "        def else_body_8():\n",
      "            nonlocal do_return, error_message, retval_\n",
      "\n",
      "            def get_state_3():\n",
      "                return (do_return, retval_)\n",
      "\n",
      "            def set_state_3(vars_):\n",
      "                nonlocal do_return, retval_\n",
      "                (do_return, retval_) = vars_\n",
      "\n",
      "            def if_body_3():\n",
      "                nonlocal do_return, retval_\n",
      "\n",
      "                def get_state_2():\n",
      "                    return (do_return, retval_)\n",
      "\n",
      "                def set_state_2(vars_):\n",
      "                    nonlocal do_return, retval_\n",
      "                    (do_return, retval_) = vars_\n",
      "\n",
      "                def if_body_2():\n",
      "                    nonlocal do_return, retval_\n",
      "                    try:\n",
      "                        do_return = True\n",
      "                        retval_ = ag__.ld(obj)\n",
      "                    except:\n",
      "                        do_return = False\n",
      "                        raise\n",
      "\n",
      "                def else_body_2():\n",
      "                    nonlocal do_return, retval_\n",
      "\n",
      "                    def get_state_1():\n",
      "                        return (do_return, retval_)\n",
      "\n",
      "                    def set_state_1(vars_):\n",
      "                        nonlocal do_return, retval_\n",
      "                        (do_return, retval_) = vars_\n",
      "\n",
      "                    def if_body_1():\n",
      "                        nonlocal do_return, retval_\n",
      "                        try:\n",
      "                            do_return = True\n",
      "                            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(obj),), None, fscope)\n",
      "                        except:\n",
      "                            do_return = False\n",
      "                            raise\n",
      "\n",
      "                    def else_body_1():\n",
      "                        nonlocal do_return, retval_\n",
      "                        raise ag__.converted_call(ag__.ld(ValueError), (\"Parameter 'func' must be callable or None.\",), None, fscope)\n",
      "                    ag__.if_stmt(ag__.converted_call(ag__.ld(callable), (ag__.ld(func),), None, fscope), if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n",
      "                ag__.if_stmt((ag__.ld(func) is None), if_body_2, else_body_2, get_state_2, set_state_2, ('do_return', 'retval_'), 2)\n",
      "\n",
      "            def else_body_3():\n",
      "                nonlocal do_return, retval_\n",
      "                pass\n",
      "            ag__.if_stmt(ag__.converted_call(ag__.ld(isinstance), (ag__.ld(obj), ag__.ld(base)), None, fscope), if_body_3, else_body_3, get_state_3, set_state_3, ('do_return', 'retval_'), 2)\n",
      "\n",
      "            def get_state_7():\n",
      "                return (error_message,)\n",
      "\n",
      "            def set_state_7(vars_):\n",
      "                nonlocal error_message\n",
      "                (error_message,) = vars_\n",
      "\n",
      "            def if_body_7():\n",
      "                nonlocal error_message\n",
      "\n",
      "                def get_state_4():\n",
      "                    return (expect,)\n",
      "\n",
      "                def set_state_4(vars_):\n",
      "                    nonlocal expect\n",
      "                    (expect,) = vars_\n",
      "\n",
      "                def if_body_4():\n",
      "                    nonlocal expect\n",
      "                    expect = (('(' + ag__.converted_call(', '.join, ((ag__.ld(cls).__name__ for cls in ag__.ld(base)),), None, fscope)) + ')')\n",
      "\n",
      "                def else_body_4():\n",
      "                    nonlocal expect\n",
      "                    expect = ag__.ld(base).__name__\n",
      "                expect = ag__.Undefined('expect')\n",
      "                ag__.if_stmt(ag__.converted_call(ag__.ld(isinstance), (ag__.ld(base), ag__.ld(tuple)), None, fscope), if_body_4, else_body_4, get_state_4, set_state_4, ('expect',), 1)\n",
      "                actual = ag__.converted_call(ag__.ld(type), (ag__.ld(obj),), None, fscope).__name__\n",
      "\n",
      "                def get_state_6():\n",
      "                    return (error_message,)\n",
      "\n",
      "                def set_state_6(vars_):\n",
      "                    nonlocal error_message\n",
      "                    (error_message,) = vars_\n",
      "\n",
      "                def if_body_6():\n",
      "                    nonlocal error_message\n",
      "                    error_message = 'Invalid type'\n",
      "\n",
      "                    def get_state_5():\n",
      "                        return (error_message,)\n",
      "\n",
      "                    def set_state_5(vars_):\n",
      "                        nonlocal error_message\n",
      "                        (error_message,) = vars_\n",
      "\n",
      "                    def if_body_5():\n",
      "                        nonlocal error_message\n",
      "                        error_message = ag__.ld(error_message)\n",
      "                        error_message += f\" for parameter '{name}'\"\n",
      "\n",
      "                    def else_body_5():\n",
      "                        nonlocal error_message\n",
      "                        pass\n",
      "                    ag__.if_stmt((ag__.ld(name) is not None), if_body_5, else_body_5, get_state_5, set_state_5, ('error_message',), 1)\n",
      "                    error_message = ag__.ld(error_message)\n",
      "                    error_message += f'. Expected: {expect}. Actual: {actual}.'\n",
      "\n",
      "                def else_body_6():\n",
      "                    nonlocal error_message\n",
      "                    pass\n",
      "                ag__.if_stmt((ag__.ld(error_message) is None), if_body_6, else_body_6, get_state_6, set_state_6, ('error_message',), 1)\n",
      "                raise ag__.converted_call(ag__.ld(TypeError), (ag__.ld(error_message),), None, fscope)\n",
      "\n",
      "            def else_body_7():\n",
      "                nonlocal error_message\n",
      "                pass\n",
      "            actual = ag__.Undefined('actual')\n",
      "            expect = ag__.Undefined('expect')\n",
      "            ag__.if_stmt(ag__.not_(do_return), if_body_7, else_body_7, get_state_7, set_state_7, ('error_message',), 0)\n",
      "        actual = ag__.Undefined('actual')\n",
      "        expect = ag__.Undefined('expect')\n",
      "        ag__.if_stmt(ag__.and_((lambda : ag__.ld(allow_none)), (lambda : (ag__.ld(obj) is None))), if_body_8, else_body_8, get_state_8, set_state_8, ('do_return', 'retval_', 'error_message'), 2)\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C250D8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C250D8> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C250D8> with\n",
      "    name: gamma\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: 2.0\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (2.0, <class 'numbers.Real'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'float'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'float'>\n",
      "    args: (2.0,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_bool at 0x0000026E839A3438>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'positive'}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_bool at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:<function check_bool at 0x0000026E839A3438> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function check_bool at 0x0000026E839A3438>:\n",
      "\n",
      "def check_bool(obj, *, name=None, allow_none=False, default=None):\n",
      "    \"\"\"Validate boolean function arguments.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    obj : object\n",
      "        The object to be validated.\n",
      "\n",
      "    name : str, optional\n",
      "        The name of `obj` in the calling function.\n",
      "\n",
      "    allow_none : bool, optional\n",
      "        Indicates whether the value None should be allowed.\n",
      "\n",
      "    default : object, optional\n",
      "        The default value to return if `obj` is None and `allow_none` is True.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    bool or None\n",
      "        The validated bool.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    TypeError\n",
      "        If `obj` is not an instance of bool.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> check_bool(True)\n",
      "    True\n",
      "    >>> check_bool(1.0)\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    TypeError: Invalid type. Expected: bool. Actual: float.\n",
      "    >>> a = (1 < 2)\n",
      "    >>> check_bool(a, name='a')\n",
      "    True\n",
      "    >>> b = 'not a bool'\n",
      "    >>> check_bool(b, name='b')\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    TypeError: Invalid type for parameter 'b'. Expected: bool. Actual: str.\n",
      "    \"\"\"\n",
      "    return check_type(obj, name=name, base=bool, func=bool,\n",
      "                      allow_none=allow_none, default=default)\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <function check_bool at 0x0000026E839A3438>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__check_bool(obj, *, name=None, allow_none=None, default=None):\n",
      "    \"Validate boolean function arguments.\\n\\n    Parameters\\n    ----------\\n    obj : object\\n        The object to be validated.\\n\\n    name : str, optional\\n        The name of `obj` in the calling function.\\n\\n    allow_none : bool, optional\\n        Indicates whether the value None should be allowed.\\n\\n    default : object, optional\\n        The default value to return if `obj` is None and `allow_none` is True.\\n\\n    Returns\\n    -------\\n    bool or None\\n        The validated bool.\\n\\n    Raises\\n    ------\\n    TypeError\\n        If `obj` is not an instance of bool.\\n\\n    Examples\\n    --------\\n    >>> check_bool(True)\\n    True\\n    >>> check_bool(1.0)\\n    Traceback (most recent call last):\\n    ...\\n    TypeError: Invalid type. Expected: bool. Actual: float.\\n    >>> a = (1 < 2)\\n    >>> check_bool(a, name='a')\\n    True\\n    >>> b = 'not a bool'\\n    >>> check_bool(b, name='b')\\n    Traceback (most recent call last):\\n    ...\\n    TypeError: Invalid type for parameter 'b'. Expected: bool. Actual: str.\\n    \"\n",
      "    with ag__.FunctionScope('check_bool', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.converted_call(ag__.ld(check_type), (ag__.ld(obj),), dict(name=ag__.ld(name), base=ag__.ld(bool), func=ag__.ld(bool), allow_none=ag__.ld(allow_none), default=ag__.ld(default)), fscope)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E85C25168> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E85C25168> : {'name': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E85C25168> with\n",
      "    name: positive\n",
      "    obj: False\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'positive', 'base': <class 'bool'>, 'func': <class 'bool'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C25C18> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C25C18> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C25C18> with\n",
      "    name: positive\n",
      "    base: <class 'bool'>\n",
      "    func: <class 'bool'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: False\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (False, <class 'bool'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'bool'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'bool'>\n",
      "    args: (False,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_float at 0x0000026E839A35E8>\n",
      "    args: (0,)\n",
      "    kwargs: {'name': 'minimum'}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_float at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_float at 0x0000026E839A35E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86647888>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8519B798> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8519B798> : {'name': None, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8519B798> with\n",
      "    name: minimum\n",
      "    obj: 0\n",
      "    positive: False\n",
      "    minimum: None\n",
      "    maximum: None\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function _check_numeric at 0x0000026E839A34C8>\n",
      "    args: ()\n",
      "    kwargs: {'check_func': <function check_float at 0x0000026E839A35E8>, 'obj': 0, 'name': 'minimum', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _check_numeric at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function _check_numeric at 0x0000026E839A34C8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8664DC88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8519B5E8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8519B5E8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8519B5E8> with\n",
      "    check_func: <function check_float at 0x0000026E839A35E8>\n",
      "    obj: 0\n",
      "    name: minimum\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    positive: False\n",
      "    minimum: None\n",
      "    maximum: None\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (0,)\n",
      "    kwargs: {'name': 'minimum', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8519B1F8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8519B1F8> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8519B1F8> with\n",
      "    name: minimum\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: 0\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (0, <class 'numbers.Real'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'float'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'float'>\n",
      "    args: (0,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_bool at 0x0000026E839A3438>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'positive'}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_bool at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_bool at 0x0000026E839A3438> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E83DCA188>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8519BB88> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8519BB88> : {'name': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8519BB88> with\n",
      "    name: positive\n",
      "    obj: False\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'positive', 'base': <class 'bool'>, 'func': <class 'bool'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8519BCA8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8519BCA8> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8519BCA8> with\n",
      "    name: positive\n",
      "    base: <class 'bool'>\n",
      "    func: <class 'bool'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: False\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (False, <class 'bool'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'bool'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'bool'>\n",
      "    args: (False,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_float at 0x0000026E839A35E8>\n",
      "    args: (None,)\n",
      "    kwargs: {'name': 'pos_weight', 'minimum': 0, 'allow_none': True}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_float at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_float at 0x0000026E839A35E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86647888>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E85C25A68> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E85C25A68> : {'name': None, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E85C25A68> with\n",
      "    name: pos_weight\n",
      "    minimum: 0\n",
      "    allow_none: True\n",
      "    obj: None\n",
      "    positive: False\n",
      "    maximum: None\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function _check_numeric at 0x0000026E839A34C8>\n",
      "    args: ()\n",
      "    kwargs: {'check_func': <function check_float at 0x0000026E839A35E8>, 'obj': None, 'name': 'pos_weight', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'positive': False, 'minimum': 0, 'maximum': None, 'allow_none': True, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _check_numeric at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function _check_numeric at 0x0000026E839A34C8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8664DC88>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E85C25288> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E85C25288> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E85C25288> with\n",
      "    check_func: <function check_float at 0x0000026E839A35E8>\n",
      "    obj: None\n",
      "    name: pos_weight\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    positive: False\n",
      "    minimum: 0\n",
      "    maximum: None\n",
      "    allow_none: True\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (None,)\n",
      "    kwargs: {'name': 'pos_weight', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'allow_none': True, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C250D8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C250D8> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C250D8> with\n",
      "    name: pos_weight\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    allow_none: True\n",
      "    default: None\n",
      "    obj: None\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_bool at 0x0000026E839A3438>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'from_logits'}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_bool at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_bool at 0x0000026E839A3438> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E83DCA188>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E85C25F78> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E85C25F78> : {'name': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E85C25F78> with\n",
      "    name: from_logits\n",
      "    obj: False\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'from_logits', 'base': <class 'bool'>, 'func': <class 'bool'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C250D8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C250D8> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C250D8> with\n",
      "    name: from_logits\n",
      "    base: <class 'bool'>\n",
      "    func: <class 'bool'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: False\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (False, <class 'bool'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'bool'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'bool'>\n",
      "    args: (False,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_float at 0x0000026E839A35E8>\n",
      "    args: (None,)\n",
      "    kwargs: {'name': 'label_smoothing', 'minimum': 0, 'maximum': 1, 'allow_none': True}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_float at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_float at 0x0000026E839A35E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86647888>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E86666C18> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E86666C18> : {'name': None, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E86666C18> with\n",
      "    name: label_smoothing\n",
      "    minimum: 0\n",
      "    maximum: 1\n",
      "    allow_none: True\n",
      "    obj: None\n",
      "    positive: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function _check_numeric at 0x0000026E839A34C8>\n",
      "    args: ()\n",
      "    kwargs: {'check_func': <function check_float at 0x0000026E839A35E8>, 'obj': None, 'name': 'label_smoothing', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'positive': False, 'minimum': 0, 'maximum': 1, 'allow_none': True, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _check_numeric at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function _check_numeric at 0x0000026E839A34C8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8664DC88>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E85C25F78> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E85C25F78> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E85C25F78> with\n",
      "    check_func: <function check_float at 0x0000026E839A35E8>\n",
      "    obj: None\n",
      "    name: label_smoothing\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    positive: False\n",
      "    minimum: 0\n",
      "    maximum: 1\n",
      "    allow_none: True\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (None,)\n",
      "    kwargs: {'name': 'label_smoothing', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'allow_none': True, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C250D8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C250D8> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E85C250D8> with\n",
      "    name: label_smoothing\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    allow_none: True\n",
      "    default: None\n",
      "    obj: None\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function convert_to_tensor_v2_with_dispatch at 0x0000026EDD0F8AF8>\n",
      "    args: (<tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function convert_to_tensor_v2_with_dispatch at 0x0000026EDD0F8AF8>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function _binary_focal_loss_from_probs at 0x0000026E839A38B8>\n",
      "    args: ()\n",
      "    kwargs: {'labels': <tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, 'p': <tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>, 'gamma': 2.0, 'pos_weight': None, 'label_smoothing': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A38B8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _binary_focal_loss_from_probs at 0x0000026E839A38B8>: default rule\n",
      "INFO:tensorflow:<function _binary_focal_loss_from_probs at 0x0000026E839A38B8> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function _binary_focal_loss_from_probs at 0x0000026E839A38B8>:\n",
      "\n",
      "def _binary_focal_loss_from_probs(labels, p, gamma, pos_weight,\n",
      "                                  label_smoothing):\n",
      "    \"\"\"Compute focal loss from probabilities.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    labels : tensor-like\n",
      "        Tensor of 0's and 1's: binary class labels.\n",
      "\n",
      "    p : tf.Tensor\n",
      "        Estimated probabilities for the positive class.\n",
      "\n",
      "    gamma : float\n",
      "        Focusing parameter.\n",
      "\n",
      "    pos_weight : float or None\n",
      "        If not None, losses for the positive class will be scaled by this\n",
      "        weight.\n",
      "\n",
      "    label_smoothing : float or None\n",
      "        Float in [0, 1]. When 0, no smoothing occurs. When positive, the binary\n",
      "        ground truth labels `y_true` are squeezed toward 0.5, with larger values\n",
      "        of `label_smoothing` leading to label values closer to 0.5.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    tf.Tensor\n",
      "        The loss for each example.\n",
      "    \"\"\"\n",
      "    # Predicted probabilities for the negative class\n",
      "    q = 1 - p\n",
      "\n",
      "    # For numerical stability (so we don't inadvertently take the log of 0)\n",
      "    p = tf.math.maximum(p, _EPSILON)\n",
      "    q = tf.math.maximum(q, _EPSILON)\n",
      "\n",
      "    # Loss for the positive examples\n",
      "    pos_loss = -(q ** gamma) * tf.math.log(p)\n",
      "    if pos_weight is not None:\n",
      "        pos_loss *= pos_weight\n",
      "\n",
      "    # Loss for the negative examples\n",
      "    neg_loss = -(p ** gamma) * tf.math.log(q)\n",
      "\n",
      "    # Combine loss terms\n",
      "    if label_smoothing is None:\n",
      "        labels = tf.dtypes.cast(labels, dtype=tf.bool)\n",
      "        loss = tf.where(labels, pos_loss, neg_loss)\n",
      "    else:\n",
      "        labels = _process_labels(labels=labels, label_smoothing=label_smoothing,\n",
      "                                 dtype=p.dtype)\n",
      "        loss = labels * pos_loss + (1 - labels) * neg_loss\n",
      "\n",
      "    return loss\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Transformed <function _binary_focal_loss_from_probs at 0x0000026E839A38B8>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf___binary_focal_loss_from_probs(labels, p, gamma, pos_weight, label_smoothing):\n",
      "    \"Compute focal loss from probabilities.\\n\\n    Parameters\\n    ----------\\n    labels : tensor-like\\n        Tensor of 0's and 1's: binary class labels.\\n\\n    p : tf.Tensor\\n        Estimated probabilities for the positive class.\\n\\n    gamma : float\\n        Focusing parameter.\\n\\n    pos_weight : float or None\\n        If not None, losses for the positive class will be scaled by this\\n        weight.\\n\\n    label_smoothing : float or None\\n        Float in [0, 1]. When 0, no smoothing occurs. When positive, the binary\\n        ground truth labels `y_true` are squeezed toward 0.5, with larger values\\n        of `label_smoothing` leading to label values closer to 0.5.\\n\\n    Returns\\n    -------\\n    tf.Tensor\\n        The loss for each example.\\n    \"\n",
      "    with ag__.FunctionScope('_binary_focal_loss_from_probs', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        q = (1 - ag__.ld(p))\n",
      "        p = ag__.converted_call(ag__.ld(tf).math.maximum, (ag__.ld(p), ag__.ld(_EPSILON)), None, fscope)\n",
      "        q = ag__.converted_call(ag__.ld(tf).math.maximum, (ag__.ld(q), ag__.ld(_EPSILON)), None, fscope)\n",
      "        pos_loss = ((- (ag__.ld(q) ** ag__.ld(gamma))) * ag__.converted_call(ag__.ld(tf).math.log, (ag__.ld(p),), None, fscope))\n",
      "\n",
      "        def get_state():\n",
      "            return (pos_loss,)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal pos_loss\n",
      "            (pos_loss,) = vars_\n",
      "\n",
      "        def if_body():\n",
      "            nonlocal pos_loss\n",
      "            pos_loss = ag__.ld(pos_loss)\n",
      "            pos_loss *= pos_weight\n",
      "\n",
      "        def else_body():\n",
      "            nonlocal pos_loss\n",
      "            pass\n",
      "        ag__.if_stmt((ag__.ld(pos_weight) is not None), if_body, else_body, get_state, set_state, ('pos_loss',), 1)\n",
      "        neg_loss = ((- (ag__.ld(p) ** ag__.ld(gamma))) * ag__.converted_call(ag__.ld(tf).math.log, (ag__.ld(q),), None, fscope))\n",
      "\n",
      "        def get_state_1():\n",
      "            return (loss, labels)\n",
      "\n",
      "        def set_state_1(vars_):\n",
      "            nonlocal loss, labels\n",
      "            (loss, labels) = vars_\n",
      "\n",
      "        def if_body_1():\n",
      "            nonlocal loss, labels\n",
      "            labels = ag__.converted_call(ag__.ld(tf).dtypes.cast, (ag__.ld(labels),), dict(dtype=ag__.ld(tf).bool), fscope)\n",
      "            loss = ag__.converted_call(ag__.ld(tf).where, (ag__.ld(labels), ag__.ld(pos_loss), ag__.ld(neg_loss)), None, fscope)\n",
      "\n",
      "        def else_body_1():\n",
      "            nonlocal loss, labels\n",
      "            labels = ag__.converted_call(ag__.ld(_process_labels), (), dict(labels=ag__.ld(labels), label_smoothing=ag__.ld(label_smoothing), dtype=ag__.ld(p).dtype), fscope)\n",
      "            loss = ((ag__.ld(labels) * ag__.ld(pos_loss)) + ((1 - ag__.ld(labels)) * ag__.ld(neg_loss)))\n",
      "        loss = ag__.Undefined('loss')\n",
      "        ag__.if_stmt((ag__.ld(label_smoothing) is None), if_body_1, else_body_1, get_state_1, set_state_1, ('loss', 'labels'), 1)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(loss)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___binary_focal_loss_from_probs at 0x0000026E851B21F8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___binary_focal_loss_from_probs at 0x0000026E851B21F8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___binary_focal_loss_from_probs at 0x0000026E851B21F8> with\n",
      "    labels: Tensor(\"IteratorGetNext:1\", shape=(None, 256, 512, 1), dtype=uint8)\n",
      "    p: Tensor(\"model/bin_seg/Sigmoid:0\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "    gamma: 2.0\n",
      "    pos_weight: None\n",
      "    label_smoothing: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function maximum at 0x0000026EDD2ED678>\n",
      "    args: (<tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>, 1e-07)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function maximum at 0x0000026EDD2ED678>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function maximum at 0x0000026EDD2ED678>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/sub:0' shape=(None, 256, 512, 1) dtype=float32>, 1e-07)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function maximum at 0x0000026EDD2ED678>: from cache\n",
      "INFO:tensorflow:Converted call: <function log at 0x0000026EDD2E7708>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/Maximum:0' shape=(None, 256, 512, 1) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function log at 0x0000026EDD2E7708>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function log at 0x0000026EDD2E7708>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/Maximum_1:0' shape=(None, 256, 512, 1) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function log at 0x0000026EDD2E7708>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x0000026EDD689B88>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>,)\n",
      "    kwargs: {'dtype': tf.bool}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function cast at 0x0000026EDD689B88>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function where_v2 at 0x0000026EDD4705E8>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/Cast:0' shape=(None, 256, 512, 1) dtype=bool>, <tf.Tensor 'BinaryFocalLoss/mul:0' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'BinaryFocalLoss/mul_1:0' shape=(None, 256, 512, 1) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function where_v2 at 0x0000026EDD4705E8>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E8406F888>>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/weighted_loss/value:0' shape=() dtype=float32>,)\n",
      "    kwargs: {'sample_weight': <tf.Tensor 'strided_slice:0' shape=() dtype=int32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E8406F888>>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <bound method LossFunctionWrapper.call of <keras.losses.LossFunctionWrapper object at 0x0000026E84763488>>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method LossFunctionWrapper.call of <keras.losses.LossFunctionWrapper object at 0x0000026E84763488>>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <function instance_loss at 0x0000026E81C23288>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E81C23288>: default rule\n",
      "INFO:tensorflow:Not allowed: <function instance_loss at 0x0000026E81C23288>: default rule\n",
      "INFO:tensorflow:<function instance_loss at 0x0000026E81C23288> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function instance_loss at 0x0000026E81C23288>:\n",
      "\n",
      "def instance_loss(correct_label, prediction):\n",
      "    k_instance = 0.3;\n",
      "    k_dist = 1.0;\n",
      "    \n",
      "    _, var_loss, dist_loss, reg_loss = discriminative_loss(prediction, correct_label, 4, (256,512))\n",
      "    \n",
      "    inst_loss = var_loss*k_instance + dist_loss*k_dist\n",
      "    \n",
      "    return inst_loss\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <function instance_loss at 0x0000026E81C23288>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__instance_loss(correct_label, prediction):\n",
      "    with ag__.FunctionScope('instance_loss', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        k_instance = 0.3\n",
      "        k_dist = 1.0\n",
      "        (_, var_loss, dist_loss, reg_loss) = ag__.converted_call(ag__.ld(discriminative_loss), (ag__.ld(prediction), ag__.ld(correct_label), 4, (256, 512)), None, fscope)\n",
      "        inst_loss = ((ag__.ld(var_loss) * ag__.ld(k_instance)) + (ag__.ld(dist_loss) * ag__.ld(k_dist)))\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(inst_loss)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__instance_loss at 0x0000026E851AD9D8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__instance_loss at 0x0000026E851AD9D8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__instance_loss at 0x0000026E851AD9D8> with\n",
      "    correct_label: Tensor(\"IteratorGetNext:2\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "    prediction: Tensor(\"model/inst_seg/Sigmoid:0\", shape=(None, 256, 512, 4), dtype=float32)\n",
      "\n",
      "INFO:tensorflow:Converted call: <function discriminative_loss at 0x0000026E81C23048>\n",
      "    args: (<tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, 4, (256, 512))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E81C23048>: default rule\n",
      "INFO:tensorflow:Not allowed: <function discriminative_loss at 0x0000026E81C23048>: default rule\n",
      "INFO:tensorflow:<function discriminative_loss at 0x0000026E81C23048> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function discriminative_loss at 0x0000026E81C23048>:\n",
      "\n",
      "def discriminative_loss(prediction, correct_label, feature_dim, image_shape,\n",
      "                        delta_v=0.5, delta_d=1.5, param_var=1.0, param_dist=1.0, param_reg=0.001):\n",
      "    \"\"\"\n",
      "    :return: discriminative loss and its three components\n",
      "    \"\"\"\n",
      "\n",
      "    def cond(label, batch, out_loss, out_var, out_dist, out_reg, i):\n",
      "        return tf.less(i, tf.shape(batch)[0])\n",
      "\n",
      "    def body(label, batch, out_loss, out_var, out_dist, out_reg, i):\n",
      "        disc_loss, l_var, l_dist, l_reg = discriminative_loss_single(\n",
      "            prediction[i], correct_label[i], feature_dim, image_shape, delta_v, delta_d, param_var, param_dist, param_reg)\n",
      "\n",
      "        out_loss = out_loss.write(i, disc_loss)\n",
      "        out_var = out_var.write(i, l_var)\n",
      "        out_dist = out_dist.write(i, l_dist)\n",
      "        out_reg = out_reg.write(i, l_reg)\n",
      "\n",
      "        return label, batch, out_loss, out_var, out_dist, out_reg, i + 1\n",
      "\n",
      "    # TensorArray is a data structure that support dynamic writing\n",
      "    output_ta_loss = tf.TensorArray(\n",
      "        dtype=tf.float32, size=0, dynamic_size=True)\n",
      "    output_ta_var = tf.TensorArray(\n",
      "        dtype=tf.float32, size=0, dynamic_size=True)\n",
      "    output_ta_dist = tf.TensorArray(\n",
      "        dtype=tf.float32, size=0, dynamic_size=True)\n",
      "    output_ta_reg = tf.TensorArray(\n",
      "        dtype=tf.float32, size=0, dynamic_size=True)\n",
      "\n",
      "    _, _, out_loss_op, out_var_op, out_dist_op, out_reg_op, _ = tf.while_loop(\n",
      "        cond, body, [\n",
      "            correct_label, prediction, output_ta_loss, output_ta_var, output_ta_dist, output_ta_reg, 0])\n",
      "    out_loss_op = out_loss_op.stack()\n",
      "    out_var_op = out_var_op.stack()\n",
      "    out_dist_op = out_dist_op.stack()\n",
      "    out_reg_op = out_reg_op.stack()\n",
      "\n",
      "    disc_loss = tf.math.reduce_mean(out_loss_op)\n",
      "    l_var = tf.math.reduce_mean(out_var_op)\n",
      "    l_dist = tf.math.reduce_mean(out_dist_op)\n",
      "    l_reg = tf.math.reduce_mean(out_reg_op)\n",
      "\n",
      "    return disc_loss, l_var, l_dist, l_reg\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <function discriminative_loss at 0x0000026E81C23048>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__discriminative_loss(prediction, correct_label, feature_dim, image_shape, delta_v=None, delta_d=None, param_var=None, param_dist=None, param_reg=None):\n",
      "    '\\n    :return: discriminative loss and its three components\\n    '\n",
      "    with ag__.FunctionScope('discriminative_loss', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "\n",
      "        @ag__.autograph_artifact\n",
      "        def cond(label, batch, out_loss, out_var, out_dist, out_reg, i):\n",
      "            with ag__.FunctionScope('cond', 'fscope_1', ag__.STD) as fscope_1:\n",
      "                do_return_1 = False\n",
      "                retval__1 = ag__.UndefinedReturnValue()\n",
      "                try:\n",
      "                    do_return_1 = True\n",
      "                    retval__1 = ag__.converted_call(ag__.ld(tf).less, (ag__.ld(i), ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(batch),), None, fscope_1)[0]), None, fscope_1)\n",
      "                except:\n",
      "                    do_return_1 = False\n",
      "                    raise\n",
      "                return fscope_1.ret(retval__1, do_return_1)\n",
      "\n",
      "        @ag__.autograph_artifact\n",
      "        def body(label, batch, out_loss, out_var, out_dist, out_reg, i):\n",
      "            with ag__.FunctionScope('body', 'fscope_2', ag__.STD) as fscope_2:\n",
      "                do_return_2 = False\n",
      "                retval__2 = ag__.UndefinedReturnValue()\n",
      "                (disc_loss, l_var, l_dist, l_reg) = ag__.converted_call(ag__.ld(discriminative_loss_single), (ag__.ld(prediction)[ag__.ld(i)], ag__.ld(correct_label)[ag__.ld(i)], ag__.ld(feature_dim), ag__.ld(image_shape), ag__.ld(delta_v), ag__.ld(delta_d), ag__.ld(param_var), ag__.ld(param_dist), ag__.ld(param_reg)), None, fscope_2)\n",
      "                out_loss = ag__.converted_call(ag__.ld(out_loss).write, (ag__.ld(i), ag__.ld(disc_loss)), None, fscope_2)\n",
      "                out_var = ag__.converted_call(ag__.ld(out_var).write, (ag__.ld(i), ag__.ld(l_var)), None, fscope_2)\n",
      "                out_dist = ag__.converted_call(ag__.ld(out_dist).write, (ag__.ld(i), ag__.ld(l_dist)), None, fscope_2)\n",
      "                out_reg = ag__.converted_call(ag__.ld(out_reg).write, (ag__.ld(i), ag__.ld(l_reg)), None, fscope_2)\n",
      "                try:\n",
      "                    do_return_2 = True\n",
      "                    retval__2 = (ag__.ld(label), ag__.ld(batch), ag__.ld(out_loss), ag__.ld(out_var), ag__.ld(out_dist), ag__.ld(out_reg), (ag__.ld(i) + 1))\n",
      "                except:\n",
      "                    do_return_2 = False\n",
      "                    raise\n",
      "                return fscope_2.ret(retval__2, do_return_2)\n",
      "        output_ta_loss = ag__.converted_call(ag__.ld(tf).TensorArray, (), dict(dtype=ag__.ld(tf).float32, size=0, dynamic_size=True), fscope)\n",
      "        output_ta_var = ag__.converted_call(ag__.ld(tf).TensorArray, (), dict(dtype=ag__.ld(tf).float32, size=0, dynamic_size=True), fscope)\n",
      "        output_ta_dist = ag__.converted_call(ag__.ld(tf).TensorArray, (), dict(dtype=ag__.ld(tf).float32, size=0, dynamic_size=True), fscope)\n",
      "        output_ta_reg = ag__.converted_call(ag__.ld(tf).TensorArray, (), dict(dtype=ag__.ld(tf).float32, size=0, dynamic_size=True), fscope)\n",
      "        (_, _, out_loss_op, out_var_op, out_dist_op, out_reg_op, _) = ag__.converted_call(ag__.ld(tf).while_loop, (ag__.ld(cond), ag__.ld(body), [ag__.ld(correct_label), ag__.ld(prediction), ag__.ld(output_ta_loss), ag__.ld(output_ta_var), ag__.ld(output_ta_dist), ag__.ld(output_ta_reg), 0]), None, fscope)\n",
      "        out_loss_op = ag__.converted_call(ag__.ld(out_loss_op).stack, (), None, fscope)\n",
      "        out_var_op = ag__.converted_call(ag__.ld(out_var_op).stack, (), None, fscope)\n",
      "        out_dist_op = ag__.converted_call(ag__.ld(out_dist_op).stack, (), None, fscope)\n",
      "        out_reg_op = ag__.converted_call(ag__.ld(out_reg_op).stack, (), None, fscope)\n",
      "        disc_loss = ag__.converted_call(ag__.ld(tf).math.reduce_mean, (ag__.ld(out_loss_op),), None, fscope)\n",
      "        l_var = ag__.converted_call(ag__.ld(tf).math.reduce_mean, (ag__.ld(out_var_op),), None, fscope)\n",
      "        l_dist = ag__.converted_call(ag__.ld(tf).math.reduce_mean, (ag__.ld(out_dist_op),), None, fscope)\n",
      "        l_reg = ag__.converted_call(ag__.ld(tf).math.reduce_mean, (ag__.ld(out_reg_op),), None, fscope)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = (ag__.ld(disc_loss), ag__.ld(l_var), ag__.ld(l_dist), ag__.ld(l_reg))\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss at 0x0000026E851AD048> : (0.5, 1.5, 1.0, 1.0, 0.001)\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss at 0x0000026E851AD048> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss at 0x0000026E851AD048> with\n",
      "    prediction: Tensor(\"model/inst_seg/Sigmoid:0\", shape=(None, 256, 512, 4), dtype=float32)\n",
      "    correct_label: Tensor(\"IteratorGetNext:2\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "    feature_dim: 4\n",
      "    image_shape: (256, 512)\n",
      "    delta_v: 0.5\n",
      "    delta_d: 1.5\n",
      "    param_var: 1.0\n",
      "    param_dist: 1.0\n",
      "    param_reg: 0.001\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>\n",
      "    args: ()\n",
      "    kwargs: {'dtype': tf.float32, 'size': 0, 'dynamic_size': True}\n",
      "\n",
      "INFO:tensorflow:Permanently allowed: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>: constructor\n",
      "INFO:tensorflow:Converted call: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>\n",
      "    args: ()\n",
      "    kwargs: {'dtype': tf.float32, 'size': 0, 'dynamic_size': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>: from cache\n",
      "INFO:tensorflow:Converted call: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>\n",
      "    args: ()\n",
      "    kwargs: {'dtype': tf.float32, 'size': 0, 'dynamic_size': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>: from cache\n",
      "INFO:tensorflow:Converted call: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>\n",
      "    args: ()\n",
      "    kwargs: {'dtype': tf.float32, 'size': 0, 'dynamic_size': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>: from cache\n",
      "INFO:tensorflow:Converted call: <function while_loop_v2 at 0x0000026EDD812E58>\n",
      "    args: (<function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss.<locals>.cond at 0x0000026E88C2F8B8>, <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss.<locals>.body at 0x0000026E88C2F5E8>, [<tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>, <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8249E908>, <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E81C4ADC8>, <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E824CBC88>, <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E81C4DD08>, 0])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function while_loop_v2 at 0x0000026EDD812E58>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function shape_v2 at 0x0000026EDD329678>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_1:0' shape=(None, 256, 512, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function shape_v2 at 0x0000026EDD329678>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function less at 0x0000026EDD2E2D38>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/strided_slice:0' shape=() dtype=int32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function less at 0x0000026EDD2E2D38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function discriminative_loss_single at 0x0000026E81C23318>\n",
      "    args: (<tf.Tensor 'instance_loss/while/strided_slice:0' shape=(256, 512, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/strided_slice_1:0' shape=(256, 512, 1) dtype=float32>, 4, (256, 512), 0.5, 1.5, 1.0, 1.0, 0.001)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E81C23318>: default rule\n",
      "INFO:tensorflow:Not allowed: <function discriminative_loss_single at 0x0000026E81C23318>: default rule\n",
      "INFO:tensorflow:<function discriminative_loss_single at 0x0000026E81C23318> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function discriminative_loss_single at 0x0000026E81C23318>:\n",
      "\n",
      "def discriminative_loss_single(\n",
      "        prediction,\n",
      "        correct_label,\n",
      "        feature_dim,\n",
      "        label_shape,\n",
      "        delta_v,\n",
      "        delta_d,\n",
      "        param_var,\n",
      "        param_dist,\n",
      "        param_reg):\n",
      "    \"\"\"\n",
      "    discriminative loss\n",
      "    :param prediction: inference of network\n",
      "    :param correct_label: instance label\n",
      "    :param feature_dim: feature dimension of prediction\n",
      "    :param label_shape: shape of label\n",
      "    :param delta_v: cut off variance distance\n",
      "    :param delta_d: cut off cluster distance\n",
      "    :param param_var: weight for intra cluster variance\n",
      "    :param param_dist: weight for inter cluster distances\n",
      "    :param param_reg: weight regularization\n",
      "    \"\"\"\n",
      "    correct_label = tf.reshape(\n",
      "        correct_label, [label_shape[1] * label_shape[0]]\n",
      "    )\n",
      "    reshaped_pred = tf.reshape(\n",
      "        prediction, [label_shape[1] * label_shape[0], feature_dim]\n",
      "    )\n",
      "\n",
      "    # calculate instance nums\n",
      "    unique_labels, unique_id, counts = tf.unique_with_counts(correct_label)\n",
      "    counts = tf.cast(counts, tf.float32)\n",
      "    num_instances = tf.size(unique_labels)\n",
      "\n",
      "    # calculate instance pixel embedding mean vec\n",
      "    segmented_sum = tf.math.unsorted_segment_sum(\n",
      "        reshaped_pred, unique_id, num_instances)\n",
      "    mu = tf.math.divide(segmented_sum, tf.reshape(counts, (-1, 1)))\n",
      "    mu_expand = tf.gather(mu, unique_id)\n",
      "\n",
      "    distance = tf.norm(tf.subtract(mu_expand, reshaped_pred), axis=1, ord=1)\n",
      "    distance = tf.subtract(distance, delta_v)\n",
      "    distance = tf.clip_by_value(distance, 0., distance)\n",
      "    distance = tf.square(distance)\n",
      "\n",
      "    l_var = tf.math.unsorted_segment_sum(distance, unique_id, num_instances)\n",
      "    l_var = tf.math.divide(l_var, counts)\n",
      "    l_var = tf.reduce_sum(l_var)\n",
      "    l_var = tf.math.divide(l_var, tf.cast(num_instances, tf.float32))\n",
      "\n",
      "    mu_interleaved_rep = tf.tile(mu, [num_instances, 1])\n",
      "    mu_band_rep = tf.tile(mu, [1, num_instances])\n",
      "    mu_band_rep = tf.reshape(\n",
      "        mu_band_rep,\n",
      "        (num_instances *\n",
      "         num_instances,\n",
      "         feature_dim))\n",
      "\n",
      "    mu_diff = tf.subtract(mu_band_rep, mu_interleaved_rep)\n",
      "\n",
      "    intermediate_tensor = tf.reduce_sum(tf.abs(mu_diff), axis=1)\n",
      "    zero_vector = tf.zeros(1, dtype=tf.float32)\n",
      "    bool_mask = tf.not_equal(intermediate_tensor, zero_vector)\n",
      "    mu_diff_bool = tf.boolean_mask(mu_diff, bool_mask)\n",
      "\n",
      "    mu_norm = tf.norm(mu_diff_bool, axis=1, ord=1)\n",
      "    mu_norm = tf.subtract(2. * delta_d, mu_norm)\n",
      "    mu_norm = tf.clip_by_value(mu_norm, 0., mu_norm)\n",
      "    mu_norm = tf.square(mu_norm)\n",
      "\n",
      "    l_dist = tf.math.reduce_mean(mu_norm)\n",
      "\n",
      "    l_reg = tf.math.reduce_mean(tf.norm(mu, axis=1, ord=1))\n",
      "\n",
      "    param_scale = 1.\n",
      "    l_var = param_var * l_var\n",
      "    l_dist = param_dist * l_dist\n",
      "    l_reg = param_reg * l_reg\n",
      "\n",
      "    loss = param_scale * (l_var + l_dist + l_reg)\n",
      "\n",
      "    return loss, l_var, l_dist, l_reg\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <function discriminative_loss_single at 0x0000026E81C23318>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__discriminative_loss_single(prediction, correct_label, feature_dim, label_shape, delta_v, delta_d, param_var, param_dist, param_reg):\n",
      "    '\\n    discriminative loss\\n    :param prediction: inference of network\\n    :param correct_label: instance label\\n    :param feature_dim: feature dimension of prediction\\n    :param label_shape: shape of label\\n    :param delta_v: cut off variance distance\\n    :param delta_d: cut off cluster distance\\n    :param param_var: weight for intra cluster variance\\n    :param param_dist: weight for inter cluster distances\\n    :param param_reg: weight regularization\\n    '\n",
      "    with ag__.FunctionScope('discriminative_loss_single', 'fscope', ag__.STD) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        correct_label = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(correct_label), [(ag__.ld(label_shape)[1] * ag__.ld(label_shape)[0])]), None, fscope)\n",
      "        reshaped_pred = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(prediction), [(ag__.ld(label_shape)[1] * ag__.ld(label_shape)[0]), ag__.ld(feature_dim)]), None, fscope)\n",
      "        (unique_labels, unique_id, counts) = ag__.converted_call(ag__.ld(tf).unique_with_counts, (ag__.ld(correct_label),), None, fscope)\n",
      "        counts = ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(counts), ag__.ld(tf).float32), None, fscope)\n",
      "        num_instances = ag__.converted_call(ag__.ld(tf).size, (ag__.ld(unique_labels),), None, fscope)\n",
      "        segmented_sum = ag__.converted_call(ag__.ld(tf).math.unsorted_segment_sum, (ag__.ld(reshaped_pred), ag__.ld(unique_id), ag__.ld(num_instances)), None, fscope)\n",
      "        mu = ag__.converted_call(ag__.ld(tf).math.divide, (ag__.ld(segmented_sum), ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(counts), ((- 1), 1)), None, fscope)), None, fscope)\n",
      "        mu_expand = ag__.converted_call(ag__.ld(tf).gather, (ag__.ld(mu), ag__.ld(unique_id)), None, fscope)\n",
      "        distance = ag__.converted_call(ag__.ld(tf).norm, (ag__.converted_call(ag__.ld(tf).subtract, (ag__.ld(mu_expand), ag__.ld(reshaped_pred)), None, fscope),), dict(axis=1, ord=1), fscope)\n",
      "        distance = ag__.converted_call(ag__.ld(tf).subtract, (ag__.ld(distance), ag__.ld(delta_v)), None, fscope)\n",
      "        distance = ag__.converted_call(ag__.ld(tf).clip_by_value, (ag__.ld(distance), 0.0, ag__.ld(distance)), None, fscope)\n",
      "        distance = ag__.converted_call(ag__.ld(tf).square, (ag__.ld(distance),), None, fscope)\n",
      "        l_var = ag__.converted_call(ag__.ld(tf).math.unsorted_segment_sum, (ag__.ld(distance), ag__.ld(unique_id), ag__.ld(num_instances)), None, fscope)\n",
      "        l_var = ag__.converted_call(ag__.ld(tf).math.divide, (ag__.ld(l_var), ag__.ld(counts)), None, fscope)\n",
      "        l_var = ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.ld(l_var),), None, fscope)\n",
      "        l_var = ag__.converted_call(ag__.ld(tf).math.divide, (ag__.ld(l_var), ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(num_instances), ag__.ld(tf).float32), None, fscope)), None, fscope)\n",
      "        mu_interleaved_rep = ag__.converted_call(ag__.ld(tf).tile, (ag__.ld(mu), [ag__.ld(num_instances), 1]), None, fscope)\n",
      "        mu_band_rep = ag__.converted_call(ag__.ld(tf).tile, (ag__.ld(mu), [1, ag__.ld(num_instances)]), None, fscope)\n",
      "        mu_band_rep = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(mu_band_rep), ((ag__.ld(num_instances) * ag__.ld(num_instances)), ag__.ld(feature_dim))), None, fscope)\n",
      "        mu_diff = ag__.converted_call(ag__.ld(tf).subtract, (ag__.ld(mu_band_rep), ag__.ld(mu_interleaved_rep)), None, fscope)\n",
      "        intermediate_tensor = ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.converted_call(ag__.ld(tf).abs, (ag__.ld(mu_diff),), None, fscope),), dict(axis=1), fscope)\n",
      "        zero_vector = ag__.converted_call(ag__.ld(tf).zeros, (1,), dict(dtype=ag__.ld(tf).float32), fscope)\n",
      "        bool_mask = ag__.converted_call(ag__.ld(tf).not_equal, (ag__.ld(intermediate_tensor), ag__.ld(zero_vector)), None, fscope)\n",
      "        mu_diff_bool = ag__.converted_call(ag__.ld(tf).boolean_mask, (ag__.ld(mu_diff), ag__.ld(bool_mask)), None, fscope)\n",
      "        mu_norm = ag__.converted_call(ag__.ld(tf).norm, (ag__.ld(mu_diff_bool),), dict(axis=1, ord=1), fscope)\n",
      "        mu_norm = ag__.converted_call(ag__.ld(tf).subtract, ((2.0 * ag__.ld(delta_d)), ag__.ld(mu_norm)), None, fscope)\n",
      "        mu_norm = ag__.converted_call(ag__.ld(tf).clip_by_value, (ag__.ld(mu_norm), 0.0, ag__.ld(mu_norm)), None, fscope)\n",
      "        mu_norm = ag__.converted_call(ag__.ld(tf).square, (ag__.ld(mu_norm),), None, fscope)\n",
      "        l_dist = ag__.converted_call(ag__.ld(tf).math.reduce_mean, (ag__.ld(mu_norm),), None, fscope)\n",
      "        l_reg = ag__.converted_call(ag__.ld(tf).math.reduce_mean, (ag__.converted_call(ag__.ld(tf).norm, (ag__.ld(mu),), dict(axis=1, ord=1), fscope),), None, fscope)\n",
      "        param_scale = 1.0\n",
      "        l_var = (ag__.ld(param_var) * ag__.ld(l_var))\n",
      "        l_dist = (ag__.ld(param_dist) * ag__.ld(l_dist))\n",
      "        l_reg = (ag__.ld(param_reg) * ag__.ld(l_reg))\n",
      "        loss = (ag__.ld(param_scale) * ((ag__.ld(l_var) + ag__.ld(l_dist)) + ag__.ld(l_reg)))\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = (ag__.ld(loss), ag__.ld(l_var), ag__.ld(l_dist), ag__.ld(l_reg))\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss_single at 0x0000026E88C2F708> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss_single at 0x0000026E88C2F708> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss_single at 0x0000026E88C2F708> with\n",
      "    prediction: Tensor(\"instance_loss/while/strided_slice:0\", shape=(256, 512, 4), dtype=float32)\n",
      "    correct_label: Tensor(\"instance_loss/while/strided_slice_1:0\", shape=(256, 512, 1), dtype=float32)\n",
      "    feature_dim: 4\n",
      "    label_shape: (256, 512)\n",
      "    delta_v: 0.5\n",
      "    delta_d: 1.5\n",
      "    param_var: 1.0\n",
      "    param_dist: 1.0\n",
      "    param_reg: 0.001\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x0000026EDD3255E8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/strided_slice_1:0' shape=(256, 512, 1) dtype=float32>, [131072])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function reshape at 0x0000026EDD3255E8>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x0000026EDD3255E8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/strided_slice:0' shape=(256, 512, 4) dtype=float32>, [131072, 4])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x0000026EDD3255E8>: from cache\n",
      "INFO:tensorflow:Converted call: <function unique_with_counts at 0x0000026EDD3348B8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Reshape:0' shape=(131072,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function unique_with_counts at 0x0000026EDD3348B8>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function cast at 0x0000026EDD689B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/UniqueWithCounts:2' shape=(None,) dtype=int32>, tf.float32)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x0000026EDD689B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function size_v2 at 0x0000026EDD329C18>\n",
      "    args: (<tf.Tensor 'instance_loss/while/UniqueWithCounts:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function size_v2 at 0x0000026EDD329C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function unsorted_segment_sum at 0x0000026EDD320A68>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Reshape_1:0' shape=(131072, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/UniqueWithCounts:1' shape=(131072,) dtype=int32>, <tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function unsorted_segment_sum at 0x0000026EDD320A68>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x0000026EDD3255E8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Cast:0' shape=(None,) dtype=float32>, (-1, 1))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x0000026EDD3255E8>: from cache\n",
      "INFO:tensorflow:Converted call: <function divide at 0x0000026EDD6831F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/UnsortedSegmentSum:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/Reshape_2:0' shape=(None, 1) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function divide at 0x0000026EDD6831F8>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function gather_v2 at 0x0000026EDD470EE8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/UniqueWithCounts:1' shape=(131072,) dtype=int32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function gather_v2 at 0x0000026EDD470EE8>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function subtract at 0x0000026EDD683678>\n",
      "    args: (<tf.Tensor 'instance_loss/while/GatherV2:0' shape=(131072, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/Reshape_1:0' shape=(131072, 4) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function subtract at 0x0000026EDD683678>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function norm_v2 at 0x0000026EDF707B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub:0' shape=(131072, 4) dtype=float32>,)\n",
      "    kwargs: {'axis': 1, 'ord': 1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function norm_v2 at 0x0000026EDF707B88>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function subtract at 0x0000026EDD683678>\n",
      "    args: (<tf.Tensor 'instance_loss/while/norm/Squeeze:0' shape=(131072,) dtype=float32>, 0.5)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function subtract at 0x0000026EDD683678>: from cache\n",
      "INFO:tensorflow:Converted call: <function clip_by_value at 0x0000026EDF7671F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub_1:0' shape=(131072,) dtype=float32>, 0.0, <tf.Tensor 'instance_loss/while/Sub_1:0' shape=(131072,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function clip_by_value at 0x0000026EDF7671F8>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function square at 0x0000026EDD317E58>\n",
      "    args: (<tf.Tensor 'instance_loss/while/clip_by_value:0' shape=(131072,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function square at 0x0000026EDD317E58>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function unsorted_segment_sum at 0x0000026EDD320A68>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Square:0' shape=(131072,) dtype=float32>, <tf.Tensor 'instance_loss/while/UniqueWithCounts:1' shape=(131072,) dtype=int32>, <tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function unsorted_segment_sum at 0x0000026EDD320A68>: from cache\n",
      "INFO:tensorflow:Converted call: <function divide at 0x0000026EDD6831F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/UnsortedSegmentSum_1:0' shape=(None,) dtype=float32>, <tf.Tensor 'instance_loss/while/Cast:0' shape=(None,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function divide at 0x0000026EDD6831F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_sum at 0x0000026EDD6A3EE8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv_1:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function reduce_sum at 0x0000026EDD6A3EE8>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function cast at 0x0000026EDD689B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>, tf.float32)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x0000026EDD689B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function divide at 0x0000026EDD6831F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sum:0' shape=() dtype=float32>, <tf.Tensor 'instance_loss/while/Cast_1:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function divide at 0x0000026EDD6831F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function tile at 0x0000026EDD24BDC8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv:0' shape=(None, 4) dtype=float32>, [<tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>, 1])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function tile at 0x0000026EDD24BDC8>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function tile at 0x0000026EDD24BDC8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv:0' shape=(None, 4) dtype=float32>, [1, <tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function tile at 0x0000026EDD24BDC8>: from cache\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x0000026EDD3255E8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Tile_1:0' shape=(None, None) dtype=float32>, (<tf.Tensor 'instance_loss/while/mul:0' shape=() dtype=int32>, 4))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x0000026EDD3255E8>: from cache\n",
      "INFO:tensorflow:Converted call: <function subtract at 0x0000026EDD683678>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Reshape_3:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/Tile:0' shape=(None, 4) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function subtract at 0x0000026EDD683678>: from cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function abs at 0x0000026EDD67ED38>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub_2:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function abs at 0x0000026EDD67ED38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function reduce_sum at 0x0000026EDD6A3EE8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Abs:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: {'axis': 1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_sum at 0x0000026EDD6A3EE8>: from cache\n",
      "INFO:tensorflow:Converted call: <function zeros at 0x0000026EDD33C9D8>\n",
      "    args: (1,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function zeros at 0x0000026EDD33C9D8>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function not_equal at 0x0000026EDD6A3318>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sum_1:0' shape=(None,) dtype=float32>, <tf.Tensor 'instance_loss/while/zeros:0' shape=(1,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function not_equal at 0x0000026EDD6A3318>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function boolean_mask_v2 at 0x0000026EDD3344C8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub_2:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/NotEqual:0' shape=(None,) dtype=bool>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function boolean_mask_v2 at 0x0000026EDD3344C8>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function norm_v2 at 0x0000026EDF707B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/boolean_mask/GatherV2:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: {'axis': 1, 'ord': 1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function norm_v2 at 0x0000026EDF707B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function subtract at 0x0000026EDD683678>\n",
      "    args: (3.0, <tf.Tensor 'instance_loss/while/norm_1/Squeeze:0' shape=(None,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function subtract at 0x0000026EDD683678>: from cache\n",
      "INFO:tensorflow:Converted call: <function clip_by_value at 0x0000026EDF7671F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub_3:0' shape=(None,) dtype=float32>, 0.0, <tf.Tensor 'instance_loss/while/Sub_3:0' shape=(None,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function clip_by_value at 0x0000026EDF7671F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function square at 0x0000026EDD317E58>\n",
      "    args: (<tf.Tensor 'instance_loss/while/clip_by_value_1:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function square at 0x0000026EDD317E58>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Square_1:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function reduce_mean at 0x0000026EDD6AAA68>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function norm_v2 at 0x0000026EDF707B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: {'axis': 1, 'ord': 1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function norm_v2 at 0x0000026EDF707B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/while/norm_2/Squeeze:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026EFB6A4248>>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/mul_4:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026EFB6A4248>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E81C7DFC8>>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/mul_1:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E81C7DFC8>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E866B28C8>>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/mul_2:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E866B28C8>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E866B2E08>>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/mul_3:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E866B2E08>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E867A5508>>\n",
      "    args: ()\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E867A5508>>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E851A5288>>\n",
      "    args: ()\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E851A5288>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E867A5688>>\n",
      "    args: ()\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E867A5688>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E867A50C8>>\n",
      "    args: ()\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E867A50C8>>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/TensorArrayV2Stack/TensorListStack:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/TensorArrayV2Stack_1/TensorListStack:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/TensorArrayV2Stack_2/TensorListStack:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/TensorArrayV2Stack_3/TensorListStack:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E8475C948>>\n",
      "    args: (<tf.Tensor 'instance_loss/weighted_loss/value:0' shape=() dtype=float32>,)\n",
      "    kwargs: {'sample_weight': <tf.Tensor 'strided_slice:0' shape=() dtype=int32>}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E8475C948>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E839AFCC8>>\n",
      "    args: (<tf.Tensor 'AddN:0' shape=() dtype=float32>,)\n",
      "    kwargs: {'sample_weight': <tf.Tensor 'strided_slice:0' shape=() dtype=int32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E839AFCC8>>: from cache\n",
      "INFO:tensorflow:Converted call: <function ReplicaContextBase.all_reduce.<locals>.batch_all_reduce at 0x0000026E8407D708>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'gradient_tape/model/conv2d/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_1/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_2/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_2/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_3/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_3/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_4/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_4/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_5/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_5/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_6/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 64, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_6/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_7/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 128, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_7/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_8/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 128, 256) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_8/BiasAdd/BiasAddGrad:0' shape=(256,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_9/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 256, 256) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_9/BiasAdd/BiasAddGrad:0' shape=(256,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_4/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 128, 256) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_4/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 128, 256) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_18/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 256, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_18/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_10/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 256, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_10/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_19/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 128, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_19/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_5/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 64, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_5/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_11/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 128, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_11/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_1/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 64, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_1/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_20/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 128, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_20/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_12/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 128, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_12/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_21/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_21/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_6/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 32, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_6/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_13/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_13/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_2/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 32, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_2/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_22/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 64, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_22/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_14/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 64, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_14/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_23/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_23/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_7/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 16, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_7/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_15/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_15/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_3/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 16, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_3/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_24/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 32, 16) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_16/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 32, 16) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_16/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_25/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/batch_normalization/FusedBatchNormGradV3:1' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/batch_normalization/FusedBatchNormGradV3:2' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_17/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_17/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/bin_seg/Conv2D/Conv2DBackpropFilter:0' shape=(1, 1, 16, 1) dtype=float32>, <tf.Tensor 'gradient_tape/model/bin_seg/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Tensor 'gradient_tape/model/inst_seg/Conv2D/Conv2DBackpropFilter:0' shape=(1, 1, 16, 4) dtype=float32>, <tf.Tensor 'gradient_tape/model/inst_seg/BiasAdd/BiasAddGrad:0' shape=(4,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted: <function ReplicaContextBase.all_reduce.<locals>.batch_all_reduce at 0x0000026E8407D708>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:0' shape=(3, 3, 3, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Tensor 'Adam/IdentityN:0' shape=(3, 3, 3, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:1' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:1' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:2' shape=(3, 3, 16, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 16, 16) dtype=float32>, <tf.Tensor 'Adam/IdentityN:2' shape=(3, 3, 16, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:3' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_1/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:3' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:4' shape=(3, 3, 16, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:4' shape=(3, 3, 16, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:5' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_2/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:5' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:6' shape=(3, 3, 32, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:6' shape=(3, 3, 32, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:7' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_3/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:7' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:8' shape=(3, 3, 32, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_4/kernel:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:8' shape=(3, 3, 32, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:9' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_4/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:9' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:10' shape=(3, 3, 64, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_5/kernel:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:10' shape=(3, 3, 64, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:11' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_5/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:11' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:12' shape=(3, 3, 64, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_6/kernel:0' shape=(3, 3, 64, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:12' shape=(3, 3, 64, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:13' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_6/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:13' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:14' shape=(3, 3, 128, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_7/kernel:0' shape=(3, 3, 128, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:14' shape=(3, 3, 128, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:15' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_7/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:15' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:16' shape=(3, 3, 128, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_8/kernel:0' shape=(3, 3, 128, 256) dtype=float32>, <tf.Tensor 'Adam/IdentityN:16' shape=(3, 3, 128, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:17' shape=(256,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_8/bias:0' shape=(256,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:17' shape=(256,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:18' shape=(3, 3, 256, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_9/kernel:0' shape=(3, 3, 256, 256) dtype=float32>, <tf.Tensor 'Adam/IdentityN:18' shape=(3, 3, 256, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:19' shape=(256,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_9/bias:0' shape=(256,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:19' shape=(256,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:20' shape=(2, 2, 128, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_4/kernel:0' shape=(2, 2, 128, 256) dtype=float32>, <tf.Tensor 'Adam/IdentityN:20' shape=(2, 2, 128, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:21' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_4/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:21' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:22' shape=(2, 2, 128, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose/kernel:0' shape=(2, 2, 128, 256) dtype=float32>, <tf.Tensor 'Adam/IdentityN:22' shape=(2, 2, 128, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:23' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:23' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:24' shape=(3, 3, 256, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_18/kernel:0' shape=(3, 3, 256, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:24' shape=(3, 3, 256, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:25' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_18/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:25' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:26' shape=(3, 3, 256, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_10/kernel:0' shape=(3, 3, 256, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:26' shape=(3, 3, 256, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:27' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_10/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:27' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:28' shape=(3, 3, 128, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_19/kernel:0' shape=(3, 3, 128, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:28' shape=(3, 3, 128, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:29' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_19/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:29' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:30' shape=(2, 2, 64, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_5/kernel:0' shape=(2, 2, 64, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:30' shape=(2, 2, 64, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:31' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_5/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:31' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:32' shape=(3, 3, 128, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_11/kernel:0' shape=(3, 3, 128, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:32' shape=(3, 3, 128, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:33' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_11/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:33' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:34' shape=(2, 2, 64, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_1/kernel:0' shape=(2, 2, 64, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:34' shape=(2, 2, 64, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:35' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_1/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:35' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:36' shape=(3, 3, 128, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_20/kernel:0' shape=(3, 3, 128, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:36' shape=(3, 3, 128, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:37' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_20/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:37' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:38' shape=(3, 3, 128, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_12/kernel:0' shape=(3, 3, 128, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:38' shape=(3, 3, 128, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:39' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_12/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:39' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:40' shape=(3, 3, 64, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_21/kernel:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:40' shape=(3, 3, 64, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:41' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_21/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:41' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:42' shape=(2, 2, 32, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_6/kernel:0' shape=(2, 2, 32, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:42' shape=(2, 2, 32, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:43' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_6/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:43' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:44' shape=(3, 3, 64, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_13/kernel:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:44' shape=(3, 3, 64, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:45' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_13/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:45' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:46' shape=(2, 2, 32, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_2/kernel:0' shape=(2, 2, 32, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:46' shape=(2, 2, 32, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:47' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_2/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:47' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:48' shape=(3, 3, 64, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_22/kernel:0' shape=(3, 3, 64, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:48' shape=(3, 3, 64, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:49' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_22/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:49' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:50' shape=(3, 3, 64, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_14/kernel:0' shape=(3, 3, 64, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:50' shape=(3, 3, 64, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:51' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_14/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:51' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:52' shape=(3, 3, 32, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_23/kernel:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:52' shape=(3, 3, 32, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:53' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_23/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:53' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:54' shape=(2, 2, 16, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_7/kernel:0' shape=(2, 2, 16, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:54' shape=(2, 2, 16, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:55' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_7/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:55' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:56' shape=(3, 3, 32, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_15/kernel:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:56' shape=(3, 3, 32, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:57' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_15/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:57' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:58' shape=(2, 2, 16, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_3/kernel:0' shape=(2, 2, 16, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:58' shape=(2, 2, 16, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:59' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_transpose_3/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:59' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:60' shape=(3, 3, 32, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 32, 16) dtype=float32>, <tf.Tensor 'Adam/IdentityN:60' shape=(3, 3, 32, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:61' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:61' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:62' shape=(3, 3, 32, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_16/kernel:0' shape=(3, 3, 32, 16) dtype=float32>, <tf.Tensor 'Adam/IdentityN:62' shape=(3, 3, 32, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:63' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_16/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:63' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:64' shape=(3, 3, 16, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 16) dtype=float32>, <tf.Tensor 'Adam/IdentityN:64' shape=(3, 3, 16, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:65' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_25/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:65' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:66' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'batch_normalization/gamma:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:66' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:67' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'batch_normalization/beta:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:67' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:68' shape=(3, 3, 16, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_17/kernel:0' shape=(3, 3, 16, 16) dtype=float32>, <tf.Tensor 'Adam/IdentityN:68' shape=(3, 3, 16, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:69' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'conv2d_17/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:69' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:70' shape=(1, 1, 16, 1) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'bin_seg/kernel:0' shape=(1, 1, 16, 1) dtype=float32>, <tf.Tensor 'Adam/IdentityN:70' shape=(1, 1, 16, 1) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:71' shape=(1,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'bin_seg/bias:0' shape=(1,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:71' shape=(1,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:72' shape=(1, 1, 16, 4) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'inst_seg/kernel:0' shape=(1, 1, 16, 4) dtype=float32>, <tf.Tensor 'Adam/IdentityN:72' shape=(1, 1, 16, 4) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:73' shape=(4,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E8407DD38>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>\n",
      "    args: (<tf.Variable 'inst_seg/bias:0' shape=(4,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:73' shape=(4,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E8407D708>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method MeanIoU.update_state of <keras.metrics.MeanIoU object at 0x0000026E83968CC8>>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, <tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>)\n",
      "    kwargs: {'sample_weight': None}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method MeanIoU.update_state of <keras.metrics.MeanIoU object at 0x0000026E83968CC8>>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <bound method MeanMetricWrapper.update_state of <keras.metrics.MeanMetricWrapper object at 0x0000026E86781588>>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>)\n",
      "    kwargs: {'sample_weight': None}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method MeanMetricWrapper.update_state of <keras.metrics.MeanMetricWrapper object at 0x0000026E86781588>>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E839AFCC8>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E839AFCC8>>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E8406F888>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E8406F888>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E8475C948>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E8475C948>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method MeanIoU.result of <keras.metrics.MeanIoU object at 0x0000026E83968CC8>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <bound method MeanIoU.result of <keras.metrics.MeanIoU object at 0x0000026E83968CC8>>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.result of <keras.metrics.MeanMetricWrapper object at 0x0000026E86781588>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.result of <keras.metrics.MeanMetricWrapper object at 0x0000026E86781588>>: from cache\n",
      "INFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.train_function at 0x0000026E81D04E58>\n",
      "    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000026E8664E5C8>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Cache hit for <function Model.make_train_function.<locals>.train_function at 0x0000026E81D04E58> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E83DE9808>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__train_function at 0x0000026E89F165E8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__train_function at 0x0000026E89F165E8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__train_function at 0x0000026E89F165E8> with\n",
      "    iterator: <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000026E8664E5C8>\n",
      "\n",
      "INFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.step_function at 0x0000026E81D049D8>\n",
      "    args: (<keras.engine.functional.Functional object at 0x0000026E83961C08>, <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000026E8664E5C8>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function Model.make_train_function.<locals>.step_function at 0x0000026E81D049D8>: from cache\n",
      "INFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x0000026E8404DEE8>\n",
      "    args: ((<tf.Tensor 'IteratorGetNext:0' shape=(None, 256, 512, 3) dtype=float32>, (<tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>)),)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x0000026E8404DEE8>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <bound method BinaryFocalLoss.call of <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, <tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x0000026E866CAF88>: default rule\n",
      "INFO:tensorflow:Not allowed: <class 'focal_loss._binary_focal_loss.BinaryFocalLoss'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method BinaryFocalLoss.call of <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>>: default rule\n",
      "INFO:tensorflow:Cache hit for <bound method BinaryFocalLoss.call of <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E85C299C8>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x0000026E8A367EE8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x0000026E8A367EE8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x0000026E8A367EE8> with\n",
      "    self: <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>\n",
      "    y_true: Tensor(\"IteratorGetNext:1\", shape=(None, 256, 512, 1), dtype=uint8)\n",
      "    y_pred: Tensor(\"model/bin_seg/Sigmoid:0\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "\n",
      "INFO:tensorflow:Converted call: <function binary_focal_loss at 0x0000026E8399C5E8>\n",
      "    args: ()\n",
      "    kwargs: {'y_true': <tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, 'y_pred': <tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>, 'gamma': 2.0, 'pos_weight': None, 'from_logits': False, 'label_smoothing': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399C5E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function binary_focal_loss at 0x0000026E8399C5E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function binary_focal_loss at 0x0000026E8399C5E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E85C391C8>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__binary_focal_loss at 0x0000026E8A367DC8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__binary_focal_loss at 0x0000026E8A367DC8> : {'pos_weight': None, 'from_logits': False, 'label_smoothing': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__binary_focal_loss at 0x0000026E8A367DC8> with\n",
      "    y_true: Tensor(\"IteratorGetNext:1\", shape=(None, 256, 512, 1), dtype=uint8)\n",
      "    y_pred: Tensor(\"model/bin_seg/Sigmoid:0\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "    gamma: 2.0\n",
      "    pos_weight: None\n",
      "    from_logits: False\n",
      "    label_smoothing: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_float at 0x0000026E839A35E8>\n",
      "    args: (2.0,)\n",
      "    kwargs: {'name': 'gamma', 'minimum': 0}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_float at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_float at 0x0000026E839A35E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86647888>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E89F169D8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E89F169D8> : {'name': None, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E89F169D8> with\n",
      "    name: gamma\n",
      "    minimum: 0\n",
      "    obj: 2.0\n",
      "    positive: False\n",
      "    maximum: None\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function _check_numeric at 0x0000026E839A34C8>\n",
      "    args: ()\n",
      "    kwargs: {'check_func': <function check_float at 0x0000026E839A35E8>, 'obj': 2.0, 'name': 'gamma', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'positive': False, 'minimum': 0, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _check_numeric at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function _check_numeric at 0x0000026E839A34C8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8664DC88>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8A383168> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8A383168> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8A383168> with\n",
      "    check_func: <function check_float at 0x0000026E839A35E8>\n",
      "    obj: 2.0\n",
      "    name: gamma\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    positive: False\n",
      "    minimum: 0\n",
      "    maximum: None\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (2.0,)\n",
      "    kwargs: {'name': 'gamma', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A3831F8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A3831F8> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A3831F8> with\n",
      "    name: gamma\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: 2.0\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (2.0, <class 'numbers.Real'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'float'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'float'>\n",
      "    args: (2.0,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_bool at 0x0000026E839A3438>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'positive'}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_bool at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_bool at 0x0000026E839A3438> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E83DCA188>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8A3830D8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8A3830D8> : {'name': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8A3830D8> with\n",
      "    name: positive\n",
      "    obj: False\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'positive', 'base': <class 'bool'>, 'func': <class 'bool'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A383AF8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A383AF8> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A383AF8> with\n",
      "    name: positive\n",
      "    base: <class 'bool'>\n",
      "    func: <class 'bool'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: False\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (False, <class 'bool'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'bool'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'bool'>\n",
      "    args: (False,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_float at 0x0000026E839A35E8>\n",
      "    args: (0,)\n",
      "    kwargs: {'name': 'minimum'}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_float at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_float at 0x0000026E839A35E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86647888>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8A3834C8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8A3834C8> : {'name': None, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8A3834C8> with\n",
      "    name: minimum\n",
      "    obj: 0\n",
      "    positive: False\n",
      "    minimum: None\n",
      "    maximum: None\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function _check_numeric at 0x0000026E839A34C8>\n",
      "    args: ()\n",
      "    kwargs: {'check_func': <function check_float at 0x0000026E839A35E8>, 'obj': 0, 'name': 'minimum', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _check_numeric at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function _check_numeric at 0x0000026E839A34C8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8664DC88>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8A383CA8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8A383CA8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8A383CA8> with\n",
      "    check_func: <function check_float at 0x0000026E839A35E8>\n",
      "    obj: 0\n",
      "    name: minimum\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    positive: False\n",
      "    minimum: None\n",
      "    maximum: None\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (0,)\n",
      "    kwargs: {'name': 'minimum', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A383948> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A383948> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A383948> with\n",
      "    name: minimum\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: 0\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (0, <class 'numbers.Real'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'float'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'float'>\n",
      "    args: (0,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_bool at 0x0000026E839A3438>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'positive'}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_bool at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_bool at 0x0000026E839A3438> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E83DCA188>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8A383DC8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8A383DC8> : {'name': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8A383DC8> with\n",
      "    name: positive\n",
      "    obj: False\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'positive', 'base': <class 'bool'>, 'func': <class 'bool'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A383288> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A383288> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A383288> with\n",
      "    name: positive\n",
      "    base: <class 'bool'>\n",
      "    func: <class 'bool'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: False\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (False, <class 'bool'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'bool'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'bool'>\n",
      "    args: (False,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_float at 0x0000026E839A35E8>\n",
      "    args: (None,)\n",
      "    kwargs: {'name': 'pos_weight', 'minimum': 0, 'allow_none': True}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_float at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_float at 0x0000026E839A35E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86647888>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8A367E58> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8A367E58> : {'name': None, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8A367E58> with\n",
      "    name: pos_weight\n",
      "    minimum: 0\n",
      "    allow_none: True\n",
      "    obj: None\n",
      "    positive: False\n",
      "    maximum: None\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function _check_numeric at 0x0000026E839A34C8>\n",
      "    args: ()\n",
      "    kwargs: {'check_func': <function check_float at 0x0000026E839A35E8>, 'obj': None, 'name': 'pos_weight', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'positive': False, 'minimum': 0, 'maximum': None, 'allow_none': True, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _check_numeric at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function _check_numeric at 0x0000026E839A34C8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8664DC88>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8A38C168> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8A38C168> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8A38C168> with\n",
      "    check_func: <function check_float at 0x0000026E839A35E8>\n",
      "    obj: None\n",
      "    name: pos_weight\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    positive: False\n",
      "    minimum: 0\n",
      "    maximum: None\n",
      "    allow_none: True\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (None,)\n",
      "    kwargs: {'name': 'pos_weight', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'allow_none': True, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A38C1F8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A38C1F8> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A38C1F8> with\n",
      "    name: pos_weight\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    allow_none: True\n",
      "    default: None\n",
      "    obj: None\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_bool at 0x0000026E839A3438>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'from_logits'}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_bool at 0x0000026E839A3438>: default rule\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Cache hit for <function check_bool at 0x0000026E839A3438> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E83DCA188>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8A367F78> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8A367F78> : {'name': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8A367F78> with\n",
      "    name: from_logits\n",
      "    obj: False\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'from_logits', 'base': <class 'bool'>, 'func': <class 'bool'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A383EE8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A383EE8> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A383EE8> with\n",
      "    name: from_logits\n",
      "    base: <class 'bool'>\n",
      "    func: <class 'bool'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: False\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (False, <class 'bool'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'bool'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'bool'>\n",
      "    args: (False,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_float at 0x0000026E839A35E8>\n",
      "    args: (None,)\n",
      "    kwargs: {'name': 'label_smoothing', 'minimum': 0, 'maximum': 1, 'allow_none': True}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_float at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_float at 0x0000026E839A35E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86647888>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E89F169D8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E89F169D8> : {'name': None, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E89F169D8> with\n",
      "    name: label_smoothing\n",
      "    minimum: 0\n",
      "    maximum: 1\n",
      "    allow_none: True\n",
      "    obj: None\n",
      "    positive: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function _check_numeric at 0x0000026E839A34C8>\n",
      "    args: ()\n",
      "    kwargs: {'check_func': <function check_float at 0x0000026E839A35E8>, 'obj': None, 'name': 'label_smoothing', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'positive': False, 'minimum': 0, 'maximum': 1, 'allow_none': True, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _check_numeric at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function _check_numeric at 0x0000026E839A34C8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8664DC88>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8A3830D8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8A3830D8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8A3830D8> with\n",
      "    check_func: <function check_float at 0x0000026E839A35E8>\n",
      "    obj: None\n",
      "    name: label_smoothing\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    positive: False\n",
      "    minimum: 0\n",
      "    maximum: 1\n",
      "    allow_none: True\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (None,)\n",
      "    kwargs: {'name': 'label_smoothing', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'allow_none': True, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A3831F8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A3831F8> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8A3831F8> with\n",
      "    name: label_smoothing\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    allow_none: True\n",
      "    default: None\n",
      "    obj: None\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function convert_to_tensor_v2_with_dispatch at 0x0000026EDD0F8AF8>\n",
      "    args: (<tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function convert_to_tensor_v2_with_dispatch at 0x0000026EDD0F8AF8>: from cache\n",
      "INFO:tensorflow:Converted call: <function _binary_focal_loss_from_probs at 0x0000026E839A38B8>\n",
      "    args: ()\n",
      "    kwargs: {'labels': <tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, 'p': <tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>, 'gamma': 2.0, 'pos_weight': None, 'label_smoothing': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A38B8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _binary_focal_loss_from_probs at 0x0000026E839A38B8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function _binary_focal_loss_from_probs at 0x0000026E839A38B8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E85C54D08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___binary_focal_loss_from_probs at 0x0000026E8A393438> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___binary_focal_loss_from_probs at 0x0000026E8A393438> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___binary_focal_loss_from_probs at 0x0000026E8A393438> with\n",
      "    labels: Tensor(\"IteratorGetNext:1\", shape=(None, 256, 512, 1), dtype=uint8)\n",
      "    p: Tensor(\"model/bin_seg/Sigmoid:0\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "    gamma: 2.0\n",
      "    pos_weight: None\n",
      "    label_smoothing: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function maximum at 0x0000026EDD2ED678>\n",
      "    args: (<tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>, 1e-07)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function maximum at 0x0000026EDD2ED678>: from cache\n",
      "INFO:tensorflow:Converted call: <function maximum at 0x0000026EDD2ED678>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/sub:0' shape=(None, 256, 512, 1) dtype=float32>, 1e-07)\n",
      "    kwargs: None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted <function maximum at 0x0000026EDD2ED678>: from cache\n",
      "INFO:tensorflow:Converted call: <function log at 0x0000026EDD2E7708>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/Maximum:0' shape=(None, 256, 512, 1) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function log at 0x0000026EDD2E7708>: from cache\n",
      "INFO:tensorflow:Converted call: <function log at 0x0000026EDD2E7708>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/Maximum_1:0' shape=(None, 256, 512, 1) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function log at 0x0000026EDD2E7708>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x0000026EDD689B88>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>,)\n",
      "    kwargs: {'dtype': tf.bool}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x0000026EDD689B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function where_v2 at 0x0000026EDD4705E8>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/Cast:0' shape=(None, 256, 512, 1) dtype=bool>, <tf.Tensor 'BinaryFocalLoss/mul:0' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'BinaryFocalLoss/mul_1:0' shape=(None, 256, 512, 1) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function where_v2 at 0x0000026EDD4705E8>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E8406F888>>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/weighted_loss/value:0' shape=() dtype=float32>,)\n",
      "    kwargs: {'sample_weight': <tf.Tensor 'strided_slice:0' shape=() dtype=int32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E8406F888>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method LossFunctionWrapper.call of <keras.losses.LossFunctionWrapper object at 0x0000026E84763488>>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method LossFunctionWrapper.call of <keras.losses.LossFunctionWrapper object at 0x0000026E84763488>>: from cache\n",
      "INFO:tensorflow:Converted call: <function instance_loss at 0x0000026E81C23288>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E81C23288>: default rule\n",
      "INFO:tensorflow:Not allowed: <function instance_loss at 0x0000026E81C23288>: default rule\n",
      "INFO:tensorflow:Cache hit for <function instance_loss at 0x0000026E81C23288> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E847484C8>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__instance_loss at 0x0000026E89F169D8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__instance_loss at 0x0000026E89F169D8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__instance_loss at 0x0000026E89F169D8> with\n",
      "    correct_label: Tensor(\"IteratorGetNext:2\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "    prediction: Tensor(\"model/inst_seg/Sigmoid:0\", shape=(None, 256, 512, 4), dtype=float32)\n",
      "\n",
      "INFO:tensorflow:Converted call: <function discriminative_loss at 0x0000026E81C23048>\n",
      "    args: (<tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, 4, (256, 512))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E81C23048>: default rule\n",
      "INFO:tensorflow:Not allowed: <function discriminative_loss at 0x0000026E81C23048>: default rule\n",
      "INFO:tensorflow:Cache hit for <function discriminative_loss at 0x0000026E81C23048> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8476E048>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss at 0x0000026E8A367EE8> : (0.5, 1.5, 1.0, 1.0, 0.001)\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss at 0x0000026E8A367EE8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss at 0x0000026E8A367EE8> with\n",
      "    prediction: Tensor(\"model/inst_seg/Sigmoid:0\", shape=(None, 256, 512, 4), dtype=float32)\n",
      "    correct_label: Tensor(\"IteratorGetNext:2\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "    feature_dim: 4\n",
      "    image_shape: (256, 512)\n",
      "    delta_v: 0.5\n",
      "    delta_d: 1.5\n",
      "    param_var: 1.0\n",
      "    param_dist: 1.0\n",
      "    param_reg: 0.001\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>\n",
      "    args: ()\n",
      "    kwargs: {'dtype': tf.float32, 'size': 0, 'dynamic_size': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>: from cache\n",
      "INFO:tensorflow:Converted call: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>\n",
      "    args: ()\n",
      "    kwargs: {'dtype': tf.float32, 'size': 0, 'dynamic_size': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>: from cache\n",
      "INFO:tensorflow:Converted call: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>\n",
      "    args: ()\n",
      "    kwargs: {'dtype': tf.float32, 'size': 0, 'dynamic_size': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>: from cache\n",
      "INFO:tensorflow:Converted call: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>\n",
      "    args: ()\n",
      "    kwargs: {'dtype': tf.float32, 'size': 0, 'dynamic_size': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>: from cache\n",
      "INFO:tensorflow:Converted call: <function while_loop_v2 at 0x0000026EDD812E58>\n",
      "    args: (<function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss.<locals>.cond at 0x0000026E89F16798>, <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss.<locals>.body at 0x0000026E8A367E58>, [<tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>, <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A3AD108>, <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A3AD2C8>, <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A3AD7C8>, <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A3AD788>, 0])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function while_loop_v2 at 0x0000026EDD812E58>: from cache\n",
      "INFO:tensorflow:Converted call: <function shape_v2 at 0x0000026EDD329678>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_1:0' shape=(None, 256, 512, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function shape_v2 at 0x0000026EDD329678>: from cache\n",
      "INFO:tensorflow:Converted call: <function less at 0x0000026EDD2E2D38>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/strided_slice:0' shape=() dtype=int32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function less at 0x0000026EDD2E2D38>: from cache\n",
      "INFO:tensorflow:Converted call: <function discriminative_loss_single at 0x0000026E81C23318>\n",
      "    args: (<tf.Tensor 'instance_loss/while/strided_slice:0' shape=(256, 512, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/strided_slice_1:0' shape=(256, 512, 1) dtype=float32>, 4, (256, 512), 0.5, 1.5, 1.0, 1.0, 0.001)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E81C23318>: default rule\n",
      "INFO:tensorflow:Not allowed: <function discriminative_loss_single at 0x0000026E81C23318>: default rule\n",
      "INFO:tensorflow:Cache hit for <function discriminative_loss_single at 0x0000026E81C23318> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86786648>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss_single at 0x0000026E8A3B6318> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss_single at 0x0000026E8A3B6318> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss_single at 0x0000026E8A3B6318> with\n",
      "    prediction: Tensor(\"instance_loss/while/strided_slice:0\", shape=(256, 512, 4), dtype=float32)\n",
      "    correct_label: Tensor(\"instance_loss/while/strided_slice_1:0\", shape=(256, 512, 1), dtype=float32)\n",
      "    feature_dim: 4\n",
      "    label_shape: (256, 512)\n",
      "    delta_v: 0.5\n",
      "    delta_d: 1.5\n",
      "    param_var: 1.0\n",
      "    param_dist: 1.0\n",
      "    param_reg: 0.001\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x0000026EDD3255E8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/strided_slice_1:0' shape=(256, 512, 1) dtype=float32>, [131072])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x0000026EDD3255E8>: from cache\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x0000026EDD3255E8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/strided_slice:0' shape=(256, 512, 4) dtype=float32>, [131072, 4])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x0000026EDD3255E8>: from cache\n",
      "INFO:tensorflow:Converted call: <function unique_with_counts at 0x0000026EDD3348B8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Reshape:0' shape=(131072,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function unique_with_counts at 0x0000026EDD3348B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x0000026EDD689B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/UniqueWithCounts:2' shape=(None,) dtype=int32>, tf.float32)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x0000026EDD689B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function size_v2 at 0x0000026EDD329C18>\n",
      "    args: (<tf.Tensor 'instance_loss/while/UniqueWithCounts:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function size_v2 at 0x0000026EDD329C18>: from cache\n",
      "INFO:tensorflow:Converted call: <function unsorted_segment_sum at 0x0000026EDD320A68>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Reshape_1:0' shape=(131072, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/UniqueWithCounts:1' shape=(131072,) dtype=int32>, <tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function unsorted_segment_sum at 0x0000026EDD320A68>: from cache\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x0000026EDD3255E8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Cast:0' shape=(None,) dtype=float32>, (-1, 1))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x0000026EDD3255E8>: from cache\n",
      "INFO:tensorflow:Converted call: <function divide at 0x0000026EDD6831F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/UnsortedSegmentSum:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/Reshape_2:0' shape=(None, 1) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function divide at 0x0000026EDD6831F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function gather_v2 at 0x0000026EDD470EE8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/UniqueWithCounts:1' shape=(131072,) dtype=int32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function gather_v2 at 0x0000026EDD470EE8>: from cache\n",
      "INFO:tensorflow:Converted call: <function subtract at 0x0000026EDD683678>\n",
      "    args: (<tf.Tensor 'instance_loss/while/GatherV2:0' shape=(131072, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/Reshape_1:0' shape=(131072, 4) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function subtract at 0x0000026EDD683678>: from cache\n",
      "INFO:tensorflow:Converted call: <function norm_v2 at 0x0000026EDF707B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub:0' shape=(131072, 4) dtype=float32>,)\n",
      "    kwargs: {'axis': 1, 'ord': 1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function norm_v2 at 0x0000026EDF707B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function subtract at 0x0000026EDD683678>\n",
      "    args: (<tf.Tensor 'instance_loss/while/norm/Squeeze:0' shape=(131072,) dtype=float32>, 0.5)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function subtract at 0x0000026EDD683678>: from cache\n",
      "INFO:tensorflow:Converted call: <function clip_by_value at 0x0000026EDF7671F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub_1:0' shape=(131072,) dtype=float32>, 0.0, <tf.Tensor 'instance_loss/while/Sub_1:0' shape=(131072,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function clip_by_value at 0x0000026EDF7671F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function square at 0x0000026EDD317E58>\n",
      "    args: (<tf.Tensor 'instance_loss/while/clip_by_value:0' shape=(131072,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function square at 0x0000026EDD317E58>: from cache\n",
      "INFO:tensorflow:Converted call: <function unsorted_segment_sum at 0x0000026EDD320A68>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Square:0' shape=(131072,) dtype=float32>, <tf.Tensor 'instance_loss/while/UniqueWithCounts:1' shape=(131072,) dtype=int32>, <tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function unsorted_segment_sum at 0x0000026EDD320A68>: from cache\n",
      "INFO:tensorflow:Converted call: <function divide at 0x0000026EDD6831F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/UnsortedSegmentSum_1:0' shape=(None,) dtype=float32>, <tf.Tensor 'instance_loss/while/Cast:0' shape=(None,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function divide at 0x0000026EDD6831F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_sum at 0x0000026EDD6A3EE8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv_1:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_sum at 0x0000026EDD6A3EE8>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x0000026EDD689B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>, tf.float32)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x0000026EDD689B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function divide at 0x0000026EDD6831F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sum:0' shape=() dtype=float32>, <tf.Tensor 'instance_loss/while/Cast_1:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function divide at 0x0000026EDD6831F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function tile at 0x0000026EDD24BDC8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv:0' shape=(None, 4) dtype=float32>, [<tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>, 1])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function tile at 0x0000026EDD24BDC8>: from cache\n",
      "INFO:tensorflow:Converted call: <function tile at 0x0000026EDD24BDC8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv:0' shape=(None, 4) dtype=float32>, [1, <tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function tile at 0x0000026EDD24BDC8>: from cache\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x0000026EDD3255E8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Tile_1:0' shape=(None, None) dtype=float32>, (<tf.Tensor 'instance_loss/while/mul:0' shape=() dtype=int32>, 4))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x0000026EDD3255E8>: from cache\n",
      "INFO:tensorflow:Converted call: <function subtract at 0x0000026EDD683678>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Reshape_3:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/Tile:0' shape=(None, 4) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function subtract at 0x0000026EDD683678>: from cache\n",
      "INFO:tensorflow:Converted call: <function abs at 0x0000026EDD67ED38>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub_2:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function abs at 0x0000026EDD67ED38>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_sum at 0x0000026EDD6A3EE8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Abs:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: {'axis': 1}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted <function reduce_sum at 0x0000026EDD6A3EE8>: from cache\n",
      "INFO:tensorflow:Converted call: <function zeros at 0x0000026EDD33C9D8>\n",
      "    args: (1,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function zeros at 0x0000026EDD33C9D8>: from cache\n",
      "INFO:tensorflow:Converted call: <function not_equal at 0x0000026EDD6A3318>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sum_1:0' shape=(None,) dtype=float32>, <tf.Tensor 'instance_loss/while/zeros:0' shape=(1,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function not_equal at 0x0000026EDD6A3318>: from cache\n",
      "INFO:tensorflow:Converted call: <function boolean_mask_v2 at 0x0000026EDD3344C8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub_2:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/NotEqual:0' shape=(None,) dtype=bool>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function boolean_mask_v2 at 0x0000026EDD3344C8>: from cache\n",
      "INFO:tensorflow:Converted call: <function norm_v2 at 0x0000026EDF707B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/boolean_mask/GatherV2:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: {'axis': 1, 'ord': 1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function norm_v2 at 0x0000026EDF707B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function subtract at 0x0000026EDD683678>\n",
      "    args: (3.0, <tf.Tensor 'instance_loss/while/norm_1/Squeeze:0' shape=(None,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function subtract at 0x0000026EDD683678>: from cache\n",
      "INFO:tensorflow:Converted call: <function clip_by_value at 0x0000026EDF7671F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub_3:0' shape=(None,) dtype=float32>, 0.0, <tf.Tensor 'instance_loss/while/Sub_3:0' shape=(None,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function clip_by_value at 0x0000026EDF7671F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function square at 0x0000026EDD317E58>\n",
      "    args: (<tf.Tensor 'instance_loss/while/clip_by_value_1:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function square at 0x0000026EDD317E58>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Square_1:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <function norm_v2 at 0x0000026EDF707B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: {'axis': 1, 'ord': 1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function norm_v2 at 0x0000026EDF707B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/while/norm_2/Squeeze:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A3C7E08>>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/mul_4:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A3C7E08>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A3C7E88>>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/mul_1:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A3C7E88>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A3C7F08>>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/mul_2:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A3C7F08>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A3C7FC8>>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/mul_3:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A3C7FC8>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A42D908>>\n",
      "    args: ()\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A42D908>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A42DA08>>\n",
      "    args: ()\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A42DA08>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A42DC88>>\n",
      "    args: ()\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A42DC88>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A42DC08>>\n",
      "    args: ()\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8A42DC08>>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/TensorArrayV2Stack/TensorListStack:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/TensorArrayV2Stack_1/TensorListStack:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/TensorArrayV2Stack_2/TensorListStack:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/TensorArrayV2Stack_3/TensorListStack:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E8475C948>>\n",
      "    args: (<tf.Tensor 'instance_loss/weighted_loss/value:0' shape=() dtype=float32>,)\n",
      "    kwargs: {'sample_weight': <tf.Tensor 'strided_slice:0' shape=() dtype=int32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E8475C948>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E839AFCC8>>\n",
      "    args: (<tf.Tensor 'AddN:0' shape=() dtype=float32>,)\n",
      "    kwargs: {'sample_weight': <tf.Tensor 'strided_slice:0' shape=() dtype=int32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E839AFCC8>>: from cache\n",
      "INFO:tensorflow:Converted call: <function ReplicaContextBase.all_reduce.<locals>.batch_all_reduce at 0x0000026E89F168B8>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'gradient_tape/model/conv2d/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_1/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_1/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_2/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_2/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_3/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_3/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_4/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_4/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_5/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_5/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_6/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 64, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_6/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_7/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 128, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_7/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_8/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 128, 256) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_8/BiasAdd/BiasAddGrad:0' shape=(256,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_9/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 256, 256) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_9/BiasAdd/BiasAddGrad:0' shape=(256,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_4/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 128, 256) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_4/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 128, 256) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_18/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 256, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_18/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_10/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 256, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_10/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_19/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 128, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_19/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_5/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 64, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_5/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_11/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 128, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_11/BiasAdd/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_1/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 64, 128) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_1/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_20/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 128, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_20/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_12/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 128, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_12/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_21/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_21/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_6/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 32, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_6/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_13/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_13/BiasAdd/BiasAddGrad:0' shape=(64,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_2/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 32, 64) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_2/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_22/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 64, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_22/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_14/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 64, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_14/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_23/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_23/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_7/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 16, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_7/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_15/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_15/BiasAdd/BiasAddGrad:0' shape=(32,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_3/conv2d_transpose/Conv2DBackpropFilter:0' shape=(2, 2, 16, 32) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_transpose_3/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_24/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 32, 16) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_24/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_16/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 32, 16) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_16/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_25/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_25/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/batch_normalization/FusedBatchNormGradV3:1' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/batch_normalization/FusedBatchNormGradV3:2' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_17/Conv2D/Conv2DBackpropFilter:0' shape=(3, 3, 16, 16) dtype=float32>, <tf.Tensor 'gradient_tape/model/conv2d_17/BiasAdd/BiasAddGrad:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradient_tape/model/bin_seg/Conv2D/Conv2DBackpropFilter:0' shape=(1, 1, 16, 1) dtype=float32>, <tf.Tensor 'gradient_tape/model/bin_seg/BiasAdd/BiasAddGrad:0' shape=(1,) dtype=float32>, <tf.Tensor 'gradient_tape/model/inst_seg/Conv2D/Conv2DBackpropFilter:0' shape=(1, 1, 16, 4) dtype=float32>, <tf.Tensor 'gradient_tape/model/inst_seg/BiasAdd/BiasAddGrad:0' shape=(4,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted: <function ReplicaContextBase.all_reduce.<locals>.batch_all_reduce at 0x0000026E89F168B8>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:0' shape=(3, 3, 3, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 16) dtype=float32>, <tf.Tensor 'Adam/IdentityN:0' shape=(3, 3, 3, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:1' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:1' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:2' shape=(3, 3, 16, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 16, 16) dtype=float32>, <tf.Tensor 'Adam/IdentityN:2' shape=(3, 3, 16, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:3' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_1/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:3' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:4' shape=(3, 3, 16, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 16, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:4' shape=(3, 3, 16, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:5' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_2/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:5' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:6' shape=(3, 3, 32, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:6' shape=(3, 3, 32, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:7' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_3/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:7' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:8' shape=(3, 3, 32, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_4/kernel:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:8' shape=(3, 3, 32, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:9' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_4/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:9' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:10' shape=(3, 3, 64, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_5/kernel:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:10' shape=(3, 3, 64, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:11' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_5/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:11' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:12' shape=(3, 3, 64, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_6/kernel:0' shape=(3, 3, 64, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:12' shape=(3, 3, 64, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:13' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_6/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:13' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:14' shape=(3, 3, 128, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_7/kernel:0' shape=(3, 3, 128, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:14' shape=(3, 3, 128, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:15' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_7/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:15' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:16' shape=(3, 3, 128, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_8/kernel:0' shape=(3, 3, 128, 256) dtype=float32>, <tf.Tensor 'Adam/IdentityN:16' shape=(3, 3, 128, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:17' shape=(256,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_8/bias:0' shape=(256,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:17' shape=(256,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:18' shape=(3, 3, 256, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_9/kernel:0' shape=(3, 3, 256, 256) dtype=float32>, <tf.Tensor 'Adam/IdentityN:18' shape=(3, 3, 256, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:19' shape=(256,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_9/bias:0' shape=(256,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:19' shape=(256,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:20' shape=(2, 2, 128, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_4/kernel:0' shape=(2, 2, 128, 256) dtype=float32>, <tf.Tensor 'Adam/IdentityN:20' shape=(2, 2, 128, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:21' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_4/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:21' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:22' shape=(2, 2, 128, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose/kernel:0' shape=(2, 2, 128, 256) dtype=float32>, <tf.Tensor 'Adam/IdentityN:22' shape=(2, 2, 128, 256) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:23' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:23' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:24' shape=(3, 3, 256, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_18/kernel:0' shape=(3, 3, 256, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:24' shape=(3, 3, 256, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:25' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_18/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:25' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:26' shape=(3, 3, 256, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_10/kernel:0' shape=(3, 3, 256, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:26' shape=(3, 3, 256, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:27' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_10/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:27' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:28' shape=(3, 3, 128, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_19/kernel:0' shape=(3, 3, 128, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:28' shape=(3, 3, 128, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:29' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_19/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:29' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:30' shape=(2, 2, 64, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_5/kernel:0' shape=(2, 2, 64, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:30' shape=(2, 2, 64, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:31' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_5/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:31' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:32' shape=(3, 3, 128, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_11/kernel:0' shape=(3, 3, 128, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:32' shape=(3, 3, 128, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:33' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_11/bias:0' shape=(128,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:33' shape=(128,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:34' shape=(2, 2, 64, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_1/kernel:0' shape=(2, 2, 64, 128) dtype=float32>, <tf.Tensor 'Adam/IdentityN:34' shape=(2, 2, 64, 128) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:35' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_1/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:35' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:36' shape=(3, 3, 128, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_20/kernel:0' shape=(3, 3, 128, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:36' shape=(3, 3, 128, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:37' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_20/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:37' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:38' shape=(3, 3, 128, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_12/kernel:0' shape=(3, 3, 128, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:38' shape=(3, 3, 128, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:39' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_12/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:39' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:40' shape=(3, 3, 64, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_21/kernel:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:40' shape=(3, 3, 64, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:41' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_21/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:41' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:42' shape=(2, 2, 32, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_6/kernel:0' shape=(2, 2, 32, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:42' shape=(2, 2, 32, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:43' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E89F16C18>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_6/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:43' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:44' shape=(3, 3, 64, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_13/kernel:0' shape=(3, 3, 64, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:44' shape=(3, 3, 64, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:45' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_13/bias:0' shape=(64,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:45' shape=(64,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:46' shape=(2, 2, 32, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_2/kernel:0' shape=(2, 2, 32, 64) dtype=float32>, <tf.Tensor 'Adam/IdentityN:46' shape=(2, 2, 32, 64) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:47' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_2/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:47' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:48' shape=(3, 3, 64, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_22/kernel:0' shape=(3, 3, 64, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:48' shape=(3, 3, 64, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:49' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_22/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:49' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:50' shape=(3, 3, 64, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_14/kernel:0' shape=(3, 3, 64, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:50' shape=(3, 3, 64, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:51' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_14/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:51' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:52' shape=(3, 3, 32, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_23/kernel:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:52' shape=(3, 3, 32, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:53' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_23/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:53' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:54' shape=(2, 2, 16, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_7/kernel:0' shape=(2, 2, 16, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:54' shape=(2, 2, 16, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:55' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_7/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:55' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:56' shape=(3, 3, 32, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_15/kernel:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:56' shape=(3, 3, 32, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:57' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_15/bias:0' shape=(32,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:57' shape=(32,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:58' shape=(2, 2, 16, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_3/kernel:0' shape=(2, 2, 16, 32) dtype=float32>, <tf.Tensor 'Adam/IdentityN:58' shape=(2, 2, 16, 32) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:59' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_transpose_3/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:59' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:60' shape=(3, 3, 32, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_24/kernel:0' shape=(3, 3, 32, 16) dtype=float32>, <tf.Tensor 'Adam/IdentityN:60' shape=(3, 3, 32, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:61' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_24/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:61' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:62' shape=(3, 3, 32, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_16/kernel:0' shape=(3, 3, 32, 16) dtype=float32>, <tf.Tensor 'Adam/IdentityN:62' shape=(3, 3, 32, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:63' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_16/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:63' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:64' shape=(3, 3, 16, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_25/kernel:0' shape=(3, 3, 16, 16) dtype=float32>, <tf.Tensor 'Adam/IdentityN:64' shape=(3, 3, 16, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:65' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_25/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:65' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:66' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'batch_normalization/gamma:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:66' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:67' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'batch_normalization/beta:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:67' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:68' shape=(3, 3, 16, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_17/kernel:0' shape=(3, 3, 16, 16) dtype=float32>, <tf.Tensor 'Adam/IdentityN:68' shape=(3, 3, 16, 16) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:69' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'conv2d_17/bias:0' shape=(16,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:69' shape=(16,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:70' shape=(1, 1, 16, 1) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'bin_seg/kernel:0' shape=(1, 1, 16, 1) dtype=float32>, <tf.Tensor 'Adam/IdentityN:70' shape=(1, 1, 16, 1) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:71' shape=(1,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'bin_seg/bias:0' shape=(1,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:71' shape=(1,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:72' shape=(1, 1, 16, 4) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'inst_seg/kernel:0' shape=(1, 1, 16, 4) dtype=float32>, <tf.Tensor 'Adam/IdentityN:72' shape=(1, 1, 16, 4) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>\n",
      "    args: (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x0000026EF9170248>, <tf.Tensor 'Adam/IdentityN:73' shape=(4,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function StrategyExtendedV2._replica_ctx_update.<locals>.merge_fn at 0x0000026E85167288>: DoNotConvert rule for tensorflow\n",
      "INFO:tensorflow:Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>\n",
      "    args: (<tf.Variable 'inst_seg/bias:0' shape=(4,) dtype=float32>, <tf.Tensor 'Adam/IdentityN:73' shape=(4,) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x0000026E89F168B8>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method MeanIoU.update_state of <keras.metrics.MeanIoU object at 0x0000026E83968CC8>>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, <tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>)\n",
      "    kwargs: {'sample_weight': None}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method MeanIoU.update_state of <keras.metrics.MeanIoU object at 0x0000026E83968CC8>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method MeanMetricWrapper.update_state of <keras.metrics.MeanMetricWrapper object at 0x0000026E86781588>>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>)\n",
      "    kwargs: {'sample_weight': None}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method MeanMetricWrapper.update_state of <keras.metrics.MeanMetricWrapper object at 0x0000026E86781588>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E839AFCC8>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E839AFCC8>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E8406F888>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E8406F888>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E8475C948>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E8475C948>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method MeanIoU.result of <keras.metrics.MeanIoU object at 0x0000026E83968CC8>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method MeanIoU.result of <keras.metrics.MeanIoU object at 0x0000026E83968CC8>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.result of <keras.metrics.MeanMetricWrapper object at 0x0000026E86781588>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.result of <keras.metrics.MeanMetricWrapper object at 0x0000026E86781588>>: from cache\n",
      "319/319 [==============================] - ETA: 0s - loss: 1.1997 - bin_seg_loss: 0.0239 - inst_seg_loss: 1.1758 - bin_seg_mean_io_u: 0.4885 - inst_seg_accuracy: 0.6913INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x0000026E8399C168>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x0000026E8399C168>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x0000026E85167288>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(900,) dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x0000026E85167288>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x0000026E8A309828>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(900, 256, 512, 3) dtype=float32>, (<tf.Tensor 'args_2:0' shape=(900, 256, 512, 1) dtype=uint8>, <tf.Tensor 'args_3:0' shape=(900, 256, 512, 1) dtype=float32>)))\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x0000026E8A309828>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <function Model.make_test_function.<locals>.test_function at 0x0000026E8A3099D8>\n",
      "    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000026E866A4708>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:<function Model.make_test_function.<locals>.test_function at 0x0000026E8A3099D8> is not cached for subkey ConversionOptions[{}]\n",
      "INFO:tensorflow:Source code of <function Model.make_test_function.<locals>.test_function at 0x0000026E8A3099D8>:\n",
      "\n",
      "def test_function(iterator):\n",
      "  \"\"\"Runs an evaluation execution with one step.\"\"\"\n",
      "  return step_function(self, iterator)\n",
      "\n",
      "\n",
      "INFO:tensorflow:Transformed <function Model.make_test_function.<locals>.test_function at 0x0000026E8A3099D8>:\n",
      "\n",
      "# coding=utf-8\n",
      "def tf__test_function(iterator):\n",
      "    'Runs an evaluation execution with one step.'\n",
      "    with ag__.FunctionScope('test_function', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__test_function at 0x0000026E8AC9E168> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__test_function at 0x0000026E8AC9E168> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__test_function at 0x0000026E8AC9E168> with\n",
      "    iterator: <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000026E866A4708>\n",
      "\n",
      "INFO:tensorflow:Converted call: <function Model.make_test_function.<locals>.step_function at 0x0000026E8A309E58>\n",
      "    args: (<keras.engine.functional.Functional object at 0x0000026E83961C08>, <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x0000026E866A4708>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function Model.make_test_function.<locals>.step_function at 0x0000026E8A309E58>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <function Model.make_test_function.<locals>.step_function.<locals>.run_step at 0x0000026E8AC9E4C8>\n",
      "    args: ((<tf.Tensor 'IteratorGetNext:0' shape=(None, 256, 512, 3) dtype=float32>, (<tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>)),)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function Model.make_test_function.<locals>.step_function.<locals>.run_step at 0x0000026E8AC9E4C8>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <bound method BinaryFocalLoss.call of <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, <tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x0000026E8A3E2D08>: default rule\n",
      "INFO:tensorflow:Not allowed: <class 'focal_loss._binary_focal_loss.BinaryFocalLoss'>: default rule\n",
      "INFO:tensorflow:Not allowed: <bound method BinaryFocalLoss.call of <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>>: default rule\n",
      "INFO:tensorflow:Cache hit for <bound method BinaryFocalLoss.call of <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E85C299C8>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x0000026E8AC9EEE8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x0000026E8AC9EEE8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__call at 0x0000026E8AC9EEE8> with\n",
      "    self: <focal_loss._binary_focal_loss.BinaryFocalLoss object at 0x0000026E81C81C48>\n",
      "    y_true: Tensor(\"IteratorGetNext:1\", shape=(None, 256, 512, 1), dtype=uint8)\n",
      "    y_pred: Tensor(\"model/bin_seg/Sigmoid:0\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function binary_focal_loss at 0x0000026E8399C5E8>\n",
      "    args: ()\n",
      "    kwargs: {'y_true': <tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, 'y_pred': <tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>, 'gamma': 2.0, 'pos_weight': None, 'from_logits': False, 'label_smoothing': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399C5E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function binary_focal_loss at 0x0000026E8399C5E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function binary_focal_loss at 0x0000026E8399C5E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E85C391C8>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__binary_focal_loss at 0x0000026E8AC9E0D8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__binary_focal_loss at 0x0000026E8AC9E0D8> : {'pos_weight': None, 'from_logits': False, 'label_smoothing': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__binary_focal_loss at 0x0000026E8AC9E0D8> with\n",
      "    y_true: Tensor(\"IteratorGetNext:1\", shape=(None, 256, 512, 1), dtype=uint8)\n",
      "    y_pred: Tensor(\"model/bin_seg/Sigmoid:0\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "    gamma: 2.0\n",
      "    pos_weight: None\n",
      "    from_logits: False\n",
      "    label_smoothing: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_float at 0x0000026E839A35E8>\n",
      "    args: (2.0,)\n",
      "    kwargs: {'name': 'gamma', 'minimum': 0}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_float at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_float at 0x0000026E839A35E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86647888>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8AC9EF78> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8AC9EF78> : {'name': None, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8AC9EF78> with\n",
      "    name: gamma\n",
      "    minimum: 0\n",
      "    obj: 2.0\n",
      "    positive: False\n",
      "    maximum: None\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function _check_numeric at 0x0000026E839A34C8>\n",
      "    args: ()\n",
      "    kwargs: {'check_func': <function check_float at 0x0000026E839A35E8>, 'obj': 2.0, 'name': 'gamma', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'positive': False, 'minimum': 0, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _check_numeric at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function _check_numeric at 0x0000026E839A34C8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8664DC88>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8AC9E8B8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8AC9E8B8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8AC9E8B8> with\n",
      "    check_func: <function check_float at 0x0000026E839A35E8>\n",
      "    obj: 2.0\n",
      "    name: gamma\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    positive: False\n",
      "    minimum: 0\n",
      "    maximum: None\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (2.0,)\n",
      "    kwargs: {'name': 'gamma', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9EDC8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9EDC8> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9EDC8> with\n",
      "    name: gamma\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: 2.0\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (2.0, <class 'numbers.Real'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'float'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'float'>\n",
      "    args: (2.0,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_bool at 0x0000026E839A3438>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'positive'}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_bool at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_bool at 0x0000026E839A3438> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E83DCA188>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8AC9E3A8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8AC9E3A8> : {'name': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8AC9E3A8> with\n",
      "    name: positive\n",
      "    obj: False\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'positive', 'base': <class 'bool'>, 'func': <class 'bool'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9E708> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9E708> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9E708> with\n",
      "    name: positive\n",
      "    base: <class 'bool'>\n",
      "    func: <class 'bool'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: False\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (False, <class 'bool'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'bool'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'bool'>\n",
      "    args: (False,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_float at 0x0000026E839A35E8>\n",
      "    args: (0,)\n",
      "    kwargs: {'name': 'minimum'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_float at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_float at 0x0000026E839A35E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86647888>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8AD1D168> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8AD1D168> : {'name': None, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8AD1D168> with\n",
      "    name: minimum\n",
      "    obj: 0\n",
      "    positive: False\n",
      "    minimum: None\n",
      "    maximum: None\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function _check_numeric at 0x0000026E839A34C8>\n",
      "    args: ()\n",
      "    kwargs: {'check_func': <function check_float at 0x0000026E839A35E8>, 'obj': 0, 'name': 'minimum', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _check_numeric at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function _check_numeric at 0x0000026E839A34C8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8664DC88>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8AD1D1F8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8AD1D1F8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8AD1D1F8> with\n",
      "    check_func: <function check_float at 0x0000026E839A35E8>\n",
      "    obj: 0\n",
      "    name: minimum\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    positive: False\n",
      "    minimum: None\n",
      "    maximum: None\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (0,)\n",
      "    kwargs: {'name': 'minimum', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AD1D288> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AD1D288> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AD1D288> with\n",
      "    name: minimum\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: 0\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (0, <class 'numbers.Real'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'float'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'float'>\n",
      "    args: (0,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_bool at 0x0000026E839A3438>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'positive'}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_bool at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_bool at 0x0000026E839A3438> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E83DCA188>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8AD1DB88> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8AD1DB88> : {'name': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8AD1DB88> with\n",
      "    name: positive\n",
      "    obj: False\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'positive', 'base': <class 'bool'>, 'func': <class 'bool'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AD1DA68> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AD1DA68> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AD1DA68> with\n",
      "    name: positive\n",
      "    base: <class 'bool'>\n",
      "    func: <class 'bool'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: False\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (False, <class 'bool'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'bool'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'bool'>\n",
      "    args: (False,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_float at 0x0000026E839A35E8>\n",
      "    args: (None,)\n",
      "    kwargs: {'name': 'pos_weight', 'minimum': 0, 'allow_none': True}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_float at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_float at 0x0000026E839A35E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86647888>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8AC9E8B8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8AC9E8B8> : {'name': None, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8AC9E8B8> with\n",
      "    name: pos_weight\n",
      "    minimum: 0\n",
      "    allow_none: True\n",
      "    obj: None\n",
      "    positive: False\n",
      "    maximum: None\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function _check_numeric at 0x0000026E839A34C8>\n",
      "    args: ()\n",
      "    kwargs: {'check_func': <function check_float at 0x0000026E839A35E8>, 'obj': None, 'name': 'pos_weight', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'positive': False, 'minimum': 0, 'maximum': None, 'allow_none': True, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _check_numeric at 0x0000026E839A34C8>: default rule\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Cache hit for <function _check_numeric at 0x0000026E839A34C8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8664DC88>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8AC9EDC8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8AC9EDC8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8AC9EDC8> with\n",
      "    check_func: <function check_float at 0x0000026E839A35E8>\n",
      "    obj: None\n",
      "    name: pos_weight\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    positive: False\n",
      "    minimum: 0\n",
      "    maximum: None\n",
      "    allow_none: True\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (None,)\n",
      "    kwargs: {'name': 'pos_weight', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'allow_none': True, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9EF78> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9EF78> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9EF78> with\n",
      "    name: pos_weight\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    allow_none: True\n",
      "    default: None\n",
      "    obj: None\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_bool at 0x0000026E839A3438>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'from_logits'}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_bool at 0x0000026E839A3438>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_bool at 0x0000026E839A3438> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E83DCA188>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8AC9EB88> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8AC9EB88> : {'name': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_bool at 0x0000026E8AC9EB88> with\n",
      "    name: from_logits\n",
      "    obj: False\n",
      "    allow_none: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (False,)\n",
      "    kwargs: {'name': 'from_logits', 'base': <class 'bool'>, 'func': <class 'bool'>, 'allow_none': False, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9EA68> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9EA68> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9EA68> with\n",
      "    name: from_logits\n",
      "    base: <class 'bool'>\n",
      "    func: <class 'bool'>\n",
      "    allow_none: False\n",
      "    default: None\n",
      "    obj: False\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function isinstance>\n",
      "    args: (False, <class 'bool'>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <built-in function callable>\n",
      "    args: (<class 'bool'>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'bool'>\n",
      "    args: (False,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_float at 0x0000026E839A35E8>\n",
      "    args: (None,)\n",
      "    kwargs: {'name': 'label_smoothing', 'minimum': 0, 'maximum': 1, 'allow_none': True}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_float at 0x0000026E839A35E8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_float at 0x0000026E839A35E8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86647888>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8AC9E048> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8AC9E048> : {'name': None, 'positive': False, 'minimum': None, 'maximum': None, 'allow_none': False, 'default': None}\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_float at 0x0000026E8AC9E048> with\n",
      "    name: label_smoothing\n",
      "    minimum: 0\n",
      "    maximum: 1\n",
      "    allow_none: True\n",
      "    obj: None\n",
      "    positive: False\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function _check_numeric at 0x0000026E839A34C8>\n",
      "    args: ()\n",
      "    kwargs: {'check_func': <function check_float at 0x0000026E839A35E8>, 'obj': None, 'name': 'label_smoothing', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'positive': False, 'minimum': 0, 'maximum': 1, 'allow_none': True, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _check_numeric at 0x0000026E839A34C8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function _check_numeric at 0x0000026E839A34C8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8664DC88>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8AC9E678> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8AC9E678> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___check_numeric at 0x0000026E8AC9E678> with\n",
      "    check_func: <function check_float at 0x0000026E839A35E8>\n",
      "    obj: None\n",
      "    name: label_smoothing\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    positive: False\n",
      "    minimum: 0\n",
      "    maximum: 1\n",
      "    allow_none: True\n",
      "    default: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function check_type at 0x0000026E8399CEE8>\n",
      "    args: (None,)\n",
      "    kwargs: {'name': 'label_smoothing', 'base': <class 'numbers.Real'>, 'func': <class 'float'>, 'allow_none': True, 'default': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function check_type at 0x0000026E8399CEE8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function check_type at 0x0000026E8399CEE8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E851BEA08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9EF78> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9EF78> : {'name': None, 'func': None, 'allow_none': False, 'default': None, 'error_message': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__check_type at 0x0000026E8AC9EF78> with\n",
      "    name: label_smoothing\n",
      "    base: <class 'numbers.Real'>\n",
      "    func: <class 'float'>\n",
      "    allow_none: True\n",
      "    default: None\n",
      "    obj: None\n",
      "    error_message: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function convert_to_tensor_v2_with_dispatch at 0x0000026EDD0F8AF8>\n",
      "    args: (<tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function convert_to_tensor_v2_with_dispatch at 0x0000026EDD0F8AF8>: from cache\n",
      "INFO:tensorflow:Converted call: <function _binary_focal_loss_from_probs at 0x0000026E839A38B8>\n",
      "    args: ()\n",
      "    kwargs: {'labels': <tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, 'p': <tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>, 'gamma': 2.0, 'pos_weight': None, 'label_smoothing': None}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E839A38B8>: default rule\n",
      "INFO:tensorflow:Not allowed: <function _binary_focal_loss_from_probs at 0x0000026E839A38B8>: default rule\n",
      "INFO:tensorflow:Cache hit for <function _binary_focal_loss_from_probs at 0x0000026E839A38B8> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E85C54D08>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___binary_focal_loss_from_probs at 0x0000026E8AC9E8B8> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf___binary_focal_loss_from_probs at 0x0000026E8AC9E8B8> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf___binary_focal_loss_from_probs at 0x0000026E8AC9E8B8> with\n",
      "    labels: Tensor(\"IteratorGetNext:1\", shape=(None, 256, 512, 1), dtype=uint8)\n",
      "    p: Tensor(\"model/bin_seg/Sigmoid:0\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "    gamma: 2.0\n",
      "    pos_weight: None\n",
      "    label_smoothing: None\n",
      "\n",
      "INFO:tensorflow:Converted call: <function maximum at 0x0000026EDD2ED678>\n",
      "    args: (<tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>, 1e-07)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function maximum at 0x0000026EDD2ED678>: from cache\n",
      "INFO:tensorflow:Converted call: <function maximum at 0x0000026EDD2ED678>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/sub:0' shape=(None, 256, 512, 1) dtype=float32>, 1e-07)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function maximum at 0x0000026EDD2ED678>: from cache\n",
      "INFO:tensorflow:Converted call: <function log at 0x0000026EDD2E7708>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/Maximum:0' shape=(None, 256, 512, 1) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function log at 0x0000026EDD2E7708>: from cache\n",
      "INFO:tensorflow:Converted call: <function log at 0x0000026EDD2E7708>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/Maximum_1:0' shape=(None, 256, 512, 1) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function log at 0x0000026EDD2E7708>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x0000026EDD689B88>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>,)\n",
      "    kwargs: {'dtype': tf.bool}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x0000026EDD689B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function where_v2 at 0x0000026EDD4705E8>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/Cast:0' shape=(None, 256, 512, 1) dtype=bool>, <tf.Tensor 'BinaryFocalLoss/mul:0' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'BinaryFocalLoss/mul_1:0' shape=(None, 256, 512, 1) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function where_v2 at 0x0000026EDD4705E8>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E8406F888>>\n",
      "    args: (<tf.Tensor 'BinaryFocalLoss/weighted_loss/value:0' shape=() dtype=float32>,)\n",
      "    kwargs: {'sample_weight': <tf.Tensor 'strided_slice:0' shape=() dtype=int32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E8406F888>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method LossFunctionWrapper.call of <keras.losses.LossFunctionWrapper object at 0x0000026E84763488>>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method LossFunctionWrapper.call of <keras.losses.LossFunctionWrapper object at 0x0000026E84763488>>: from cache\n",
      "INFO:tensorflow:Converted call: <function instance_loss at 0x0000026E81C23288>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E81C23288>: default rule\n",
      "INFO:tensorflow:Not allowed: <function instance_loss at 0x0000026E81C23288>: default rule\n",
      "INFO:tensorflow:Cache hit for <function instance_loss at 0x0000026E81C23288> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E847484C8>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__instance_loss at 0x0000026E8AC9E438> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__instance_loss at 0x0000026E8AC9E438> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__instance_loss at 0x0000026E8AC9E438> with\n",
      "    correct_label: Tensor(\"IteratorGetNext:2\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "    prediction: Tensor(\"model/inst_seg/Sigmoid:0\", shape=(None, 256, 512, 4), dtype=float32)\n",
      "\n",
      "INFO:tensorflow:Converted call: <function discriminative_loss at 0x0000026E81C23048>\n",
      "    args: (<tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, 4, (256, 512))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E81C23048>: default rule\n",
      "INFO:tensorflow:Not allowed: <function discriminative_loss at 0x0000026E81C23048>: default rule\n",
      "INFO:tensorflow:Cache hit for <function discriminative_loss at 0x0000026E81C23048> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E8476E048>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss at 0x0000026E8AC9E798> : (0.5, 1.5, 1.0, 1.0, 0.001)\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss at 0x0000026E8AC9E798> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss at 0x0000026E8AC9E798> with\n",
      "    prediction: Tensor(\"model/inst_seg/Sigmoid:0\", shape=(None, 256, 512, 4), dtype=float32)\n",
      "    correct_label: Tensor(\"IteratorGetNext:2\", shape=(None, 256, 512, 1), dtype=float32)\n",
      "    feature_dim: 4\n",
      "    image_shape: (256, 512)\n",
      "    delta_v: 0.5\n",
      "    delta_d: 1.5\n",
      "    param_var: 1.0\n",
      "    param_dist: 1.0\n",
      "    param_reg: 0.001\n",
      "\n",
      "INFO:tensorflow:Converted call: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>\n",
      "    args: ()\n",
      "    kwargs: {'dtype': tf.float32, 'size': 0, 'dynamic_size': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>: from cache\n",
      "INFO:tensorflow:Converted call: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>\n",
      "    args: ()\n",
      "    kwargs: {'dtype': tf.float32, 'size': 0, 'dynamic_size': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>: from cache\n",
      "INFO:tensorflow:Converted call: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>\n",
      "    args: ()\n",
      "    kwargs: {'dtype': tf.float32, 'size': 0, 'dynamic_size': True}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>: from cache\n",
      "INFO:tensorflow:Converted call: <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>\n",
      "    args: ()\n",
      "    kwargs: {'dtype': tf.float32, 'size': 0, 'dynamic_size': True}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>: from cache\n",
      "INFO:tensorflow:Converted call: <function while_loop_v2 at 0x0000026EDD812E58>\n",
      "    args: (<function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss.<locals>.cond at 0x0000026E8AC9EF78>, <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss.<locals>.body at 0x0000026E8AC9E048>, [<tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>, <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8AD44108>, <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8AD41148>, <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8AD440C8>, <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8AD44BC8>, 0])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function while_loop_v2 at 0x0000026EDD812E58>: from cache\n",
      "INFO:tensorflow:Converted call: <function shape_v2 at 0x0000026EDD329678>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_1:0' shape=(None, 256, 512, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function shape_v2 at 0x0000026EDD329678>: from cache\n",
      "INFO:tensorflow:Converted call: <function less at 0x0000026EDD2E2D38>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/strided_slice:0' shape=() dtype=int32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function less at 0x0000026EDD2E2D38>: from cache\n",
      "INFO:tensorflow:Converted call: <function discriminative_loss_single at 0x0000026E81C23318>\n",
      "    args: (<tf.Tensor 'instance_loss/while/strided_slice:0' shape=(256, 512, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/strided_slice_1:0' shape=(256, 512, 1) dtype=float32>, 4, (256, 512), 0.5, 1.5, 1.0, 1.0, 0.001)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Not allowed: <method-wrapper '__call__' of function object at 0x0000026E81C23318>: default rule\n",
      "INFO:tensorflow:Not allowed: <function discriminative_loss_single at 0x0000026E81C23318>: default rule\n",
      "INFO:tensorflow:Cache hit for <function discriminative_loss_single at 0x0000026E81C23318> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x0000026E86786648>\n",
      "INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss_single at 0x0000026E8AC9EB88> : None\n",
      "INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss_single at 0x0000026E8AC9EB88> : None\n",
      "INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__discriminative_loss_single at 0x0000026E8AC9EB88> with\n",
      "    prediction: Tensor(\"instance_loss/while/strided_slice:0\", shape=(256, 512, 4), dtype=float32)\n",
      "    correct_label: Tensor(\"instance_loss/while/strided_slice_1:0\", shape=(256, 512, 1), dtype=float32)\n",
      "    feature_dim: 4\n",
      "    label_shape: (256, 512)\n",
      "    delta_v: 0.5\n",
      "    delta_d: 1.5\n",
      "    param_var: 1.0\n",
      "    param_dist: 1.0\n",
      "    param_reg: 0.001\n",
      "\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x0000026EDD3255E8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/strided_slice_1:0' shape=(256, 512, 1) dtype=float32>, [131072])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x0000026EDD3255E8>: from cache\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x0000026EDD3255E8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/strided_slice:0' shape=(256, 512, 4) dtype=float32>, [131072, 4])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x0000026EDD3255E8>: from cache\n",
      "INFO:tensorflow:Converted call: <function unique_with_counts at 0x0000026EDD3348B8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Reshape:0' shape=(131072,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function unique_with_counts at 0x0000026EDD3348B8>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x0000026EDD689B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/UniqueWithCounts:2' shape=(None,) dtype=int32>, tf.float32)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x0000026EDD689B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function size_v2 at 0x0000026EDD329C18>\n",
      "    args: (<tf.Tensor 'instance_loss/while/UniqueWithCounts:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function size_v2 at 0x0000026EDD329C18>: from cache\n",
      "INFO:tensorflow:Converted call: <function unsorted_segment_sum at 0x0000026EDD320A68>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Reshape_1:0' shape=(131072, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/UniqueWithCounts:1' shape=(131072,) dtype=int32>, <tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function unsorted_segment_sum at 0x0000026EDD320A68>: from cache\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x0000026EDD3255E8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Cast:0' shape=(None,) dtype=float32>, (-1, 1))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x0000026EDD3255E8>: from cache\n",
      "INFO:tensorflow:Converted call: <function divide at 0x0000026EDD6831F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/UnsortedSegmentSum:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/Reshape_2:0' shape=(None, 1) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function divide at 0x0000026EDD6831F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function gather_v2 at 0x0000026EDD470EE8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/UniqueWithCounts:1' shape=(131072,) dtype=int32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function gather_v2 at 0x0000026EDD470EE8>: from cache\n",
      "INFO:tensorflow:Converted call: <function subtract at 0x0000026EDD683678>\n",
      "    args: (<tf.Tensor 'instance_loss/while/GatherV2:0' shape=(131072, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/Reshape_1:0' shape=(131072, 4) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function subtract at 0x0000026EDD683678>: from cache\n",
      "INFO:tensorflow:Converted call: <function norm_v2 at 0x0000026EDF707B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub:0' shape=(131072, 4) dtype=float32>,)\n",
      "    kwargs: {'axis': 1, 'ord': 1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function norm_v2 at 0x0000026EDF707B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function subtract at 0x0000026EDD683678>\n",
      "    args: (<tf.Tensor 'instance_loss/while/norm/Squeeze:0' shape=(131072,) dtype=float32>, 0.5)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function subtract at 0x0000026EDD683678>: from cache\n",
      "INFO:tensorflow:Converted call: <function clip_by_value at 0x0000026EDF7671F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub_1:0' shape=(131072,) dtype=float32>, 0.0, <tf.Tensor 'instance_loss/while/Sub_1:0' shape=(131072,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function clip_by_value at 0x0000026EDF7671F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function square at 0x0000026EDD317E58>\n",
      "    args: (<tf.Tensor 'instance_loss/while/clip_by_value:0' shape=(131072,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function square at 0x0000026EDD317E58>: from cache\n",
      "INFO:tensorflow:Converted call: <function unsorted_segment_sum at 0x0000026EDD320A68>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Square:0' shape=(131072,) dtype=float32>, <tf.Tensor 'instance_loss/while/UniqueWithCounts:1' shape=(131072,) dtype=int32>, <tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function unsorted_segment_sum at 0x0000026EDD320A68>: from cache\n",
      "INFO:tensorflow:Converted call: <function divide at 0x0000026EDD6831F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/UnsortedSegmentSum_1:0' shape=(None,) dtype=float32>, <tf.Tensor 'instance_loss/while/Cast:0' shape=(None,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function divide at 0x0000026EDD6831F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_sum at 0x0000026EDD6A3EE8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv_1:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Allowlisted <function reduce_sum at 0x0000026EDD6A3EE8>: from cache\n",
      "INFO:tensorflow:Converted call: <function cast at 0x0000026EDD689B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>, tf.float32)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function cast at 0x0000026EDD689B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function divide at 0x0000026EDD6831F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sum:0' shape=() dtype=float32>, <tf.Tensor 'instance_loss/while/Cast_1:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function divide at 0x0000026EDD6831F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function tile at 0x0000026EDD24BDC8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv:0' shape=(None, 4) dtype=float32>, [<tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>, 1])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function tile at 0x0000026EDD24BDC8>: from cache\n",
      "INFO:tensorflow:Converted call: <function tile at 0x0000026EDD24BDC8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv:0' shape=(None, 4) dtype=float32>, [1, <tf.Tensor 'instance_loss/while/Size:0' shape=() dtype=int32>])\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function tile at 0x0000026EDD24BDC8>: from cache\n",
      "INFO:tensorflow:Converted call: <function reshape at 0x0000026EDD3255E8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Tile_1:0' shape=(None, None) dtype=float32>, (<tf.Tensor 'instance_loss/while/mul:0' shape=() dtype=int32>, 4))\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reshape at 0x0000026EDD3255E8>: from cache\n",
      "INFO:tensorflow:Converted call: <function subtract at 0x0000026EDD683678>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Reshape_3:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/Tile:0' shape=(None, 4) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function subtract at 0x0000026EDD683678>: from cache\n",
      "INFO:tensorflow:Converted call: <function abs at 0x0000026EDD67ED38>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub_2:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function abs at 0x0000026EDD67ED38>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_sum at 0x0000026EDD6A3EE8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Abs:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: {'axis': 1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_sum at 0x0000026EDD6A3EE8>: from cache\n",
      "INFO:tensorflow:Converted call: <function zeros at 0x0000026EDD33C9D8>\n",
      "    args: (1,)\n",
      "    kwargs: {'dtype': tf.float32}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function zeros at 0x0000026EDD33C9D8>: from cache\n",
      "INFO:tensorflow:Converted call: <function not_equal at 0x0000026EDD6A3318>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sum_1:0' shape=(None,) dtype=float32>, <tf.Tensor 'instance_loss/while/zeros:0' shape=(1,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function not_equal at 0x0000026EDD6A3318>: from cache\n",
      "INFO:tensorflow:Converted call: <function boolean_mask_v2 at 0x0000026EDD3344C8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub_2:0' shape=(None, 4) dtype=float32>, <tf.Tensor 'instance_loss/while/NotEqual:0' shape=(None,) dtype=bool>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function boolean_mask_v2 at 0x0000026EDD3344C8>: from cache\n",
      "INFO:tensorflow:Converted call: <function norm_v2 at 0x0000026EDF707B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/boolean_mask/GatherV2:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: {'axis': 1, 'ord': 1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function norm_v2 at 0x0000026EDF707B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function subtract at 0x0000026EDD683678>\n",
      "    args: (3.0, <tf.Tensor 'instance_loss/while/norm_1/Squeeze:0' shape=(None,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function subtract at 0x0000026EDD683678>: from cache\n",
      "INFO:tensorflow:Converted call: <function clip_by_value at 0x0000026EDF7671F8>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Sub_3:0' shape=(None,) dtype=float32>, 0.0, <tf.Tensor 'instance_loss/while/Sub_3:0' shape=(None,) dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function clip_by_value at 0x0000026EDF7671F8>: from cache\n",
      "INFO:tensorflow:Converted call: <function square at 0x0000026EDD317E58>\n",
      "    args: (<tf.Tensor 'instance_loss/while/clip_by_value_1:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function square at 0x0000026EDD317E58>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Square_1:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <function norm_v2 at 0x0000026EDF707B88>\n",
      "    args: (<tf.Tensor 'instance_loss/while/truediv:0' shape=(None, 4) dtype=float32>,)\n",
      "    kwargs: {'axis': 1, 'ord': 1}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function norm_v2 at 0x0000026EDF707B88>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/while/norm_2/Squeeze:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8AD5F988>>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/mul_4:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8AD5F988>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8AD5FA48>>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/mul_1:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8AD5FA48>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8AD5FAC8>>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/mul_2:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8AD5FAC8>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8AD5FB88>>\n",
      "    args: (<tf.Tensor 'instance_loss/while/Placeholder_6:0' shape=() dtype=int32>, <tf.Tensor 'instance_loss/while/mul_3:0' shape=() dtype=float32>)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.write of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8AD5FB88>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8ADC3F08>>\n",
      "    args: ()\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8ADC3F08>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8ADC5048>>\n",
      "    args: ()\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8ADC5048>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8ADC6788>>\n",
      "    args: ()\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8ADC6788>>: from cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8ADC6808>>\n",
      "    args: ()\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method TensorArray.stack of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000026E8ADC6808>>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/TensorArrayV2Stack/TensorListStack:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/TensorArrayV2Stack_1/TensorListStack:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/TensorArrayV2Stack_2/TensorListStack:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <function reduce_mean at 0x0000026EDD6AAA68>\n",
      "    args: (<tf.Tensor 'instance_loss/TensorArrayV2Stack_3/TensorListStack:0' shape=(None,) dtype=float32>,)\n",
      "    kwargs: None\n",
      "\n",
      "INFO:tensorflow:Allowlisted <function reduce_mean at 0x0000026EDD6AAA68>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E8475C948>>\n",
      "    args: (<tf.Tensor 'instance_loss/weighted_loss/value:0' shape=() dtype=float32>,)\n",
      "    kwargs: {'sample_weight': <tf.Tensor 'strided_slice:0' shape=() dtype=int32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E8475C948>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E839AFCC8>>\n",
      "    args: (<tf.Tensor 'AddN:0' shape=() dtype=float32>,)\n",
      "    kwargs: {'sample_weight': <tf.Tensor 'strided_slice:0' shape=() dtype=int32>}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.update_state of <keras.metrics.Mean object at 0x0000026E839AFCC8>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method MeanIoU.update_state of <keras.metrics.MeanIoU object at 0x0000026E83968CC8>>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 512, 1) dtype=uint8>, <tf.Tensor 'model/bin_seg/Sigmoid:0' shape=(None, 256, 512, 1) dtype=float32>)\n",
      "    kwargs: {'sample_weight': None}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method MeanIoU.update_state of <keras.metrics.MeanIoU object at 0x0000026E83968CC8>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method MeanMetricWrapper.update_state of <keras.metrics.MeanMetricWrapper object at 0x0000026E86781588>>\n",
      "    args: (<tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 512, 1) dtype=float32>, <tf.Tensor 'model/inst_seg/Sigmoid:0' shape=(None, 256, 512, 4) dtype=float32>)\n",
      "    kwargs: {'sample_weight': None}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method MeanMetricWrapper.update_state of <keras.metrics.MeanMetricWrapper object at 0x0000026E86781588>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E839AFCC8>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E839AFCC8>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E8406F888>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E8406F888>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E8475C948>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.result of <keras.metrics.Mean object at 0x0000026E8475C948>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method MeanIoU.result of <keras.metrics.MeanIoU object at 0x0000026E83968CC8>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method MeanIoU.result of <keras.metrics.MeanIoU object at 0x0000026E83968CC8>>: from cache\n",
      "INFO:tensorflow:Converted call: <bound method Reduce.result of <keras.metrics.MeanMetricWrapper object at 0x0000026E86781588>>\n",
      "    args: ()\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted <bound method Reduce.result of <keras.metrics.MeanMetricWrapper object at 0x0000026E86781588>>: from cache\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.19973, saving model to ./output\\saved-model.hdf5\n",
      "319/319 [==============================] - 149s 428ms/step - loss: 1.1997 - bin_seg_loss: 0.0239 - inst_seg_loss: 1.1758 - bin_seg_mean_io_u: 0.4885 - inst_seg_accuracy: 0.6913 - val_loss: 1.0669 - val_bin_seg_loss: 0.0218 - val_inst_seg_loss: 1.0451 - val_bin_seg_mean_io_u: 0.4883 - val_inst_seg_accuracy: 0.7082\n",
      "Epoch 2/300\n",
      " 78/319 [======>.......................] - ETA: 1:31 - loss: 1.0683 - bin_seg_loss: 0.0221 - inst_seg_loss: 1.0462 - bin_seg_mean_io_u: 0.4884 - inst_seg_accuracy: 0.7040"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DIAZOA~1\\AppData\\Local\\Temp/ipykernel_15736/1663421510.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbin_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minst_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                        callbacks=[term, tensorboard_callback, checkpoint, es])\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3131\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3133\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1960\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    604\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath = \"./output/saved-model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True,\n",
    "                             save_weights_only=True, mode='auto', period=1)\n",
    "history = model.fit(X_train, [bin_train, inst_train],\n",
    "                       batch_size = BS,\n",
    "                       verbose=1,\n",
    "                       epochs=EPOCHS,\n",
    "                       validation_data=(X_test, [bin_test, inst_test]),\n",
    "                        shuffle=False,\n",
    "                       callbacks=[term, tensorboard_callback, checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e913850",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('device:GPU:0'):\n",
    "    history = model.fit(X_train, bin_train,\n",
    "                       batch_size = BS,\n",
    "                       verbose=1,\n",
    "                       epochs=EPOCHS,\n",
    "                       validation_data=(X_test, bin_test),\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c95ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X_train, inst_train_cat,\n",
    "                   batch_size = 2,\n",
    "                   verbose=1,\n",
    "                   epochs=20,\n",
    "                   validation_data=(X_test, inst_test_cat),\n",
    "                    class_weight = class_weights,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('device:GPU:1'):\n",
    "    model.save('./output/lanenet_4lane2_5_model2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dcdf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_unet_model(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('device:GPU:1'):\n",
    "    model.load_weights('./output/Model_VariousWeather/lanenet_4lane2_5_model2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2139f07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 256, 512, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8e8cce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x0000026EBF32EDC8>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x0000026EBF32EDC8>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x0000026EBF32EF78>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(900,) dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x0000026EBF32EF78>: DoNotConvert rule for keras\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x0000026EBF34D708>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, <tf.Tensor 'args_1:0' shape=(900, 256, 512, 3) dtype=float32>)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x0000026EBF34D708>: DoNotConvert rule for keras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bin_pred, inst_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6345db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9dc98aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2710e67d688>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand = np.random.randint(500)\n",
    "plt.imshow(X_test[rand,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "669a2b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 512, 4)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst_pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ed303f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2710e871288>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((bin_pred[rand,:,:,0] > 0.22).astype(np.uint8), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecd2ff31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00147344, 0.00147457, 0.00147523, ..., 0.00236585, 0.00236901,\n",
       "       0.00236979], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.nn.softmax(bin_pred[rand], axis=1)\n",
    "np.unique(np.array(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc28a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b537567",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tf.math.sigmoid(inst_pred[rand,:,:,3]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (bin_pred[rand,:,:,0] > 0.2).astype(np.uint8)\n",
    "p = tf.math.sigmoid(inst_pred[rand,:,:,1])\n",
    "ap = (b*p*255)\n",
    "plt.imshow(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b738e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale(input_arr):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_arr:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    min_val = np.min(input_arr)\n",
    "    max_val = np.max(input_arr)\n",
    "\n",
    "    output_arr = (input_arr - min_val) * 255.0 / (max_val - min_val)\n",
    "\n",
    "    return output_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c93af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    inst_pred[rand,:,:,i] = minmax_scale(np.array(inst_pred[rand][:, :, i]))\n",
    "    embedding_image = np.array(inst_pred[rand], np.uint8)\n",
    "h = (bin_pred[rand,:,:,:] > 0.2).astype(np.uint8)\n",
    "plt.imshow(embedding_image[:,:,(2,1,0)]*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19caf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = (bin_pred[rand,:,:,:] > 0.2).astype(np.uint8)\n",
    "s = h[:,:,(0,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(embedding_image[:,:,(2,1,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = cv2.addWeighted((image_ds[rand,:,:,:]*255).astype(int), 0.5, ap, 1, 0.0)\n",
    "dst\n",
    "plt.imshow(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1999):\n",
    "    b = (bin_pred[i,:,:,0] > 0.37).astype(np.uint8)\n",
    "    p = inst_pred[i,:,:,:]\n",
    "    b = cv2.cvtColor(b, cv2.COLOR_GRAY2BGR)\n",
    "    ap = (b*p*255).astype(int)\n",
    "    dst = cv2.addWeighted((image_ds[i,:,:,:]*255).astype(int), 0.5, ap, 0.5, 0.0)\n",
    "    dst = dst.astype(np.uint8)\n",
    "    cv2.imshow('restult', dst)\n",
    "    cv2.waitKey(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, acc = model.evaluate(X_test, [bin_test, inst_test])\n",
    "print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
    "\n",
    "\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['acc']\n",
    "#acc = history.history['accuracy']\n",
    "val_acc = history.history['val_acc']\n",
    "#val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0561dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin_pred, ins=model.predict(X_test)\n",
    "y_pred_thresholded = bin_pred > 0.5\n",
    "\n",
    "intersection = np.logical_and(y_test, y_pred_thresholded)\n",
    "union = np.logical_or(y_test, y_pred_thresholded)\n",
    "iou_score = np.sum(intersection) / np.sum(union)\n",
    "print(\"IoU socre is: \", iou_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f990ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('lanenet_20211223_instance.hdf5') #Trained for 50 epochs and then additional 100\n",
    "#model.load_weights('mitochondria_gpu_tf1.4.hdf5')  #Trained for 50 epochs\n",
    "\n",
    "test_img_number = random.randint(0, len(X_test))\n",
    "test_img = X_test[test_img_number]\n",
    "ground_truth=y_test[test_img_number]\n",
    "test_img_norm=test_img[:,:,0][:,:,None]\n",
    "test_img_input=np.expand_dims(test_img_norm, 0)\n",
    "prediction = (model.predict(test_img_input)[0,:,:,0] > 0.3).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbdcb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d95876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_img_other = image_ds[0]\n",
    "#test_img_other = cv2.imread('data/training_data_example/image/0000.png', 0)\n",
    "test_img_other = cv2.imread('D:/Project/IrohXu_LaneDetection/data/CARLA/images/00005649.png')\n",
    "test_img_other = Image.fromarray(test_img_other)\n",
    "test_img_other = test_img_other.resize((512, 256))\n",
    "test_img_other_norm = np.array(test_img_other)/255.0\n",
    "#test_img_other_norm=test_img_other_norm[:,:,0][:,:,None]\n",
    "test_img_other_input=np.expand_dims(test_img_other_norm, 0)\n",
    "\n",
    "with tf.device('device:GPU:0'):\n",
    "    bin_pred, inst_pred = model.predict(test_img_other_input)\n",
    "    prediction_other = (bin_pred[0,:,:,0] > 0.3).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f5aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(inst_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_other = cv2.imread('dataset/val/images/Town04_Clear_Noon_09_09_2020_14_57_22_frame_633_validation_set.png')\n",
    "test_img_other = Image.fromarray(test_img_other)\n",
    "test_img_other = test_img_other.resize((512, 256))\n",
    "test_img_other_norm = np.array(test_img_other)/255.0\n",
    "#test_img_other_norm=test_img_other_norm[:,:,0][:,:,None]\n",
    "test_img_other_input=np.expand_dims(test_img_other_norm, 0)\n",
    "\n",
    "bin_pred, inst_pred = model.predict(test_img_other_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9dd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_pred = inst_pred*255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a62603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inst_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed20b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_other = (bin_pred[0,:,:,0] > 0.5).astype(np.uint8)\n",
    "#huf0 = (inst_pred[0][:,:,0] < 120).astype(np.uint8)\n",
    "#huf1 = (inst_pred[0][:,:,1] < 120).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(221)\n",
    "plt.title('Input Image')\n",
    "plt.imshow(test_img_other, cmap='gray')\n",
    "plt.subplot(222)\n",
    "plt.title('Prediction of Lane')\n",
    "plt.imshow(prediction_other, cmap='gray')\n",
    "plt.subplot(223)\n",
    "plt.title('Prediction of Left Lane')\n",
    "plt.imshow(inst_pred[0])\n",
    "plt.subplot(224)\n",
    "plt.title('Prediction of Right Lane')\n",
    "plt.imshow(huf1, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6a3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
